<meta charset="utf-8" /><meta name="hf:doc:metadata" content="{&quot;title&quot;:&quot;Introduction to Reinforcement Learning and its Role in LLMs&quot;,&quot;local&quot;:&quot;introduction-to-reinforcement-learning-and-its-role-in-llms&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;What is Reinforcement Learning (RL)?&quot;,&quot;local&quot;:&quot;what-is-reinforcement-learning-rl&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Agent&quot;,&quot;local&quot;:&quot;agent&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Environment&quot;,&quot;local&quot;:&quot;environment&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Action&quot;,&quot;local&quot;:&quot;action&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Reward&quot;,&quot;local&quot;:&quot;reward&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Policy&quot;,&quot;local&quot;:&quot;policy&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3}],&quot;depth&quot;:2},{&quot;title&quot;:&quot;The RL Process: Trial and Error&quot;,&quot;local&quot;:&quot;the-rl-process-trial-and-error&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Role of RL in Large Language Models (LLMs)&quot;,&quot;local&quot;:&quot;role-of-rl-in-large-language-models-llms&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Reinforcement Learning from Human Feedback (RLHF)&quot;,&quot;local&quot;:&quot;reinforcement-learning-from-human-feedback-rlhf&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Why should we care about GRPO (Group Relative Policy Optimization)?&quot;,&quot;local&quot;:&quot;why-should-we-care-about-grpo-group-relative-policy-optimization&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Quiz&quot;,&quot;local&quot;:&quot;quiz&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;1. What are the key components of Reinforcement Learning?&quot;,&quot;local&quot;:&quot;1-what-are-the-key-components-of-reinforcement-learning&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;2. What is the main advantage of RLHF for training language models?&quot;,&quot;local&quot;:&quot;2-what-is-the-main-advantage-of-rlhf-for-training-language-models&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;3. In the context of RL for LLMs, what represents an “action”?&quot;,&quot;local&quot;:&quot;3-in-the-context-of-rl-for-llms-what-represents-an-action&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;4. What is the role of the reward in RL training of language models?&quot;,&quot;local&quot;:&quot;4-what-is-the-role-of-the-reward-in-rl-training-of-language-models&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;5. What is a reward in the context of RL for LLMs?&quot;,&quot;local&quot;:&quot;5-what-is-a-reward-in-the-context-of-rl-for-llms&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3}],&quot;depth&quot;:2}],&quot;depth&quot;:1}">
		<link href="/docs/course/main/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
		<link rel="modulepreload" href="/docs/course/main/en/_app/immutable/entry/start.a141f479.js">
		<link rel="modulepreload" href="/docs/course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
		<link rel="modulepreload" href="/docs/course/main/en/_app/immutable/chunks/singletons.e3db02f7.js">
		<link rel="modulepreload" href="/docs/course/main/en/_app/immutable/chunks/index.18351ede.js">
		<link rel="modulepreload" href="/docs/course/main/en/_app/immutable/chunks/paths.075b66ab.js">
		<link rel="modulepreload" href="/docs/course/main/en/_app/immutable/entry/app.18a6b6ae.js">
		<link rel="modulepreload" href="/docs/course/main/en/_app/immutable/chunks/index.2bf4358c.js">
		<link rel="modulepreload" href="/docs/course/main/en/_app/immutable/nodes/0.17344edb.js">
		<link rel="modulepreload" href="/docs/course/main/en/_app/immutable/chunks/each.e59479a4.js">
		<link rel="modulepreload" href="/docs/course/main/en/_app/immutable/nodes/28.330f4c8e.js">
		<link rel="modulepreload" href="/docs/course/main/en/_app/immutable/chunks/Tip.363c041f.js">
		<link rel="modulepreload" href="/docs/course/main/en/_app/immutable/chunks/Question.668688bc.js">
		<link rel="modulepreload" href="/docs/course/main/en/_app/immutable/chunks/Heading.8ada512a.js"><!-- HEAD_svelte-u9bgzb_START --><meta name="hf:doc:metadata" content="{&quot;title&quot;:&quot;Introduction to Reinforcement Learning and its Role in LLMs&quot;,&quot;local&quot;:&quot;introduction-to-reinforcement-learning-and-its-role-in-llms&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;What is Reinforcement Learning (RL)?&quot;,&quot;local&quot;:&quot;what-is-reinforcement-learning-rl&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Agent&quot;,&quot;local&quot;:&quot;agent&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Environment&quot;,&quot;local&quot;:&quot;environment&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Action&quot;,&quot;local&quot;:&quot;action&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Reward&quot;,&quot;local&quot;:&quot;reward&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Policy&quot;,&quot;local&quot;:&quot;policy&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3}],&quot;depth&quot;:2},{&quot;title&quot;:&quot;The RL Process: Trial and Error&quot;,&quot;local&quot;:&quot;the-rl-process-trial-and-error&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Role of RL in Large Language Models (LLMs)&quot;,&quot;local&quot;:&quot;role-of-rl-in-large-language-models-llms&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Reinforcement Learning from Human Feedback (RLHF)&quot;,&quot;local&quot;:&quot;reinforcement-learning-from-human-feedback-rlhf&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Why should we care about GRPO (Group Relative Policy Optimization)?&quot;,&quot;local&quot;:&quot;why-should-we-care-about-grpo-group-relative-policy-optimization&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Quiz&quot;,&quot;local&quot;:&quot;quiz&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;1. What are the key components of Reinforcement Learning?&quot;,&quot;local&quot;:&quot;1-what-are-the-key-components-of-reinforcement-learning&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;2. What is the main advantage of RLHF for training language models?&quot;,&quot;local&quot;:&quot;2-what-is-the-main-advantage-of-rlhf-for-training-language-models&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;3. In the context of RL for LLMs, what represents an “action”?&quot;,&quot;local&quot;:&quot;3-in-the-context-of-rl-for-llms-what-represents-an-action&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;4. What is the role of the reward in RL training of language models?&quot;,&quot;local&quot;:&quot;4-what-is-the-role-of-the-reward-in-rl-training-of-language-models&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;5. What is a reward in the context of RL for LLMs?&quot;,&quot;local&quot;:&quot;5-what-is-a-reward-in-the-context-of-rl-for-llms&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3}],&quot;depth&quot;:2}],&quot;depth&quot;:1}"><!-- HEAD_svelte-u9bgzb_END -->      <p></p>  <h1 class="relative group"><a id="introduction-to-reinforcement-learning-and-its-role-in-llms" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#introduction-to-reinforcement-learning-and-its-role-in-llms"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Introduction to Reinforcement Learning and its Role in LLMs</span></h1> <p data-svelte-h="svelte-cir3ne">Welcome to the first page!</p> <p data-svelte-h="svelte-bu5wvy">We’re going to start our journey into the exciting world of Reinforcement Learning (RL) and discover how it’s revolutionizing the way we train Language Models like the ones you might use every day.</p>  <div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">In this chapter, we are focusing on reinforcement learning for language models. However, reinforcement learning is a broad field with many applications beyond language models. If you&#39;re interested in learning more about reinforcement learning, you should check out the [Deep Reinforcement Learning course](https://huggingface.co/courses/deep-rl-course/en/unit1/introduction).</div> <p data-svelte-h="svelte-slvo6d">This page will give you a friendly and clear introduction to RL, even if you’ve never encountered it before. We’ll break down the core ideas and see why RL is becoming so important in the field of Large Language Models (LLMs).</p>  <h2 class="relative group"><a id="what-is-reinforcement-learning-rl" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#what-is-reinforcement-learning-rl"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>What is Reinforcement Learning (RL)?</span></h2> <p data-svelte-h="svelte-zixz1p">Imagine you’re training a dog. You want to teach it to sit. You might say “Sit!” and then, if the dog sits, you give it a treat and praise. If it doesn’t sit, you might gently guide it or just try again. Over time, the dog learns to associate sitting with the positive reward (treat and praise) and is more likely to sit when you say “Sit!” again. In reinforcement learning, we refer to this feedback as a <strong>reward</strong>.</p> <p data-svelte-h="svelte-12ibefg">That, in a nutshell, is the basic idea behind Reinforcement Learning! Instead of a dog, we have a <strong>language model</strong> (in reinforcement learning, we call it an <strong>agent</strong>), and instead of you, we have the <strong>environment</strong> that gives feedback.</p> <p data-svelte-h="svelte-sy5vgw"><img src="https://huggingface.co/reasoning-course/images/resolve/main/grpo/3.jpg" alt="RL terms Process"></p> <p data-svelte-h="svelte-1iaj6ti">Let’s break down the key pieces of RL:</p>  <h3 class="relative group"><a id="agent" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#agent"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Agent</span></h3> <p data-svelte-h="svelte-yob3xl">This is our learner. In the dog example, the dog is the agent. In the context of LLMs, the LLM itself becomes the agent we want to train. The agent is the one making decisions and learning from the environment and its rewards.</p>  <h3 class="relative group"><a id="environment" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#environment"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Environment</span></h3> <p data-svelte-h="svelte-1veabrk">This is the world the agent lives in and interacts with. For the dog, the environment is your house and you. For an LLM, the environment is a bit more abstract – it could be the users it interacts with, or a simulated scenario we set up for it. The environment provides feedback to the agent.</p>  <h3 class="relative group"><a id="action" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#action"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Action</span></h3> <p data-svelte-h="svelte-2rjcti">These are the choices the agent can make in the environment. The dog’s actions are things like “sit”, “stand”, “bark”, etc. For an LLM, actions could be generating words in a sentence, choosing which answer to give to a question, or deciding how to respond in a conversation.</p>  <h3 class="relative group"><a id="reward" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#reward"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Reward</span></h3> <p data-svelte-h="svelte-1qcqexo">This is the feedback the environment gives to the agent after it takes an action. Rewards are usually numbers.</p> <p data-svelte-h="svelte-utdwbs"><strong>Positive rewards</strong> are like treats and praise – they tell the agent “good job, you did something right!“.</p> <p data-svelte-h="svelte-11duu8f"><strong>Negative rewards</strong> (or penalties) are like a gentle “no” – they tell the agent “that wasn’t quite right, try something else”. For the dog, the treat is the reward.</p> <p data-svelte-h="svelte-4rbpl1">For an LLM, rewards are designed to reflect how well the LLM is doing at a specific task – maybe it’s how helpful, truthful, or harmless its response is.</p>  <h3 class="relative group"><a id="policy" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#policy"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Policy</span></h3> <p data-svelte-h="svelte-1jv6l0y">This is the agent’s strategy for choosing actions. It’s like the dog’s understanding of what it should do when you say “Sit!“. In RL, the policy is what we’re really trying to learn and improve. It’s a set of rules or a function that tells the agent what action to take in different situations. Initially, the policy might be random, but as the agent learns, the policy becomes better at choosing actions that lead to higher rewards.</p>  <h2 class="relative group"><a id="the-rl-process-trial-and-error" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#the-rl-process-trial-and-error"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>The RL Process: Trial and Error</span></h2> <p data-svelte-h="svelte-t8x49h"><img src="https://huggingface.co/reasoning-course/images/resolve/main/grpo/1.jpg" alt="RL Process"></p> <p data-svelte-h="svelte-73ben5">Reinforcement Learning happens through a process of trial and error:</p> <table data-svelte-h="svelte-4hnhaa"><thead><tr><th>Step</th> <th>Process</th> <th>Description</th></tr></thead> <tbody><tr><td>1. Observation</td> <td>The agent observes the environment</td> <td>The agent takes in information about its current state and surroundings</td></tr> <tr><td>2. Action</td> <td>The agent takes an action based on its current policy</td> <td>Using its learned strategy (policy), the agent decides what to do next</td></tr> <tr><td>3. Feedback</td> <td>The environment gives the agent a reward</td> <td>The agent receives feedback on how good or bad its action was</td></tr> <tr><td>4. Learning</td> <td>The agent updates its policy based on the reward</td> <td>The agent adjusts its strategy - reinforcing actions that led to high rewards and avoiding those that led low rewards</td></tr> <tr><td>5. Iteration</td> <td>Repeat the process</td> <td>This cycle continues, allowing the agent to continuously improve its decision-making</td></tr></tbody></table> <p data-svelte-h="svelte-wet5s9">Think about learning to ride a bike. You might wobble and fall at first (negative reward!). But when you manage to balance and pedal smoothly, you feel good (positive reward!). You adjust your actions based on this feedback – leaning slightly, pedaling faster, etc. – until you learn to ride well. RL is similar – it’s about learning through interaction and feedback.</p>  <h2 class="relative group"><a id="role-of-rl-in-large-language-models-llms" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#role-of-rl-in-large-language-models-llms"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Role of RL in Large Language Models (LLMs)</span></h2> <p data-svelte-h="svelte-2nnkb3">Now, why is RL so important for Large Language Models?</p> <p data-svelte-h="svelte-59sj0k">Well, training really good LLMs is tricky. We can train them on massive amounts of text from the internet, and they become very good at predicting the next word in a sentence. This is how they learn to generate fluent and grammatically correct text, as we learned in <a href="/chapters/en/chapter2/1">chapter 2</a>.</p> <p data-svelte-h="svelte-1f12e0u">However, just being fluent isn’t enough. We want our LLMs to be more than just good at stringing words together. We want them to be:</p> <ul data-svelte-h="svelte-12mjlhm"><li><strong>Helpful:</strong> Provide useful and relevant information.</li> <li><strong>Harmless:</strong> Avoid generating toxic, biased, or harmful content.</li> <li><strong>Aligned with Human Preferences:</strong> Respond in ways that humans find natural, helpful, and engaging.</li></ul> <p data-svelte-h="svelte-fggtyf">Pre-training LLM methods, which mostly rely on predicting the next word from text data, sometimes fall short on these aspects.</p> <p data-svelte-h="svelte-ao9cj7">Whilst supervised training is excellent at producing structured outputs, it can be less effective at producing helpful, harmless, and aligned responses. We explore supervised training in <a href="/chapters/en/chapter11/1">chapter 11</a>.</p> <p data-svelte-h="svelte-l992q4">Fine-tuned models might generate fluent and structured text that is still factually incorrect, biased, or doesn’t really answer the user’s question in a helpful way.</p> <p data-svelte-h="svelte-13h0geo"><strong>Enter Reinforcement Learning!</strong> RL gives us a way to fine-tune these pre-trained LLMs to better achieve these desired qualities. It’s like giving our LLM dog extra training to become a well-behaved and helpful companion, not just a dog that knows how to bark fluently!</p>  <h2 class="relative group"><a id="reinforcement-learning-from-human-feedback-rlhf" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#reinforcement-learning-from-human-feedback-rlhf"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Reinforcement Learning from Human Feedback (RLHF)</span></h2> <p data-svelte-h="svelte-vbayee">A very popular technique for aligning language models is <strong>Reinforcement Learning from Human Feedback (RLHF)</strong>. In RLHF, we use human feedback as a proxy for the “reward” signal in RL. Here’s how it works:</p> <ol data-svelte-h="svelte-13rt60o"><li><p><strong>Get Human Preferences:</strong> We might ask humans to compare different responses generated by the LLM for the same input prompt and tell us which response they prefer. For example, we might show a human two different answers to the question “What is the capital of France?” and ask them “Which answer is better?“.</p></li> <li><p><strong>Train a Reward Model:</strong> We use this human preference data to train a separate model called a <strong>reward model</strong>. This reward model learns to predict what kind of responses humans will prefer. It learns to score responses based on helpfulness, harmlessness, and alignment with human preferences.</p></li> <li><p><strong>Fine-tune the LLM with RL:</strong> Now we use the reward model as the environment for our LLM agent. The LLM generates responses (actions), and the reward model scores these responses (provides rewards). In essence, we’re training the LLM to produce text that our reward model (which learned from human preferences) thinks is good.</p></li></ol> <p data-svelte-h="svelte-20078t"><img src="https://huggingface.co/reasoning-course/images/resolve/main/grpo/2.jpg" alt="RL Basic Concept"></p> <p data-svelte-h="svelte-1ljlbgz">From a general perspective, let’s look at the benefits of using RL in LLMs:</p> <table data-svelte-h="svelte-omrei2"><thead><tr><th>Benefit</th> <th>Description</th></tr></thead> <tbody><tr><td>Improved Control</td> <td>RL allows us to have more control over the kind of text LLMs generate. We can guide them to produce text that is more aligned with specific goals, like being helpful, creative, or concise.</td></tr> <tr><td>Enhanced Alignment with Human Values</td> <td>RLHF, in particular, helps us align LLMs with complex and often subjective human preferences. It’s hard to write down rules for “what makes a good answer,” but humans can easily judge and compare responses. RLHF lets the model learn from these human judgments.</td></tr> <tr><td>Mitigating Undesirable Behaviors</td> <td>RL can be used to reduce negative behaviors in LLMs, such as generating toxic language, spreading misinformation, or exhibiting biases. By designing rewards that penalize these behaviors, we can nudge the model to avoid them.</td></tr></tbody></table> <p data-svelte-h="svelte-cocpof">Reinforcement Learning from Human Feedback has been used to train many of the most popular LLMs today, such as OpenAI’s GPT-4, Google’s Gemini, and DeepSeek’s R1. There are a wide range of techniques for RLHF, with varying degrees of complexity and sophistication. In this chapter, we will focus on Group Relative Policy Optimization (GRPO), which is a technique for RLHF that has been shown to be effective at training LLMs that are helpful, harmless, and aligned with human preferences.</p>  <h2 class="relative group"><a id="why-should-we-care-about-grpo-group-relative-policy-optimization" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#why-should-we-care-about-grpo-group-relative-policy-optimization"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Why should we care about GRPO (Group Relative Policy Optimization)?</span></h2> <p data-svelte-h="svelte-1toktt4">There are many techniques for RLHF but this course is focused on GRPO because it represents a significant advancement in reinforcement learning for language models.</p> <p data-svelte-h="svelte-1kk40pt">Let’s briefly consider two of other popular techniques for RLHF:</p> <ul data-svelte-h="svelte-19pdi9y"><li>Proximal Policy Optimization (PPO)</li> <li>Direct Preference Optimization (DPO)</li></ul> <p data-svelte-h="svelte-1uprfeu">Proximal Policy Optimization (PPO) was one of the first highly effective techniques for RLHF. It uses a policy gradient method to update the policy based on the reward from a separate reward model.</p> <p data-svelte-h="svelte-gtn9pd">Direct Preference Optimization (DPO) was later developed as a simpler technique that eliminates the need for a separate reward model using preference data directly. Essentially, framing the problem as a classification task between the chosen and rejected responses.</p>  <div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400"><p data-svelte-h="svelte-1irn41k">DPO and PPO are complex reinforcement learning algorithms in their own right, which we will not cover in this course. If you’re interested in learning more about them, you can check out the following resources:</p> <ul data-svelte-h="svelte-1hp1bbe"><li><a href="https://huggingface.co/docs/trl/main/en/ppo_trainer" rel="nofollow">Proximal Policy Optimization</a></li> <li><a href="https://huggingface.co/docs/trl/main/en/dpo_trainer" rel="nofollow">Direct Preference Optimization</a></li></ul></div> <p data-svelte-h="svelte-1r71chc">Unlike DPO and PPO, GRPO groups similar samples together and compares them as a group. The group-based approach provides more stable gradients and better convergence properties compared to other methods.</p> <p data-svelte-h="svelte-172rnsv">GRPO does not use preference data like DPO, but instead compares groups of similar samples using a reward signal from a model or function.</p> <p data-svelte-h="svelte-azw8h4">GRPO is flexible in how it obtains reward signals - it can work with a reward model (like PPO does) but doesn’t strictly require one. This is because GRPO can incorporate reward signals from any function or model that can evaluate the quality of responses.</p> <p data-svelte-h="svelte-hlmg2x">For example, we could use a length function to reward shorter responses, a mathematical solver to verify solution correctness, or a factual correctness function to reward responses that are more factually accurate. This flexibility makes GRPO particularly versatile for different types of alignment tasks.</p> <hr> <p data-svelte-h="svelte-ax6i43">Congratulations on completing Module 1! You’ve now got a solid introduction to Reinforcement Learning and its crucial role in shaping the future of Large Language Models. You understand the basic concepts of RL, why it’s used for LLMs, and you’ve been introduced to GRPO, a key algorithm in this field.</p> <p data-svelte-h="svelte-1k1hoix">In the next module, we’ll get our hands dirty and dive into the DeepSeek R1 paper to see these concepts in action!</p>  <h2 class="relative group"><a id="quiz" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#quiz"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Quiz</span></h2>  <h3 class="relative group"><a id="1-what-are-the-key-components-of-reinforcement-learning" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#1-what-are-the-key-components-of-reinforcement-learning"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>1. What are the key components of Reinforcement Learning?</span></h3> <div><form><label class="block"><input autocomplete="off" class="form-input -mt-1.5 mr-2" name="choice" type="checkbox" value="0"> <!-- HTML_TAG_START -->Agent, Environment, Action, Reward, and Policy<!-- HTML_TAG_END --></label> <label class="block"><input autocomplete="off" class="form-input -mt-1.5 mr-2" name="choice" type="checkbox" value="1"> <!-- HTML_TAG_START -->Model, Data, Loss Function, and Optimizer<!-- HTML_TAG_END --></label> <label class="block"><input autocomplete="off" class="form-input -mt-1.5 mr-2" name="choice" type="checkbox" value="2"> <!-- HTML_TAG_START -->Input, Output, and Hidden Layers<!-- HTML_TAG_END --></label>  <div class="flex flex-row items-center mt-3"><button class="btn px-4 mr-4" type="submit" disabled>Submit</button> </div></form></div>  <h3 class="relative group"><a id="2-what-is-the-main-advantage-of-rlhf-for-training-language-models" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#2-what-is-the-main-advantage-of-rlhf-for-training-language-models"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>2. What is the main advantage of RLHF for training language models?</span></h3> <div><form><label class="block"><input autocomplete="off" class="form-input -mt-1.5 mr-2" name="choice" type="checkbox" value="0"> <!-- HTML_TAG_START -->It helps align models with human preferences and values<!-- HTML_TAG_END --></label> <label class="block"><input autocomplete="off" class="form-input -mt-1.5 mr-2" name="choice" type="checkbox" value="1"> <!-- HTML_TAG_START -->It makes models generate text faster<!-- HTML_TAG_END --></label> <label class="block"><input autocomplete="off" class="form-input -mt-1.5 mr-2" name="choice" type="checkbox" value="2"> <!-- HTML_TAG_START -->It reduces the model's memory usage<!-- HTML_TAG_END --></label>  <div class="flex flex-row items-center mt-3"><button class="btn px-4 mr-4" type="submit" disabled>Submit</button> </div></form></div>  <h3 class="relative group"><a id="3-in-the-context-of-rl-for-llms-what-represents-an-action" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#3-in-the-context-of-rl-for-llms-what-represents-an-action"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>3. In the context of RL for LLMs, what represents an “action”?</span></h3> <div><form><label class="block"><input autocomplete="off" class="form-input -mt-1.5 mr-2" name="choice" type="checkbox" value="0"> <!-- HTML_TAG_START -->Generating words or choosing responses in a conversation<!-- HTML_TAG_END --></label> <label class="block"><input autocomplete="off" class="form-input -mt-1.5 mr-2" name="choice" type="checkbox" value="1"> <!-- HTML_TAG_START -->Updating model weights<!-- HTML_TAG_END --></label> <label class="block"><input autocomplete="off" class="form-input -mt-1.5 mr-2" name="choice" type="checkbox" value="2"> <!-- HTML_TAG_START -->Processing input tokens<!-- HTML_TAG_END --></label>  <div class="flex flex-row items-center mt-3"><button class="btn px-4 mr-4" type="submit" disabled>Submit</button> </div></form></div>  <h3 class="relative group"><a id="4-what-is-the-role-of-the-reward-in-rl-training-of-language-models" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#4-what-is-the-role-of-the-reward-in-rl-training-of-language-models"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>4. What is the role of the reward in RL training of language models?</span></h3> <div><form><label class="block"><input autocomplete="off" class="form-input -mt-1.5 mr-2" name="choice" type="checkbox" value="0"> <!-- HTML_TAG_START -->To provide feedback on how well the model's responses align with desired behavior<!-- HTML_TAG_END --></label> <label class="block"><input autocomplete="off" class="form-input -mt-1.5 mr-2" name="choice" type="checkbox" value="1"> <!-- HTML_TAG_START -->To measure the model's vocabulary size<!-- HTML_TAG_END --></label> <label class="block"><input autocomplete="off" class="form-input -mt-1.5 mr-2" name="choice" type="checkbox" value="2"> <!-- HTML_TAG_START -->To determine the model's training speed<!-- HTML_TAG_END --></label>  <div class="flex flex-row items-center mt-3"><button class="btn px-4 mr-4" type="submit" disabled>Submit</button> </div></form></div>  <h3 class="relative group"><a id="5-what-is-a-reward-in-the-context-of-rl-for-llms" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#5-what-is-a-reward-in-the-context-of-rl-for-llms"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>5. What is a reward in the context of RL for LLMs?</span></h3> <div><form><label class="block"><input autocomplete="off" class="form-input -mt-1.5 mr-2" name="choice" type="checkbox" value="0"> <!-- HTML_TAG_START -->A numerical score that measures the quality of a response<!-- HTML_TAG_END --></label> <label class="block"><input autocomplete="off" class="form-input -mt-1.5 mr-2" name="choice" type="checkbox" value="1"> <!-- HTML_TAG_START -->A function that generates responses<!-- HTML_TAG_END --></label> <label class="block"><input autocomplete="off" class="form-input -mt-1.5 mr-2" name="choice" type="checkbox" value="2"> <!-- HTML_TAG_START -->A model that evaluates the quality of responses<!-- HTML_TAG_END --></label>  <div class="flex flex-row items-center mt-3"><button class="btn px-4 mr-4" type="submit" disabled>Submit</button> </div></form></div>  <p></p> 
			
			<script>
				{
					__sveltekit_lashas = {
						assets: "/docs/course/main/en",
						base: "/docs/course/main/en",
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,null];

					Promise.all([
						import("/docs/course/main/en/_app/immutable/entry/start.a141f479.js"),
						import("/docs/course/main/en/_app/immutable/entry/app.18a6b6ae.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 28],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		
