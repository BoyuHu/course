import{s as Q,n as X,o as I}from"../chunks/scheduler.37c15a92.js";import{S as J,i as K,g as r,s,r as E,A as O,h as i,f as n,c as a,j as F,u as A,x as v,k as N,y as V,a as o,v as D,d as R,t as S,w as j}from"../chunks/index.2bf4358c.js";import{Y as W}from"../chunks/Youtube.1e50a667.js";import{C as Z}from"../chunks/CourseFloatingBanner.15ba07e6.js";import{H as ee}from"../chunks/Heading.8ada512a.js";function te(k){let l,x,_,w,m,C,f,T,p,P,d,q="Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called <em>auto-regressive models</em>.",y,c,G="The pretraining of decoder models usually revolves around predicting the next word in the sentence.",b,u,U="These models are best suited for tasks involving text generation.",L,h,Y="Representatives of this family of models include:",H,g,B='<li><a href="https://huggingface.co/transformers/model_doc/ctrl" rel="nofollow">CTRL</a></li> <li><a href="https://huggingface.co/docs/transformers/model_doc/openai-gpt" rel="nofollow">GPT</a></li> <li><a href="https://huggingface.co/transformers/model_doc/gpt2" rel="nofollow">GPT-2</a></li> <li><a href="https://huggingface.co/transformers/model_doc/transfo-xl" rel="nofollow">Transformer XL</a></li>',M,$,z;return m=new ee({props:{title:"Decoder models",local:"decoder-models",headingTag:"h1"}}),f=new Z({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),p=new W({props:{id:"d_ixlCubqQw"}}),{c(){l=r("meta"),x=s(),_=r("p"),w=s(),E(m.$$.fragment),C=s(),E(f.$$.fragment),T=s(),E(p.$$.fragment),P=s(),d=r("p"),d.innerHTML=q,y=s(),c=r("p"),c.textContent=G,b=s(),u=r("p"),u.textContent=U,L=s(),h=r("p"),h.textContent=Y,H=s(),g=r("ul"),g.innerHTML=B,M=s(),$=r("p"),this.h()},l(e){const t=O("svelte-u9bgzb",document.head);l=i(t,"META",{name:!0,content:!0}),t.forEach(n),x=a(e),_=i(e,"P",{}),F(_).forEach(n),w=a(e),A(m.$$.fragment,e),C=a(e),A(f.$$.fragment,e),T=a(e),A(p.$$.fragment,e),P=a(e),d=i(e,"P",{"data-svelte-h":!0}),v(d)!=="svelte-1n9iaa3"&&(d.innerHTML=q),y=a(e),c=i(e,"P",{"data-svelte-h":!0}),v(c)!=="svelte-5nsp9x"&&(c.textContent=G),b=a(e),u=i(e,"P",{"data-svelte-h":!0}),v(u)!=="svelte-zgz7vj"&&(u.textContent=U),L=a(e),h=i(e,"P",{"data-svelte-h":!0}),v(h)!=="svelte-s27rrg"&&(h.textContent=Y),H=a(e),g=i(e,"UL",{"data-svelte-h":!0}),v(g)!=="svelte-1p88rih"&&(g.innerHTML=B),M=a(e),$=i(e,"P",{}),F($).forEach(n),this.h()},h(){N(l,"name","hf:doc:metadata"),N(l,"content",ne)},m(e,t){V(document.head,l),o(e,x,t),o(e,_,t),o(e,w,t),D(m,e,t),o(e,C,t),D(f,e,t),o(e,T,t),D(p,e,t),o(e,P,t),o(e,d,t),o(e,y,t),o(e,c,t),o(e,b,t),o(e,u,t),o(e,L,t),o(e,h,t),o(e,H,t),o(e,g,t),o(e,M,t),o(e,$,t),z=!0},p:X,i(e){z||(R(m.$$.fragment,e),R(f.$$.fragment,e),R(p.$$.fragment,e),z=!0)},o(e){S(m.$$.fragment,e),S(f.$$.fragment,e),S(p.$$.fragment,e),z=!1},d(e){e&&(n(x),n(_),n(w),n(C),n(T),n(P),n(d),n(y),n(c),n(b),n(u),n(L),n(h),n(H),n(g),n(M),n($)),n(l),j(m,e),j(f,e),j(p,e)}}}const ne='{"title":"Decoder models","local":"decoder-models","sections":[],"depth":1}';function oe(k){return I(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class me extends J{constructor(l){super(),K(this,l,oe,te,Q,{})}}export{me as component};
