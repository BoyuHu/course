import{s as ya,o as Ja,n as gl}from"../chunks/scheduler.37c15a92.js";import{S as ba,i as Ta,g as J,s as r,r as M,m as ie,H as ca,A as fa,h as b,f as n,c as o,j as Il,u as h,x as f,n as re,E as Ma,k as Sl,y as _,a,v as u,t as m,b as g,d as p,w as d,p as v}from"../chunks/index.2bf4358c.js";import{T as kl}from"../chunks/Tip.363c041f.js";import{Y as ha}from"../chunks/Youtube.1e50a667.js";import{C as T}from"../chunks/CodeBlock.4f5fc1ad.js";import{C as ua}from"../chunks/CourseFloatingBanner.15ba07e6.js";import{F as Ua}from"../chunks/FrameworkSwitchCourse.8d4d4ab6.js";import{H as _l}from"../chunks/Heading.8ada512a.js";function ja(w){let s,i;return s=new ua({props:{chapter:6,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3b_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3b_tf.ipynb"}]}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function Ia(w){let s,i;return s=new ua({props:{chapter:6,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section3b_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section3b_pt.ipynb"}]}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function ka(w){let s,i;return s=new ha({props:{id:"b3u8RzBCX9Y"}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function _a(w){let s,i;return s=new ha({props:{id:"_wxyB3j3mk4"}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function ga(w){let s,i;return s=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBURkF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBbW9kZWxfY2hlY2twb2ludCUyMCUzRCUyMCUyMmRpc3RpbGJlcnQtYmFzZS1jYXNlZC1kaXN0aWxsZWQtc3F1YWQlMjIlMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9jaGVja3BvaW50KSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQobW9kZWxfY2hlY2twb2ludCklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIocXVlc3Rpb24lMkMlMjBjb250ZXh0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJ0ZiUyMiklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForQuestionAnswering

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-cased-distilled-squad&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
outputs = model(**inputs)`,wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function va(w){let s,i;return s=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZyUwQSUwQW1vZGVsX2NoZWNrcG9pbnQlMjAlM0QlMjAlMjJkaXN0aWxiZXJ0LWJhc2UtY2FzZWQtZGlzdGlsbGVkLXNxdWFkJTIyJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfY2hlY2twb2ludCklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZChtb2RlbF9jaGVja3BvaW50KSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplcihxdWVzdGlvbiUyQyUyMGNvbnRleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForQuestionAnswering

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-cased-distilled-squad&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
outputs = model(**inputs)`,wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function Ba(w){let s,i;return s=new T({props:{code:"KDElMkMlMjA2NiklMjAoMSUyQyUyMDY2KQ==",highlighted:'(<span class="hljs-number">1</span>, <span class="hljs-number">66</span>) (<span class="hljs-number">1</span>, <span class="hljs-number">66</span>)',wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function Za(w){let s,i;return s=new T({props:{code:"dG9yY2guU2l6ZSglNUIxJTJDJTIwNjYlNUQpJTIwdG9yY2guU2l6ZSglNUIxJTJDJTIwNjYlNUQp",highlighted:'torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">66</span>]) torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">66</span>])',wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function $a(w){let s,i;return s=new T({props:{code:"aW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEElMEFzZXF1ZW5jZV9pZHMlMjAlM0QlMjBpbnB1dHMuc2VxdWVuY2VfaWRzKCklMEElMjMlMjBNYXNrJTIwZXZlcnl0aGluZyUyMGFwYXJ0JTIwZnJvbSUyMHRoZSUyMHRva2VucyUyMG9mJTIwdGhlJTIwY29udGV4dCUwQW1hc2slMjAlM0QlMjAlNUJpJTIwISUzRCUyMDElMjBmb3IlMjBpJTIwaW4lMjBzZXF1ZW5jZV9pZHMlNUQlMEElMjMlMjBVbm1hc2slMjB0aGUlMjAlNUJDTFMlNUQlMjB0b2tlbiUwQW1hc2slNUIwJTVEJTIwJTNEJTIwRmFsc2UlMEFtYXNrJTIwJTNEJTIwdGYuY29uc3RhbnQobWFzayklNUJOb25lJTVEJTBBJTBBc3RhcnRfbG9naXRzJTIwJTNEJTIwdGYud2hlcmUobWFzayUyQyUyMC0xMDAwMCUyQyUyMHN0YXJ0X2xvZ2l0cyklMEFlbmRfbG9naXRzJTIwJTNEJTIwdGYud2hlcmUobWFzayUyQyUyMC0xMDAwMCUyQyUyMGVuZF9sb2dpdHMp",highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Mask everything apart from the tokens of the context</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Unmask the [CLS] token</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
mask = tf.constant(mask)[<span class="hljs-literal">None</span>]

start_logits = tf.where(mask, -<span class="hljs-number">10000</span>, start_logits)
end_logits = tf.where(mask, -<span class="hljs-number">10000</span>, end_logits)`,wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function xa(w){let s,i;return s=new T({props:{code:"aW1wb3J0JTIwdG9yY2glMEElMEFzZXF1ZW5jZV9pZHMlMjAlM0QlMjBpbnB1dHMuc2VxdWVuY2VfaWRzKCklMEElMjMlMjBNYXNrJTIwZXZlcnl0aGluZyUyMGFwYXJ0JTIwZnJvbSUyMHRoZSUyMHRva2VucyUyMG9mJTIwdGhlJTIwY29udGV4dCUwQW1hc2slMjAlM0QlMjAlNUJpJTIwISUzRCUyMDElMjBmb3IlMjBpJTIwaW4lMjBzZXF1ZW5jZV9pZHMlNUQlMEElMjMlMjBVbm1hc2slMjB0aGUlMjAlNUJDTFMlNUQlMjB0b2tlbiUwQW1hc2slNUIwJTVEJTIwJTNEJTIwRmFsc2UlMEFtYXNrJTIwJTNEJTIwdG9yY2gudGVuc29yKG1hc2spJTVCTm9uZSU1RCUwQSUwQXN0YXJ0X2xvZ2l0cyU1Qm1hc2slNUQlMjAlM0QlMjAtMTAwMDAlMEFlbmRfbG9naXRzJTVCbWFzayU1RCUyMCUzRCUyMC0xMDAwMA==",highlighted:`<span class="hljs-keyword">import</span> torch

sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Mask everything apart from the tokens of the context</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Unmask the [CLS] token</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
mask = torch.tensor(mask)[<span class="hljs-literal">None</span>]

start_logits[mask] = -<span class="hljs-number">10000</span>
end_logits[mask] = -<span class="hljs-number">10000</span>`,wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function Ga(w){let s,i;return s=new T({props:{code:"c3RhcnRfcHJvYmFiaWxpdGllcyUyMCUzRCUyMHRmLm1hdGguc29mdG1heChzdGFydF9sb2dpdHMlMkMlMjBheGlzJTNELTEpJTVCMCU1RC5udW1weSgpJTBBZW5kX3Byb2JhYmlsaXRpZXMlMjAlM0QlMjB0Zi5tYXRoLnNvZnRtYXgoZW5kX2xvZ2l0cyUyQyUyMGF4aXMlM0QtMSklNUIwJTVELm51bXB5KCk=",highlighted:`start_probabilities = tf.math.softmax(start_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].numpy()`,wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function Ca(w){let s,i;return s=new T({props:{code:"c3RhcnRfcHJvYmFiaWxpdGllcyUyMCUzRCUyMHRvcmNoLm5uLmZ1bmN0aW9uYWwuc29mdG1heChzdGFydF9sb2dpdHMlMkMlMjBkaW0lM0QtMSklNUIwJTVEJTBBZW5kX3Byb2JhYmlsaXRpZXMlMjAlM0QlMjB0b3JjaC5ubi5mdW5jdGlvbmFsLnNvZnRtYXgoZW5kX2xvZ2l0cyUyQyUyMGRpbSUzRC0xKSU1QjAlNUQ=",highlighted:`start_probabilities = torch.nn.functional.softmax(start_logits, dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]`,wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function Wa(w){let s,i="Then we’ll mask the values where <code>start_index &gt; end_index</code> by setting them to <code>0</code> (the other probabilities are all positive numbers). The <code>np.triu()</code> function returns the upper triangular part of the 2D tensor passed as an argument, so it will do that masking for us:",t,c,y;return c=new T({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBJTBBc2NvcmVzJTIwJTNEJTIwbnAudHJpdShzY29yZXMp",highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

scores = np.triu(scores)`,wrap:!1}}),{c(){s=J("p"),s.innerHTML=i,t=r(),M(c.$$.fragment)},l(U){s=b(U,"P",{"data-svelte-h":!0}),f(s)!=="svelte-sxoqrk"&&(s.innerHTML=i),t=o(U),h(c.$$.fragment,U)},m(U,j){a(U,s,j),a(U,t,j),u(c,U,j),y=!0},i(U){y||(p(c.$$.fragment,U),y=!0)},o(U){m(c.$$.fragment,U),y=!1},d(U){U&&(n(s),n(t)),d(c,U)}}}function Ra(w){let s,i="Then we’ll mask the values where <code>start_index &gt; end_index</code> by setting them to <code>0</code> (the other probabilities are all positive numbers). The <code>torch.triu()</code> function returns the upper triangular part of the 2D tensor passed as an argument, so it will do that masking for us:",t,c,y;return c=new T({props:{code:"c2NvcmVzJTIwJTNEJTIwdG9yY2gudHJpdShzY29yZXMp",highlighted:"scores = torch.triu(scores)",wrap:!1}}),{c(){s=J("p"),s.innerHTML=i,t=r(),M(c.$$.fragment)},l(U){s=b(U,"P",{"data-svelte-h":!0}),f(s)!=="svelte-1htym8"&&(s.innerHTML=i),t=o(U),h(c.$$.fragment,U)},m(U,j){a(U,s,j),a(U,t,j),u(c,U,j),y=!0},i(U){y||(p(c.$$.fragment,U),y=!0)},o(U){m(c.$$.fragment,U),y=!1},d(U){U&&(n(s),n(t)),d(c,U)}}}function Na(w){let s,i="✏️ <strong>Try it out!</strong> Compute the start and end indices for the five most likely answers.";return{c(){s=J("p"),s.innerHTML=i},l(t){s=b(t,"P",{"data-svelte-h":!0}),f(s)!=="svelte-1agqe6l"&&(s.innerHTML=i)},m(t,c){a(t,s,c)},p:gl,d(t){t&&n(s)}}}function Va(w){let s,i="✏️ <strong>Try it out!</strong> Use the best scores you computed earlier to show the five most likely answers. To check your results, go back to the first pipeline and pass in <code>top_k=5</code> when calling it.";return{c(){s=J("p"),s.innerHTML=i},l(t){s=b(t,"P",{"data-svelte-h":!0}),f(s)!=="svelte-gm880d"&&(s.innerHTML=i)},m(t,c){a(t,s,c)},p:gl,d(t){t&&n(s)}}}function Xa(w){let s,i,t,c;return s=new T({props:{code:"XyUyMCUzRCUyMGlucHV0cy5wb3AoJTIyb3ZlcmZsb3dfdG9fc2FtcGxlX21hcHBpbmclMjIpJTBBb2Zmc2V0cyUyMCUzRCUyMGlucHV0cy5wb3AoJTIyb2Zmc2V0X21hcHBpbmclMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW5wdXRzLmNvbnZlcnRfdG9fdGVuc29ycyglMjJ0ZiUyMiklMEFwcmludChpbnB1dHMlNUIlMjJpbnB1dF9pZHMlMjIlNUQuc2hhcGUp",highlighted:`_ = inputs.pop(<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>)
offsets = inputs.pop(<span class="hljs-string">&quot;offset_mapping&quot;</span>)

inputs = inputs.convert_to_tensors(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape)`,wrap:!1}}),t=new T({props:{code:"KDIlMkMlMjAzODQp",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">384</span>)',wrap:!1}}),{c(){M(s.$$.fragment),i=r(),M(t.$$.fragment)},l(y){h(s.$$.fragment,y),i=o(y),h(t.$$.fragment,y)},m(y,U){u(s,y,U),a(y,i,U),u(t,y,U),c=!0},i(y){c||(p(s.$$.fragment,y),p(t.$$.fragment,y),c=!0)},o(y){m(s.$$.fragment,y),m(t.$$.fragment,y),c=!1},d(y){y&&n(i),d(s,y),d(t,y)}}}function za(w){let s,i,t,c;return s=new T({props:{code:"XyUyMCUzRCUyMGlucHV0cy5wb3AoJTIyb3ZlcmZsb3dfdG9fc2FtcGxlX21hcHBpbmclMjIpJTBBb2Zmc2V0cyUyMCUzRCUyMGlucHV0cy5wb3AoJTIyb2Zmc2V0X21hcHBpbmclMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW5wdXRzLmNvbnZlcnRfdG9fdGVuc29ycyglMjJwdCUyMiklMEFwcmludChpbnB1dHMlNUIlMjJpbnB1dF9pZHMlMjIlNUQuc2hhcGUp",highlighted:`_ = inputs.pop(<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>)
offsets = inputs.pop(<span class="hljs-string">&quot;offset_mapping&quot;</span>)

inputs = inputs.convert_to_tensors(<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape)`,wrap:!1}}),t=new T({props:{code:"dG9yY2guU2l6ZSglNUIyJTJDJTIwMzg0JTVEKQ==",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">384</span>])',wrap:!1}}),{c(){M(s.$$.fragment),i=r(),M(t.$$.fragment)},l(y){h(s.$$.fragment,y),i=o(y),h(t.$$.fragment,y)},m(y,U){u(s,y,U),a(y,i,U),u(t,y,U),c=!0},i(y){c||(p(s.$$.fragment,y),p(t.$$.fragment,y),c=!0)},o(y){m(s.$$.fragment,y),m(t.$$.fragment,y),c=!1},d(y){y&&n(i),d(s,y),d(t,y)}}}function Ha(w){let s,i;return s=new T({props:{code:"KDIlMkMlMjAzODQpJTIwKDIlMkMlMjAzODQp",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">384</span>) (<span class="hljs-number">2</span>, <span class="hljs-number">384</span>)',wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function Qa(w){let s,i;return s=new T({props:{code:"dG9yY2guU2l6ZSglNUIyJTJDJTIwMzg0JTVEKSUyMHRvcmNoLlNpemUoJTVCMiUyQyUyMDM4NCU1RCk=",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">384</span>]) torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">384</span>])',wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function Ea(w){let s,i;return s=new T({props:{code:"c2VxdWVuY2VfaWRzJTIwJTNEJTIwaW5wdXRzLnNlcXVlbmNlX2lkcygpJTBBJTIzJTIwTWFzayUyMGV2ZXJ5dGhpbmclMjBhcGFydCUyMGZyb20lMjB0aGUlMjB0b2tlbnMlMjBvZiUyMHRoZSUyMGNvbnRleHQlMEFtYXNrJTIwJTNEJTIwJTVCaSUyMCElM0QlMjAxJTIwZm9yJTIwaSUyMGluJTIwc2VxdWVuY2VfaWRzJTVEJTBBJTIzJTIwVW5tYXNrJTIwdGhlJTIwJTVCQ0xTJTVEJTIwdG9rZW4lMEFtYXNrJTVCMCU1RCUyMCUzRCUyMEZhbHNlJTBBJTIzJTIwTWFzayUyMGFsbCUyMHRoZSUyMCU1QlBBRCU1RCUyMHRva2VucyUwQW1hc2slMjAlM0QlMjB0Zi5tYXRoLmxvZ2ljYWxfb3IodGYuY29uc3RhbnQobWFzayklNUJOb25lJTVEJTJDJTIwaW5wdXRzJTVCJTIyYXR0ZW50aW9uX21hc2slMjIlNUQlMjAlM0QlM0QlMjAwKSUwQSUwQXN0YXJ0X2xvZ2l0cyUyMCUzRCUyMHRmLndoZXJlKG1hc2slMkMlMjAtMTAwMDAlMkMlMjBzdGFydF9sb2dpdHMpJTBBZW5kX2xvZ2l0cyUyMCUzRCUyMHRmLndoZXJlKG1hc2slMkMlMjAtMTAwMDAlMkMlMjBlbmRfbG9naXRzKQ==",highlighted:`sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Mask everything apart from the tokens of the context</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Unmask the [CLS] token</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
<span class="hljs-comment"># Mask all the [PAD] tokens</span>
mask = tf.math.logical_or(tf.constant(mask)[<span class="hljs-literal">None</span>], inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>] == <span class="hljs-number">0</span>)

start_logits = tf.where(mask, -<span class="hljs-number">10000</span>, start_logits)
end_logits = tf.where(mask, -<span class="hljs-number">10000</span>, end_logits)`,wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function Fa(w){let s,i;return s=new T({props:{code:"c2VxdWVuY2VfaWRzJTIwJTNEJTIwaW5wdXRzLnNlcXVlbmNlX2lkcygpJTBBJTIzJTIwTWFzayUyMGV2ZXJ5dGhpbmclMjBhcGFydCUyMGZyb20lMjB0aGUlMjB0b2tlbnMlMjBvZiUyMHRoZSUyMGNvbnRleHQlMEFtYXNrJTIwJTNEJTIwJTVCaSUyMCElM0QlMjAxJTIwZm9yJTIwaSUyMGluJTIwc2VxdWVuY2VfaWRzJTVEJTBBJTIzJTIwVW5tYXNrJTIwdGhlJTIwJTVCQ0xTJTVEJTIwdG9rZW4lMEFtYXNrJTVCMCU1RCUyMCUzRCUyMEZhbHNlJTBBJTIzJTIwTWFzayUyMGFsbCUyMHRoZSUyMCU1QlBBRCU1RCUyMHRva2VucyUwQW1hc2slMjAlM0QlMjB0b3JjaC5sb2dpY2FsX29yKHRvcmNoLnRlbnNvcihtYXNrKSU1Qk5vbmUlNUQlMkMlMjAoaW5wdXRzJTVCJTIyYXR0ZW50aW9uX21hc2slMjIlNUQlMjAlM0QlM0QlMjAwKSklMEElMEFzdGFydF9sb2dpdHMlNUJtYXNrJTVEJTIwJTNEJTIwLTEwMDAwJTBBZW5kX2xvZ2l0cyU1Qm1hc2slNUQlMjAlM0QlMjAtMTAwMDA=",highlighted:`sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Mask everything apart from the tokens of the context</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Unmask the [CLS] token</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
<span class="hljs-comment"># Mask all the [PAD] tokens</span>
mask = torch.logical_or(torch.tensor(mask)[<span class="hljs-literal">None</span>], (inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>] == <span class="hljs-number">0</span>))

start_logits[mask] = -<span class="hljs-number">10000</span>
end_logits[mask] = -<span class="hljs-number">10000</span>`,wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function Ya(w){let s,i;return s=new T({props:{code:"c3RhcnRfcHJvYmFiaWxpdGllcyUyMCUzRCUyMHRmLm1hdGguc29mdG1heChzdGFydF9sb2dpdHMlMkMlMjBheGlzJTNELTEpLm51bXB5KCklMEFlbmRfcHJvYmFiaWxpdGllcyUyMCUzRCUyMHRmLm1hdGguc29mdG1heChlbmRfbG9naXRzJTJDJTIwYXhpcyUzRC0xKS5udW1weSgp",highlighted:`start_probabilities = tf.math.softmax(start_logits, axis=-<span class="hljs-number">1</span>).numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-<span class="hljs-number">1</span>).numpy()`,wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function Sa(w){let s,i;return s=new T({props:{code:"c3RhcnRfcHJvYmFiaWxpdGllcyUyMCUzRCUyMHRvcmNoLm5uLmZ1bmN0aW9uYWwuc29mdG1heChzdGFydF9sb2dpdHMlMkMlMjBkaW0lM0QtMSklMEFlbmRfcHJvYmFiaWxpdGllcyUyMCUzRCUyMHRvcmNoLm5uLmZ1bmN0aW9uYWwuc29mdG1heChlbmRfbG9naXRzJTJDJTIwZGltJTNELTEp",highlighted:`start_probabilities = torch.nn.functional.softmax(start_logits, dim=-<span class="hljs-number">1</span>)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-<span class="hljs-number">1</span>)`,wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function Aa(w){let s,i;return s=new T({props:{code:"Y2FuZGlkYXRlcyUyMCUzRCUyMCU1QiU1RCUwQWZvciUyMHN0YXJ0X3Byb2JzJTJDJTIwZW5kX3Byb2JzJTIwaW4lMjB6aXAoc3RhcnRfcHJvYmFiaWxpdGllcyUyQyUyMGVuZF9wcm9iYWJpbGl0aWVzKSUzQSUwQSUyMCUyMCUyMCUyMHNjb3JlcyUyMCUzRCUyMHN0YXJ0X3Byb2JzJTVCJTNBJTJDJTIwTm9uZSU1RCUyMColMjBlbmRfcHJvYnMlNUJOb25lJTJDJTIwJTNBJTVEJTBBJTIwJTIwJTIwJTIwaWR4JTIwJTNEJTIwbnAudHJpdShzY29yZXMpLmFyZ21heCgpLml0ZW0oKSUwQSUwQSUyMCUyMCUyMCUyMHN0YXJ0X2lkeCUyMCUzRCUyMGlkeCUyMCUyRiUyRiUyMHNjb3Jlcy5zaGFwZSU1QjElNUQlMEElMjAlMjAlMjAlMjBlbmRfaWR4JTIwJTNEJTIwaWR4JTIwJTI1JTIwc2NvcmVzLnNoYXBlJTVCMSU1RCUwQSUyMCUyMCUyMCUyMHNjb3JlJTIwJTNEJTIwc2NvcmVzJTVCc3RhcnRfaWR4JTJDJTIwZW5kX2lkeCU1RC5pdGVtKCklMEElMjAlMjAlMjAlMjBjYW5kaWRhdGVzLmFwcGVuZCgoc3RhcnRfaWR4JTJDJTIwZW5kX2lkeCUyQyUyMHNjb3JlKSklMEElMEFwcmludChjYW5kaWRhdGVzKQ==",highlighted:`candidates = []
<span class="hljs-keyword">for</span> start_probs, end_probs <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(start_probabilities, end_probabilities):
    scores = start_probs[:, <span class="hljs-literal">None</span>] * end_probs[<span class="hljs-literal">None</span>, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[<span class="hljs-number">1</span>]
    end_idx = idx % scores.shape[<span class="hljs-number">1</span>]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

<span class="hljs-built_in">print</span>(candidates)`,wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function qa(w){let s,i;return s=new T({props:{code:"Y2FuZGlkYXRlcyUyMCUzRCUyMCU1QiU1RCUwQWZvciUyMHN0YXJ0X3Byb2JzJTJDJTIwZW5kX3Byb2JzJTIwaW4lMjB6aXAoc3RhcnRfcHJvYmFiaWxpdGllcyUyQyUyMGVuZF9wcm9iYWJpbGl0aWVzKSUzQSUwQSUyMCUyMCUyMCUyMHNjb3JlcyUyMCUzRCUyMHN0YXJ0X3Byb2JzJTVCJTNBJTJDJTIwTm9uZSU1RCUyMColMjBlbmRfcHJvYnMlNUJOb25lJTJDJTIwJTNBJTVEJTBBJTIwJTIwJTIwJTIwaWR4JTIwJTNEJTIwdG9yY2gudHJpdShzY29yZXMpLmFyZ21heCgpLml0ZW0oKSUwQSUwQSUyMCUyMCUyMCUyMHN0YXJ0X2lkeCUyMCUzRCUyMGlkeCUyMCUyRiUyRiUyMHNjb3Jlcy5zaGFwZSU1QjElNUQlMEElMjAlMjAlMjAlMjBlbmRfaWR4JTIwJTNEJTIwaWR4JTIwJTI1JTIwc2NvcmVzLnNoYXBlJTVCMSU1RCUwQSUyMCUyMCUyMCUyMHNjb3JlJTIwJTNEJTIwc2NvcmVzJTVCc3RhcnRfaWR4JTJDJTIwZW5kX2lkeCU1RC5pdGVtKCklMEElMjAlMjAlMjAlMjBjYW5kaWRhdGVzLmFwcGVuZCgoc3RhcnRfaWR4JTJDJTIwZW5kX2lkeCUyQyUyMHNjb3JlKSklMEElMEFwcmludChjYW5kaWRhdGVzKQ==",highlighted:`candidates = []
<span class="hljs-keyword">for</span> start_probs, end_probs <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(start_probabilities, end_probabilities):
    scores = start_probs[:, <span class="hljs-literal">None</span>] * end_probs[<span class="hljs-literal">None</span>, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[<span class="hljs-number">1</span>]
    end_idx = idx % scores.shape[<span class="hljs-number">1</span>]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

<span class="hljs-built_in">print</span>(candidates)`,wrap:!1}}),{c(){M(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,c){u(s,t,c),i=!0},i(t){i||(p(s.$$.fragment,t),i=!0)},o(t){m(s.$$.fragment,t),i=!1},d(t){d(s,t)}}}function La(w){let s,i="✏️ <strong>Try it out!</strong> Adapt the code above to return the scores and spans for the five most likely answers (in total, not per chunk).";return{c(){s=J("p"),s.innerHTML=i},l(t){s=b(t,"P",{"data-svelte-h":!0}),f(s)!=="svelte-ox2gfp"&&(s.innerHTML=i)},m(t,c){a(t,s,c)},p:gl,d(t){t&&n(s)}}}function Da(w){let s,i="✏️ <strong>Try it out!</strong> Use the best scores you computed before to show the five most likely answers (for the whole context, not each chunk). To check your results, go back to the first pipeline and pass in <code>top_k=5</code> when calling it.";return{c(){s=J("p"),s.innerHTML=i},l(t){s=b(t,"P",{"data-svelte-h":!0}),f(s)!=="svelte-5r45to"&&(s.innerHTML=i)},m(t,c){a(t,s,c)},p:gl,d(t){t&&n(s)}}}function Pa(w){let s,i,t,c,y,U,j,Kt,B,Z,Ct,oe,Al="We will now dive into the <code>question-answering</code> pipeline and see how to leverage the offsets to grab the answer to the question at hand from the context, a bit like we did for the grouped entities in the previous section. Then we will see how we can deal with very long contexts that end up being truncated. You can skip this section if you’re not interested in the question answering task.",Ot,$,x,Wt,me,es,pe,ql='As we saw in <a href="/course/chapter1">Chapter 1</a>, we can use the <code>question-answering</code> pipeline like this to get the answer to a question:',ts,ce,ss,Me,ls,he,Ll="Unlike the other pipelines, which can’t truncate and split texts that are longer than the maximum length accepted by the model (and thus may miss information at the end of a document), this pipeline can deal with very long contexts and will return the answer to the question even if it’s at the end:",ns,ue,as,de,is,we,Dl="Let’s see how it does all of this!",rs,ye,os,Je,Pl='Like with any other pipeline, we start by tokenizing our input and then send it through the model. The checkpoint used by default for the <code>question-answering</code> pipeline is <a href="https://huggingface.co/distilbert-base-cased-distilled-squad" rel="nofollow"><code>distilbert-base-cased-distilled-squad</code></a> (the “squad” in the name comes from the dataset on which the model was fine-tuned; we’ll talk more about the SQuAD dataset in <a href="/course/chapter7/7">Chapter 7</a>):',ms,G,C,Rt,be,Kl="Note that we tokenize the question and the context as a pair, with the question first.",ps,te,Ol='<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens.svg" alt="An example of tokenization of question and context"/> <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens-dark.svg" alt="An example of tokenization of question and context"/>',cs,Te,en="Models for question answering work a little differently from the models we’ve seen up to now. Using the picture above as an example, the model has been trained to predict the index of the token starting the answer (here 21) and the index of the token where the answer ends (here 24). This is why those models don’t return one tensor of logits but two: one for the logits corresponding to the start token of the answer, and one for the logits corresponding to the end token of the answer. Since in this case we have only one input containing 66 tokens, we get:",Ms,fe,hs,W,R,Nt,Ue,tn="To convert those logits into probabilities, we will apply a softmax function — but before that, we need to make sure we mask the indices that are not part of the context. Our input is <code>[CLS] question [SEP] context [SEP]</code>, so we need to mask the tokens of the question as well as the <code>[SEP]</code> token. We’ll keep the <code>[CLS]</code> token, however, as some models use it to indicate that the answer is not in the context.",us,je,sn="Since we will apply a softmax afterward, we just need to replace the logits we want to mask with a large negative number. Here, we use <code>-10000</code>:",ds,N,V,Vt,Ie,ln="Now that we have properly masked the logits corresponding to positions we don’t want to predict, we can apply the softmax:",ws,X,z,Xt,ke,nn="At this stage, we could take the argmax of the start and end probabilities — but we might end up with a start index that is greater than the end index, so we need to take a few more precautions. We will compute the probabilities of each possible <code>start_index</code> and <code>end_index</code> where <code>start_index &lt;= end_index</code>, then take the tuple <code>(start_index, end_index)</code> with the highest probability.",ys,I,vl,zt,an="start_index",Bl,Ht,rn="end_index",Zl,Qt,on="start_index",$l,Et,mn="end_index",xl,Js,da='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo><mo>×</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\\mathrm{start\\_probabilities}[\\mathrm{start\\_index}] \\times \\mathrm{end\\_probabilities}[\\mathrm{end\\_index}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">start_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">start_index</span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">end_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">end_index</span></span><span class="mclose">]</span></span></span></span></span>',bs,O,Gl,Ts,wa='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo><mo>×</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\\mathrm{start\\_probabilities}[\\mathrm{start\\_index}] \\times \\mathrm{end\\_probabilities}[\\mathrm{end\\_index}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">start_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">start_index</span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">end_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">end_index</span></span><span class="mclose">]</span></span></span></span>',fs,Ft,pn="start_index <= end_index",Cl,Us,_e,cn="First let’s compute all the possible products:",js,ge,Is,H,Q,Yt,ve,Mn="Now we just have to get the index of the maximum. Since PyTorch will return the index in the flattened tensor, we need to use the floor division <code>//</code> and modulus <code>%</code> operations to get the <code>start_index</code> and <code>end_index</code>:",ks,Be,_s,Ze,hn="We’re not quite done yet, but at least we already have the correct score for the answer (you can check this by comparing it to the first result in the previous section):",gs,$e,vs,se,Bs,xe,un="We have the <code>start_index</code> and <code>end_index</code> of the answer in terms of tokens, so now we just need to convert to the character indices in the context. This is where the offsets will be super useful. We can grab them and use them like we did in the token classification task:",Zs,Ge,$s,Ce,dn="Now we just have to format everything to get our result:",xs,We,Gs,Re,Cs,Ne,wn="Great! That’s the same as in our first example!",Ws,le,Rs,Ve,Ns,Xe,yn="If we try to tokenize the question and long context we used as an example previously, we’ll get a number of tokens higher than the maximum length used in the <code>question-answering</code> pipeline (which is 384):",Vs,ze,Xs,He,zs,Qe,Jn="So, we’ll need to truncate our inputs at that maximum length. There are several ways we can do this, but we don’t want to truncate the question, only the context. Since the context is the second sentence, we’ll use the <code>&quot;only_second&quot;</code> truncation strategy. The problem that arises then is that the answer to the question may not be in the truncated context. Here, for instance, we picked a question where the answer is toward the end of the context, and when we truncate it that answer is not present:",Hs,Ee,Qs,Fe,Es,Ye,bn="This means the model will have a hard time picking the correct answer. To fix this, the <code>question-answering</code> pipeline allows us to split the context into smaller chunks, specifying the maximum length. To make sure we don’t split the context at exactly the wrong place to make it possible to find the answer, it also includes some overlap between the chunks.",Fs,Se,Tn="We can have the tokenizer (fast or slow) do this for us by adding <code>return_overflowing_tokens=True</code>, and we can specify the overlap we want with the <code>stride</code> argument. Here is an example, using a smaller sentence:",Ys,Ae,Ss,qe,As,Le,fn="As we can see, the sentence has been split into chunks in such a way that each entry in <code>inputs[&quot;input_ids&quot;]</code> has at most 6 tokens (we would need to add padding to have the last entry be the same size as the others) and there is an overlap of 2 tokens between each of the entries.",qs,De,Un="Let’s take a closer look at the result of the tokenization:",Ls,Pe,Ds,Ke,Ps,Oe,jn="As expected, we get input IDs and an attention mask. The last key, <code>overflow_to_sample_mapping</code>, is a map that tells us which sentence each of the results corresponds to — here we have 7 results that all come from the (only) sentence we passed the tokenizer:",Ks,et,Os,tt,el,st,In="This is more useful when we tokenize several sentences together. For instance, this:",tl,lt,sl,nt,kn="gets us:",ll,at,nl,it,_n="which means the first sentence is split into 7 chunks as before, and the next 4 chunks come from the second sentence.",al,rt,gn="Now let’s go back to our long context. By default the <code>question-answering</code> pipeline uses a maximum length of 384, as we mentioned earlier, and a stride of 128, which correspond to the way the model was fine-tuned (you can adjust those parameters by passing <code>max_seq_len</code> and <code>stride</code> arguments when calling the pipeline). We will thus use those parameters when tokenizing. We’ll also add padding (to have samples of the same length, so we can build tensors) as well as ask for the offsets:",il,ot,rl,mt,vn="Those <code>inputs</code> will contain the input IDs and attention masks the model expects, as well as the offsets and the <code>overflow_to_sample_mapping</code> we just talked about. Since those two are not parameters used by the model, we’ll pop them out of the <code>inputs</code> (and we won’t store the map, since it’s not useful here) before converting it to a tensor:",ol,E,F,St,pt,Bn="Our long context was split in two, which means that after it goes through our model, we will have two sets of start and end logits:",ml,ct,pl,Y,S,At,Mt,Zn="Like before, we first mask the tokens that are not part of the context before taking the softmax. We also mask all the padding tokens (as flagged by the attention mask):",cl,A,q,qt,ht,$n="Then we can use the softmax to convert our logits to probabilities:",Ml,L,D,Lt,ut,xn="The next step is similar to what we did for the small context, but we repeat it for each of our two chunks. We attribute a score to all possible spans of answer, then take the span with the best score:",hl,P,K,Dt,dt,ul,wt,Gn="Those two candidates correspond to the best answers the model was able to find in each chunk. The model is way more confident the right answer is in the second part (which is a good sign!). Now we just have to map those two token spans to spans of characters in the context (we only need to map the second one to have our answer, but it’s interesting to see what the model has picked in the first chunk).",dl,ne,wl,yt,Cn="The <code>offsets</code> we grabbed earlier is actually a list of offsets, with one list per chunk of text:",yl,Jt,Jl,bt,bl,Tt,Wn="If we ignore the first result, we get the same result as our pipeline for this long context — yay!",Tl,ae,fl,ft,Rn="This concludes our deep dive into the tokenizer’s capabilities. We will put all of this in practice again in the next chapter, when we show you how to fine-tune a model on a range of common NLP tasks.",Ul,Pt,jl;y=new Ua({props:{fw:w[0]}}),j=new _l({props:{title:"Fast tokenizers in the QA pipeline",local:"fast-tokenizers-in-the-qa-pipeline",headingTag:"h1"}});const Nn=[Ia,ja],Ut=[];function Vn(e,l){return e[0]==="pt"?0:1}B=Vn(w),Z=Ut[B]=Nn[B](w);const Xn=[_a,ka],jt=[];function zn(e,l){return e[0]==="pt"?0:1}$=zn(w),x=jt[$]=Xn[$](w),me=new _l({props:{title:"Using the question-answering pipeline",local:"using-the-question-answering-pipeline",headingTag:"h2"}}),ce=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBcXVlc3Rpb25fYW5zd2VyZXIlMjAlM0QlMjBwaXBlbGluZSglMjJxdWVzdGlvbi1hbnN3ZXJpbmclMjIpJTBBY29udGV4dCUyMCUzRCUyMCUyMiUyMiUyMiUwQSVGMCU5RiVBNCU5NyUyMFRyYW5zZm9ybWVycyUyMGlzJTIwYmFja2VkJTIwYnklMjB0aGUlMjB0aHJlZSUyMG1vc3QlMjBwb3B1bGFyJTIwZGVlcCUyMGxlYXJuaW5nJTIwbGlicmFyaWVzJTIwJUUyJTgwJTk0JTIwSmF4JTJDJTIwUHlUb3JjaCUyQyUyMGFuZCUyMFRlbnNvckZsb3clMjAlRTIlODAlOTQlMjB3aXRoJTIwYSUyMHNlYW1sZXNzJTIwaW50ZWdyYXRpb24lMEFiZXR3ZWVuJTIwdGhlbS4lMjBJdCdzJTIwc3RyYWlnaHRmb3J3YXJkJTIwdG8lMjB0cmFpbiUyMHlvdXIlMjBtb2RlbHMlMjB3aXRoJTIwb25lJTIwYmVmb3JlJTIwbG9hZGluZyUyMHRoZW0lMjBmb3IlMjBpbmZlcmVuY2UlMjB3aXRoJTIwdGhlJTIwb3RoZXIuJTBBJTIyJTIyJTIyJTBBcXVlc3Rpb24lMjAlM0QlMjAlMjJXaGljaCUyMGRlZXAlMjBsZWFybmluZyUyMGxpYnJhcmllcyUyMGJhY2slMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlM0YlMjIlMEFxdWVzdGlvbl9hbnN3ZXJlcihxdWVzdGlvbiUzRHF1ZXN0aW9uJTJDJTIwY29udGV4dCUzRGNvbnRleHQp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

question_answerer = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>)
context = <span class="hljs-string">&quot;&quot;&quot;
🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration
between them. It&#x27;s straightforward to train your models with one before loading them for inference with the other.
&quot;&quot;&quot;</span>
question = <span class="hljs-string">&quot;Which deep learning libraries back 🤗 Transformers?&quot;</span>
question_answerer(question=question, context=context)`,wrap:!1}}),Me=new T({props:{code:"JTdCJ3Njb3JlJyUzQSUyMDAuOTc3NzMlMkMlMEElMjAnc3RhcnQnJTNBJTIwNzglMkMlMEElMjAnZW5kJyUzQSUyMDEwNSUyQyUwQSUyMCdhbnN3ZXInJTNBJTIwJ0pheCUyQyUyMFB5VG9yY2glMjBhbmQlMjBUZW5zb3JGbG93JyU3RA==",highlighted:`{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97773</span>,
 <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">78</span>,
 <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">105</span>,
 <span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>}`,wrap:!1}}),ue=new T({props:{code:"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",highlighted:`long_context = <span class="hljs-string">&quot;&quot;&quot;
🤗 Transformers: State of the Art NLP

🤗 Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model&#x27;s lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internals are exposed as consistently as possible.
  - Model files can be used independently of the library for quick experiments.

🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration
between them. It&#x27;s straightforward to train your models with one before loading them for inference with the other.
&quot;&quot;&quot;</span>
question_answerer(question=question, context=long_context)`,wrap:!1}}),de=new T({props:{code:"JTdCJ3Njb3JlJyUzQSUyMDAuOTcxNDklMkMlMEElMjAnc3RhcnQnJTNBJTIwMTg5MiUyQyUwQSUyMCdlbmQnJTNBJTIwMTkxOSUyQyUwQSUyMCdhbnN3ZXInJTNBJTIwJ0pheCUyQyUyMFB5VG9yY2glMjBhbmQlMjBUZW5zb3JGbG93JyU3RA==",highlighted:`{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97149</span>,
 <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">1892</span>,
 <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">1919</span>,
 <span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>}`,wrap:!1}}),ye=new _l({props:{title:"Using a model for question answering",local:"using-a-model-for-question-answering",headingTag:"h2"}});const Hn=[va,ga],It=[];function Qn(e,l){return e[0]==="pt"?0:1}G=Qn(w),C=It[G]=Hn[G](w),fe=new T({props:{code:"c3RhcnRfbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5zdGFydF9sb2dpdHMlMEFlbmRfbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5lbmRfbG9naXRzJTBBcHJpbnQoc3RhcnRfbG9naXRzLnNoYXBlJTJDJTIwZW5kX2xvZ2l0cy5zaGFwZSk=",highlighted:`start_logits = outputs.start_logits
end_logits = outputs.end_logits
<span class="hljs-built_in">print</span>(start_logits.shape, end_logits.shape)`,wrap:!1}});const En=[Za,Ba],kt=[];function Fn(e,l){return e[0]==="pt"?0:1}W=Fn(w),R=kt[W]=En[W](w);const Yn=[xa,$a],_t=[];function Sn(e,l){return e[0]==="pt"?0:1}N=Sn(w),V=_t[N]=Yn[N](w);const An=[Ca,Ga],gt=[];function qn(e,l){return e[0]==="pt"?0:1}X=qn(w),z=gt[X]=An[X](w),ge=new T({props:{code:"c2NvcmVzJTIwJTNEJTIwc3RhcnRfcHJvYmFiaWxpdGllcyU1QiUzQSUyQyUyME5vbmUlNUQlMjAqJTIwZW5kX3Byb2JhYmlsaXRpZXMlNUJOb25lJTJDJTIwJTNBJTVE",highlighted:'scores = start_probabilities[:, <span class="hljs-literal">None</span>] * end_probabilities[<span class="hljs-literal">None</span>, :]',wrap:!1}});const Ln=[Ra,Wa],vt=[];function Dn(e,l){return e[0]==="pt"?0:1}H=Dn(w),Q=vt[H]=Ln[H](w),Be=new T({props:{code:"bWF4X2luZGV4JTIwJTNEJTIwc2NvcmVzLmFyZ21heCgpLml0ZW0oKSUwQXN0YXJ0X2luZGV4JTIwJTNEJTIwbWF4X2luZGV4JTIwJTJGJTJGJTIwc2NvcmVzLnNoYXBlJTVCMSU1RCUwQWVuZF9pbmRleCUyMCUzRCUyMG1heF9pbmRleCUyMCUyNSUyMHNjb3Jlcy5zaGFwZSU1QjElNUQlMEFwcmludChzY29yZXMlNUJzdGFydF9pbmRleCUyQyUyMGVuZF9pbmRleCU1RCk=",highlighted:`max_index = scores.argmax().item()
start_index = max_index // scores.shape[<span class="hljs-number">1</span>]
end_index = max_index % scores.shape[<span class="hljs-number">1</span>]
<span class="hljs-built_in">print</span>(scores[start_index, end_index])`,wrap:!1}}),$e=new T({props:{code:"MC45Nzc3Mw==",highlighted:'<span class="hljs-number">0.97773</span>',wrap:!1}}),se=new kl({props:{$$slots:{default:[Na]},$$scope:{ctx:w}}}),Ge=new T({props:{code:"aW5wdXRzX3dpdGhfb2Zmc2V0cyUyMCUzRCUyMHRva2VuaXplcihxdWVzdGlvbiUyQyUyMGNvbnRleHQlMkMlMjByZXR1cm5fb2Zmc2V0c19tYXBwaW5nJTNEVHJ1ZSklMEFvZmZzZXRzJTIwJTNEJTIwaW5wdXRzX3dpdGhfb2Zmc2V0cyU1QiUyMm9mZnNldF9tYXBwaW5nJTIyJTVEJTBBJTBBc3RhcnRfY2hhciUyQyUyMF8lMjAlM0QlMjBvZmZzZXRzJTVCc3RhcnRfaW5kZXglNUQlMEFfJTJDJTIwZW5kX2NoYXIlMjAlM0QlMjBvZmZzZXRzJTVCZW5kX2luZGV4JTVEJTBBYW5zd2VyJTIwJTNEJTIwY29udGV4dCU1QnN0YXJ0X2NoYXIlM0FlbmRfY2hhciU1RA==",highlighted:`inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=<span class="hljs-literal">True</span>)
offsets = inputs_with_offsets[<span class="hljs-string">&quot;offset_mapping&quot;</span>]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char]`,wrap:!1}}),We=new T({props:{code:"cmVzdWx0JTIwJTNEJTIwJTdCJTBBJTIwJTIwJTIwJTIwJTIyYW5zd2VyJTIyJTNBJTIwYW5zd2VyJTJDJTBBJTIwJTIwJTIwJTIwJTIyc3RhcnQlMjIlM0ElMjBzdGFydF9jaGFyJTJDJTBBJTIwJTIwJTIwJTIwJTIyZW5kJTIyJTNBJTIwZW5kX2NoYXIlMkMlMEElMjAlMjAlMjAlMjAlMjJzY29yZSUyMiUzQSUyMHNjb3JlcyU1QnN0YXJ0X2luZGV4JTJDJTIwZW5kX2luZGV4JTVEJTJDJTBBJTdEJTBBcHJpbnQocmVzdWx0KQ==",highlighted:`result = {
    <span class="hljs-string">&quot;answer&quot;</span>: answer,
    <span class="hljs-string">&quot;start&quot;</span>: start_char,
    <span class="hljs-string">&quot;end&quot;</span>: end_char,
    <span class="hljs-string">&quot;score&quot;</span>: scores[start_index, end_index],
}
<span class="hljs-built_in">print</span>(result)`,wrap:!1}}),Re=new T({props:{code:"JTdCJ2Fuc3dlciclM0ElMjAnSmF4JTJDJTIwUHlUb3JjaCUyMGFuZCUyMFRlbnNvckZsb3cnJTJDJTBBJTIwJ3N0YXJ0JyUzQSUyMDc4JTJDJTBBJTIwJ2VuZCclM0ElMjAxMDUlMkMlMEElMjAnc2NvcmUnJTNBJTIwMC45Nzc3MyU3RA==",highlighted:`{<span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>,
 <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">78</span>,
 <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">105</span>,
 <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97773</span>}`,wrap:!1}}),le=new kl({props:{$$slots:{default:[Va]},$$scope:{ctx:w}}}),Ve=new _l({props:{title:"Handling long contexts",local:"handling-long-contexts",headingTag:"h2"}}),ze=new T({props:{code:"aW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHF1ZXN0aW9uJTJDJTIwbG9uZ19jb250ZXh0KSUwQXByaW50KGxlbihpbnB1dHMlNUIlMjJpbnB1dF9pZHMlMjIlNUQpKQ==",highlighted:`inputs = tokenizer(question, long_context)
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]))`,wrap:!1}}),He=new T({props:{code:"NDYx",highlighted:'<span class="hljs-number">461</span>',wrap:!1}}),Ee=new T({props:{code:"aW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHF1ZXN0aW9uJTJDJTIwbG9uZ19jb250ZXh0JTJDJTIwbWF4X2xlbmd0aCUzRDM4NCUyQyUyMHRydW5jYXRpb24lM0QlMjJvbmx5X3NlY29uZCUyMiklMEFwcmludCh0b2tlbml6ZXIuZGVjb2RlKGlucHV0cyU1QiUyMmlucHV0X2lkcyUyMiU1RCkp",highlighted:`inputs = tokenizer(question, long_context, max_length=<span class="hljs-number">384</span>, truncation=<span class="hljs-string">&quot;only_second&quot;</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]))`,wrap:!1}}),Fe=new T({props:{code:"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",highlighted:`<span class="hljs-string">&quot;&quot;&quot;
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model&#x27;s lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internal [SEP]
&quot;&quot;&quot;</span>`,wrap:!1}}),Ae=new T({props:{code:"c2VudGVuY2UlMjAlM0QlMjAlMjJUaGlzJTIwc2VudGVuY2UlMjBpcyUyMG5vdCUyMHRvbyUyMGxvbmclMjBidXQlMjB3ZSUyMGFyZSUyMGdvaW5nJTIwdG8lMjBzcGxpdCUyMGl0JTIwYW55d2F5LiUyMiUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglMEElMjAlMjAlMjAlMjBzZW50ZW5jZSUyQyUyMHRydW5jYXRpb24lM0RUcnVlJTJDJTIwcmV0dXJuX292ZXJmbG93aW5nX3Rva2VucyUzRFRydWUlMkMlMjBtYXhfbGVuZ3RoJTNENiUyQyUyMHN0cmlkZSUzRDIlMEEpJTBBJTBBZm9yJTIwaWRzJTIwaW4lMjBpbnB1dHMlNUIlMjJpbnB1dF9pZHMlMjIlNUQlM0ElMEElMjAlMjAlMjAlMjBwcmludCh0b2tlbml6ZXIuZGVjb2RlKGlkcykp",highlighted:`sentence = <span class="hljs-string">&quot;This sentence is not too long but we are going to split it anyway.&quot;</span>
inputs = tokenizer(
    sentence, truncation=<span class="hljs-literal">True</span>, return_overflowing_tokens=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">6</span>, stride=<span class="hljs-number">2</span>
)

<span class="hljs-keyword">for</span> ids <span class="hljs-keyword">in</span> inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]:
    <span class="hljs-built_in">print</span>(tokenizer.decode(ids))`,wrap:!1}}),qe=new T({props:{code:"JyU1QkNMUyU1RCUyMFRoaXMlMjBzZW50ZW5jZSUyMGlzJTIwbm90JTIwJTVCU0VQJTVEJyUwQSclNUJDTFMlNUQlMjBpcyUyMG5vdCUyMHRvbyUyMGxvbmclMjAlNUJTRVAlNUQnJTBBJyU1QkNMUyU1RCUyMHRvbyUyMGxvbmclMjBidXQlMjB3ZSUyMCU1QlNFUCU1RCclMEEnJTVCQ0xTJTVEJTIwYnV0JTIwd2UlMjBhcmUlMjBnb2luZyUyMCU1QlNFUCU1RCclMEEnJTVCQ0xTJTVEJTIwYXJlJTIwZ29pbmclMjB0byUyMHNwbGl0JTIwJTVCU0VQJTVEJyUwQSclNUJDTFMlNUQlMjB0byUyMHNwbGl0JTIwaXQlMjBhbnl3YXklMjAlNUJTRVAlNUQnJTBBJyU1QkNMUyU1RCUyMGl0JTIwYW55d2F5LiUyMCU1QlNFUCU1RCc=",highlighted:`<span class="hljs-string">&#x27;[CLS] This sentence is not [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] is not too long [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] too long but we [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] but we are going [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] are going to split [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] to split it anyway [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] it anyway. [SEP]&#x27;</span>`,wrap:!1}}),Pe=new T({props:{code:"cHJpbnQoaW5wdXRzLmtleXMoKSk=",highlighted:'<span class="hljs-built_in">print</span>(inputs.keys())',wrap:!1}}),Ke=new T({props:{code:"ZGljdF9rZXlzKCU1QidpbnB1dF9pZHMnJTJDJTIwJ2F0dGVudGlvbl9tYXNrJyUyQyUyMCdvdmVyZmxvd190b19zYW1wbGVfbWFwcGluZyclNUQp",highlighted:'dict_keys([<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;overflow_to_sample_mapping&#x27;</span>])',wrap:!1}}),et=new T({props:{code:"cHJpbnQoaW5wdXRzJTVCJTIyb3ZlcmZsb3dfdG9fc2FtcGxlX21hcHBpbmclMjIlNUQp",highlighted:'<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>])',wrap:!1}}),tt=new T({props:{code:"JTVCMCUyQyUyMDAlMkMlMjAwJTJDJTIwMCUyQyUyMDAlMkMlMjAwJTJDJTIwMCU1RA==",highlighted:'[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]',wrap:!1}}),lt=new T({props:{code:"c2VudGVuY2VzJTIwJTNEJTIwJTVCJTBBJTIwJTIwJTIwJTIwJTIyVGhpcyUyMHNlbnRlbmNlJTIwaXMlMjBub3QlMjB0b28lMjBsb25nJTIwYnV0JTIwd2UlMjBhcmUlMjBnb2luZyUyMHRvJTIwc3BsaXQlMjBpdCUyMGFueXdheS4lMjIlMkMlMEElMjAlMjAlMjAlMjAlMjJUaGlzJTIwc2VudGVuY2UlMjBpcyUyMHNob3J0ZXIlMjBidXQlMjB3aWxsJTIwc3RpbGwlMjBnZXQlMjBzcGxpdC4lMjIlMkMlMEElNUQlMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTBBJTIwJTIwJTIwJTIwc2VudGVuY2VzJTJDJTIwdHJ1bmNhdGlvbiUzRFRydWUlMkMlMjByZXR1cm5fb3ZlcmZsb3dpbmdfdG9rZW5zJTNEVHJ1ZSUyQyUyMG1heF9sZW5ndGglM0Q2JTJDJTIwc3RyaWRlJTNEMiUwQSklMEElMEFwcmludChpbnB1dHMlNUIlMjJvdmVyZmxvd190b19zYW1wbGVfbWFwcGluZyUyMiU1RCk=",highlighted:`sentences = [
    <span class="hljs-string">&quot;This sentence is not too long but we are going to split it anyway.&quot;</span>,
    <span class="hljs-string">&quot;This sentence is shorter but will still get split.&quot;</span>,
]
inputs = tokenizer(
    sentences, truncation=<span class="hljs-literal">True</span>, return_overflowing_tokens=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">6</span>, stride=<span class="hljs-number">2</span>
)

<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>])`,wrap:!1}}),at=new T({props:{code:"JTVCMCUyQyUyMDAlMkMlMjAwJTJDJTIwMCUyQyUyMDAlMkMlMjAwJTJDJTIwMCUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElNUQ=",highlighted:'[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]',wrap:!1}}),ot=new T({props:{code:"aW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUwQSUyMCUyMCUyMCUyMHF1ZXN0aW9uJTJDJTBBJTIwJTIwJTIwJTIwbG9uZ19jb250ZXh0JTJDJTBBJTIwJTIwJTIwJTIwc3RyaWRlJTNEMTI4JTJDJTBBJTIwJTIwJTIwJTIwbWF4X2xlbmd0aCUzRDM4NCUyQyUwQSUyMCUyMCUyMCUyMHBhZGRpbmclM0QlMjJsb25nZXN0JTIyJTJDJTBBJTIwJTIwJTIwJTIwdHJ1bmNhdGlvbiUzRCUyMm9ubHlfc2Vjb25kJTIyJTJDJTBBJTIwJTIwJTIwJTIwcmV0dXJuX292ZXJmbG93aW5nX3Rva2VucyUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjByZXR1cm5fb2Zmc2V0c19tYXBwaW5nJTNEVHJ1ZSUyQyUwQSk=",highlighted:`inputs = tokenizer(
    question,
    long_context,
    stride=<span class="hljs-number">128</span>,
    max_length=<span class="hljs-number">384</span>,
    padding=<span class="hljs-string">&quot;longest&quot;</span>,
    truncation=<span class="hljs-string">&quot;only_second&quot;</span>,
    return_overflowing_tokens=<span class="hljs-literal">True</span>,
    return_offsets_mapping=<span class="hljs-literal">True</span>,
)`,wrap:!1}});const Pn=[za,Xa],Bt=[];function Kn(e,l){return e[0]==="pt"?0:1}E=Kn(w),F=Bt[E]=Pn[E](w),ct=new T({props:{code:"b3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQXN0YXJ0X2xvZ2l0cyUyMCUzRCUyMG91dHB1dHMuc3RhcnRfbG9naXRzJTBBZW5kX2xvZ2l0cyUyMCUzRCUyMG91dHB1dHMuZW5kX2xvZ2l0cyUwQXByaW50KHN0YXJ0X2xvZ2l0cy5zaGFwZSUyQyUyMGVuZF9sb2dpdHMuc2hhcGUp",highlighted:`outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
<span class="hljs-built_in">print</span>(start_logits.shape, end_logits.shape)`,wrap:!1}});const On=[Qa,Ha],Zt=[];function ea(e,l){return e[0]==="pt"?0:1}Y=ea(w),S=Zt[Y]=On[Y](w);const ta=[Fa,Ea],$t=[];function sa(e,l){return e[0]==="pt"?0:1}A=sa(w),q=$t[A]=ta[A](w);const la=[Sa,Ya],xt=[];function na(e,l){return e[0]==="pt"?0:1}L=na(w),D=xt[L]=la[L](w);const aa=[qa,Aa],Gt=[];function ia(e,l){return e[0]==="pt"?0:1}return P=ia(w),K=Gt[P]=aa[P](w),dt=new T({props:{code:"JTVCKDAlMkMlMjAxOCUyQyUyMDAuMzM4NjcpJTJDJTIwKDE3MyUyQyUyMDE4NCUyQyUyMDAuOTcxNDkpJTVE",highlighted:'[(<span class="hljs-number">0</span>, <span class="hljs-number">18</span>, <span class="hljs-number">0.33867</span>), (<span class="hljs-number">173</span>, <span class="hljs-number">184</span>, <span class="hljs-number">0.97149</span>)]',wrap:!1}}),ne=new kl({props:{$$slots:{default:[La]},$$scope:{ctx:w}}}),Jt=new T({props:{code:"Zm9yJTIwY2FuZGlkYXRlJTJDJTIwb2Zmc2V0JTIwaW4lMjB6aXAoY2FuZGlkYXRlcyUyQyUyMG9mZnNldHMpJTNBJTBBJTIwJTIwJTIwJTIwc3RhcnRfdG9rZW4lMkMlMjBlbmRfdG9rZW4lMkMlMjBzY29yZSUyMCUzRCUyMGNhbmRpZGF0ZSUwQSUyMCUyMCUyMCUyMHN0YXJ0X2NoYXIlMkMlMjBfJTIwJTNEJTIwb2Zmc2V0JTVCc3RhcnRfdG9rZW4lNUQlMEElMjAlMjAlMjAlMjBfJTJDJTIwZW5kX2NoYXIlMjAlM0QlMjBvZmZzZXQlNUJlbmRfdG9rZW4lNUQlMEElMjAlMjAlMjAlMjBhbnN3ZXIlMjAlM0QlMjBsb25nX2NvbnRleHQlNUJzdGFydF9jaGFyJTNBZW5kX2NoYXIlNUQlMEElMjAlMjAlMjAlMjByZXN1bHQlMjAlM0QlMjAlN0IlMjJhbnN3ZXIlMjIlM0ElMjBhbnN3ZXIlMkMlMjAlMjJzdGFydCUyMiUzQSUyMHN0YXJ0X2NoYXIlMkMlMjAlMjJlbmQlMjIlM0ElMjBlbmRfY2hhciUyQyUyMCUyMnNjb3JlJTIyJTNBJTIwc2NvcmUlN0QlMEElMjAlMjAlMjAlMjBwcmludChyZXN1bHQp",highlighted:`<span class="hljs-keyword">for</span> candidate, offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {<span class="hljs-string">&quot;answer&quot;</span>: answer, <span class="hljs-string">&quot;start&quot;</span>: start_char, <span class="hljs-string">&quot;end&quot;</span>: end_char, <span class="hljs-string">&quot;score&quot;</span>: score}
    <span class="hljs-built_in">print</span>(result)`,wrap:!1}}),bt=new T({props:{code:"JTdCJ2Fuc3dlciclM0ElMjAnJTVDbiVGMCU5RiVBNCU5NyUyMFRyYW5zZm9ybWVycyUzQSUyMFN0YXRlJTIwb2YlMjB0aGUlMjBBcnQlMjBOTFAnJTJDJTIwJ3N0YXJ0JyUzQSUyMDAlMkMlMjAnZW5kJyUzQSUyMDM3JTJDJTIwJ3Njb3JlJyUzQSUyMDAuMzM4NjclN0QlMEElN0InYW5zd2VyJyUzQSUyMCdKYXglMkMlMjBQeVRvcmNoJTIwYW5kJTIwVGVuc29yRmxvdyclMkMlMjAnc3RhcnQnJTNBJTIwMTg5MiUyQyUyMCdlbmQnJTNBJTIwMTkxOSUyQyUyMCdzY29yZSclM0ElMjAwLjk3MTQ5JTdE",highlighted:`{<span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;\\n🤗 Transformers: State of the Art NLP&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">37</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.33867</span>}
{<span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">1892</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">1919</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97149</span>}`,wrap:!1}}),ae=new kl({props:{$$slots:{default:[Da]},$$scope:{ctx:w}}}),{c(){s=J("meta"),i=r(),t=J("p"),c=r(),M(y.$$.fragment),U=r(),M(j.$$.fragment),Kt=r(),Z.c(),Ct=r(),oe=J("p"),oe.innerHTML=Al,Ot=r(),x.c(),Wt=r(),M(me.$$.fragment),es=r(),pe=J("p"),pe.innerHTML=ql,ts=r(),M(ce.$$.fragment),ss=r(),M(Me.$$.fragment),ls=r(),he=J("p"),he.textContent=Ll,ns=r(),M(ue.$$.fragment),as=r(),M(de.$$.fragment),is=r(),we=J("p"),we.textContent=Dl,rs=r(),M(ye.$$.fragment),os=r(),Je=J("p"),Je.innerHTML=Pl,ms=r(),C.c(),Rt=r(),be=J("p"),be.textContent=Kl,ps=r(),te=J("div"),te.innerHTML=Ol,cs=r(),Te=J("p"),Te.textContent=en,Ms=r(),M(fe.$$.fragment),hs=r(),R.c(),Nt=r(),Ue=J("p"),Ue.innerHTML=tn,us=r(),je=J("p"),je.innerHTML=sn,ds=r(),V.c(),Vt=r(),Ie=J("p"),Ie.textContent=ln,ws=r(),z.c(),Xt=r(),ke=J("p"),ke.innerHTML=nn,ys=r(),I=J("p"),vl=ie("Assuming the events “The answer starts at "),zt=J("code"),zt.textContent=an,Bl=ie("” and “The answer ends at "),Ht=J("code"),Ht.textContent=rn,Zl=ie("” to be independent, the probability that the answer starts at "),Qt=J("code"),Qt.textContent=on,$l=ie(" and ends at "),Et=J("code"),Et.textContent=mn,xl=ie(` is:
`),Js=new ca(!1),bs=r(),O=J("p"),Gl=ie("So, to compute all the scores, we just need to compute all the products"),Ts=new ca(!1),fs=ie(" where "),Ft=J("code"),Ft.textContent=pn,Cl=ie("."),Us=r(),_e=J("p"),_e.textContent=cn,js=r(),M(ge.$$.fragment),Is=r(),Q.c(),Yt=r(),ve=J("p"),ve.innerHTML=Mn,ks=r(),M(Be.$$.fragment),_s=r(),Ze=J("p"),Ze.textContent=hn,gs=r(),M($e.$$.fragment),vs=r(),M(se.$$.fragment),Bs=r(),xe=J("p"),xe.innerHTML=un,Zs=r(),M(Ge.$$.fragment),$s=r(),Ce=J("p"),Ce.textContent=dn,xs=r(),M(We.$$.fragment),Gs=r(),M(Re.$$.fragment),Cs=r(),Ne=J("p"),Ne.textContent=wn,Ws=r(),M(le.$$.fragment),Rs=r(),M(Ve.$$.fragment),Ns=r(),Xe=J("p"),Xe.innerHTML=yn,Vs=r(),M(ze.$$.fragment),Xs=r(),M(He.$$.fragment),zs=r(),Qe=J("p"),Qe.innerHTML=Jn,Hs=r(),M(Ee.$$.fragment),Qs=r(),M(Fe.$$.fragment),Es=r(),Ye=J("p"),Ye.innerHTML=bn,Fs=r(),Se=J("p"),Se.innerHTML=Tn,Ys=r(),M(Ae.$$.fragment),Ss=r(),M(qe.$$.fragment),As=r(),Le=J("p"),Le.innerHTML=fn,qs=r(),De=J("p"),De.textContent=Un,Ls=r(),M(Pe.$$.fragment),Ds=r(),M(Ke.$$.fragment),Ps=r(),Oe=J("p"),Oe.innerHTML=jn,Ks=r(),M(et.$$.fragment),Os=r(),M(tt.$$.fragment),el=r(),st=J("p"),st.textContent=In,tl=r(),M(lt.$$.fragment),sl=r(),nt=J("p"),nt.textContent=kn,ll=r(),M(at.$$.fragment),nl=r(),it=J("p"),it.textContent=_n,al=r(),rt=J("p"),rt.innerHTML=gn,il=r(),M(ot.$$.fragment),rl=r(),mt=J("p"),mt.innerHTML=vn,ol=r(),F.c(),St=r(),pt=J("p"),pt.textContent=Bn,ml=r(),M(ct.$$.fragment),pl=r(),S.c(),At=r(),Mt=J("p"),Mt.textContent=Zn,cl=r(),q.c(),qt=r(),ht=J("p"),ht.textContent=$n,Ml=r(),D.c(),Lt=r(),ut=J("p"),ut.textContent=xn,hl=r(),K.c(),Dt=r(),M(dt.$$.fragment),ul=r(),wt=J("p"),wt.textContent=Gn,dl=r(),M(ne.$$.fragment),wl=r(),yt=J("p"),yt.innerHTML=Cn,yl=r(),M(Jt.$$.fragment),Jl=r(),M(bt.$$.fragment),bl=r(),Tt=J("p"),Tt.textContent=Wn,Tl=r(),M(ae.$$.fragment),fl=r(),ft=J("p"),ft.textContent=Rn,Ul=r(),Pt=J("p"),this.h()},l(e){const l=fa("svelte-u9bgzb",document.head);s=b(l,"META",{name:!0,content:!0}),l.forEach(n),i=o(e),t=b(e,"P",{}),Il(t).forEach(n),c=o(e),h(y.$$.fragment,e),U=o(e),h(j.$$.fragment,e),Kt=o(e),Z.l(e),Ct=o(e),oe=b(e,"P",{"data-svelte-h":!0}),f(oe)!=="svelte-lvoxg9"&&(oe.innerHTML=Al),Ot=o(e),x.l(e),Wt=o(e),h(me.$$.fragment,e),es=o(e),pe=b(e,"P",{"data-svelte-h":!0}),f(pe)!=="svelte-1rzsno5"&&(pe.innerHTML=ql),ts=o(e),h(ce.$$.fragment,e),ss=o(e),h(Me.$$.fragment,e),ls=o(e),he=b(e,"P",{"data-svelte-h":!0}),f(he)!=="svelte-1pagerf"&&(he.textContent=Ll),ns=o(e),h(ue.$$.fragment,e),as=o(e),h(de.$$.fragment,e),is=o(e),we=b(e,"P",{"data-svelte-h":!0}),f(we)!=="svelte-sapdef"&&(we.textContent=Dl),rs=o(e),h(ye.$$.fragment,e),os=o(e),Je=b(e,"P",{"data-svelte-h":!0}),f(Je)!=="svelte-w4r3vw"&&(Je.innerHTML=Pl),ms=o(e),C.l(e),Rt=o(e),be=b(e,"P",{"data-svelte-h":!0}),f(be)!=="svelte-1f1yf5i"&&(be.textContent=Kl),ps=o(e),te=b(e,"DIV",{class:!0,"data-svelte-h":!0}),f(te)!=="svelte-47wedv"&&(te.innerHTML=Ol),cs=o(e),Te=b(e,"P",{"data-svelte-h":!0}),f(Te)!=="svelte-lbyhwl"&&(Te.textContent=en),Ms=o(e),h(fe.$$.fragment,e),hs=o(e),R.l(e),Nt=o(e),Ue=b(e,"P",{"data-svelte-h":!0}),f(Ue)!=="svelte-bkvch0"&&(Ue.innerHTML=tn),us=o(e),je=b(e,"P",{"data-svelte-h":!0}),f(je)!=="svelte-6ptczc"&&(je.innerHTML=sn),ds=o(e),V.l(e),Vt=o(e),Ie=b(e,"P",{"data-svelte-h":!0}),f(Ie)!=="svelte-1j5ys7d"&&(Ie.textContent=ln),ws=o(e),z.l(e),Xt=o(e),ke=b(e,"P",{"data-svelte-h":!0}),f(ke)!=="svelte-1qmjo9o"&&(ke.innerHTML=nn),ys=o(e),I=b(e,"P",{});var k=Il(I);vl=re(k,"Assuming the events “The answer starts at "),zt=b(k,"CODE",{"data-svelte-h":!0}),f(zt)!=="svelte-8eqb3b"&&(zt.textContent=an),Bl=re(k,"” and “The answer ends at "),Ht=b(k,"CODE",{"data-svelte-h":!0}),f(Ht)!=="svelte-9cistc"&&(Ht.textContent=rn),Zl=re(k,"” to be independent, the probability that the answer starts at "),Qt=b(k,"CODE",{"data-svelte-h":!0}),f(Qt)!=="svelte-8eqb3b"&&(Qt.textContent=on),$l=re(k," and ends at "),Et=b(k,"CODE",{"data-svelte-h":!0}),f(Et)!=="svelte-9cistc"&&(Et.textContent=mn),xl=re(k,` is:
`),Js=Ma(k,!1),k.forEach(n),bs=o(e),O=b(e,"P",{});var ee=Il(O);Gl=re(ee,"So, to compute all the scores, we just need to compute all the products"),Ts=Ma(ee,!1),fs=re(ee," where "),Ft=b(ee,"CODE",{"data-svelte-h":!0}),f(Ft)!=="svelte-1kp3szf"&&(Ft.textContent=pn),Cl=re(ee,"."),ee.forEach(n),Us=o(e),_e=b(e,"P",{"data-svelte-h":!0}),f(_e)!=="svelte-193wrn1"&&(_e.textContent=cn),js=o(e),h(ge.$$.fragment,e),Is=o(e),Q.l(e),Yt=o(e),ve=b(e,"P",{"data-svelte-h":!0}),f(ve)!=="svelte-1mc8mc3"&&(ve.innerHTML=Mn),ks=o(e),h(Be.$$.fragment,e),_s=o(e),Ze=b(e,"P",{"data-svelte-h":!0}),f(Ze)!=="svelte-mq04r0"&&(Ze.textContent=hn),gs=o(e),h($e.$$.fragment,e),vs=o(e),h(se.$$.fragment,e),Bs=o(e),xe=b(e,"P",{"data-svelte-h":!0}),f(xe)!=="svelte-1vbvikx"&&(xe.innerHTML=un),Zs=o(e),h(Ge.$$.fragment,e),$s=o(e),Ce=b(e,"P",{"data-svelte-h":!0}),f(Ce)!=="svelte-o78gqr"&&(Ce.textContent=dn),xs=o(e),h(We.$$.fragment,e),Gs=o(e),h(Re.$$.fragment,e),Cs=o(e),Ne=b(e,"P",{"data-svelte-h":!0}),f(Ne)!=="svelte-erpa1c"&&(Ne.textContent=wn),Ws=o(e),h(le.$$.fragment,e),Rs=o(e),h(Ve.$$.fragment,e),Ns=o(e),Xe=b(e,"P",{"data-svelte-h":!0}),f(Xe)!=="svelte-1ny8u7i"&&(Xe.innerHTML=yn),Vs=o(e),h(ze.$$.fragment,e),Xs=o(e),h(He.$$.fragment,e),zs=o(e),Qe=b(e,"P",{"data-svelte-h":!0}),f(Qe)!=="svelte-1vshog5"&&(Qe.innerHTML=Jn),Hs=o(e),h(Ee.$$.fragment,e),Qs=o(e),h(Fe.$$.fragment,e),Es=o(e),Ye=b(e,"P",{"data-svelte-h":!0}),f(Ye)!=="svelte-cfs4jm"&&(Ye.innerHTML=bn),Fs=o(e),Se=b(e,"P",{"data-svelte-h":!0}),f(Se)!=="svelte-1nms3xo"&&(Se.innerHTML=Tn),Ys=o(e),h(Ae.$$.fragment,e),Ss=o(e),h(qe.$$.fragment,e),As=o(e),Le=b(e,"P",{"data-svelte-h":!0}),f(Le)!=="svelte-1qc9ozy"&&(Le.innerHTML=fn),qs=o(e),De=b(e,"P",{"data-svelte-h":!0}),f(De)!=="svelte-1f82rb6"&&(De.textContent=Un),Ls=o(e),h(Pe.$$.fragment,e),Ds=o(e),h(Ke.$$.fragment,e),Ps=o(e),Oe=b(e,"P",{"data-svelte-h":!0}),f(Oe)!=="svelte-k27ihj"&&(Oe.innerHTML=jn),Ks=o(e),h(et.$$.fragment,e),Os=o(e),h(tt.$$.fragment,e),el=o(e),st=b(e,"P",{"data-svelte-h":!0}),f(st)!=="svelte-w7y7n8"&&(st.textContent=In),tl=o(e),h(lt.$$.fragment,e),sl=o(e),nt=b(e,"P",{"data-svelte-h":!0}),f(nt)!=="svelte-b6k3qh"&&(nt.textContent=kn),ll=o(e),h(at.$$.fragment,e),nl=o(e),it=b(e,"P",{"data-svelte-h":!0}),f(it)!=="svelte-dqom4c"&&(it.textContent=_n),al=o(e),rt=b(e,"P",{"data-svelte-h":!0}),f(rt)!=="svelte-vpesag"&&(rt.innerHTML=gn),il=o(e),h(ot.$$.fragment,e),rl=o(e),mt=b(e,"P",{"data-svelte-h":!0}),f(mt)!=="svelte-8qj5sv"&&(mt.innerHTML=vn),ol=o(e),F.l(e),St=o(e),pt=b(e,"P",{"data-svelte-h":!0}),f(pt)!=="svelte-1rmefba"&&(pt.textContent=Bn),ml=o(e),h(ct.$$.fragment,e),pl=o(e),S.l(e),At=o(e),Mt=b(e,"P",{"data-svelte-h":!0}),f(Mt)!=="svelte-12rpmqq"&&(Mt.textContent=Zn),cl=o(e),q.l(e),qt=o(e),ht=b(e,"P",{"data-svelte-h":!0}),f(ht)!=="svelte-xc3z5h"&&(ht.textContent=$n),Ml=o(e),D.l(e),Lt=o(e),ut=b(e,"P",{"data-svelte-h":!0}),f(ut)!=="svelte-t87wk3"&&(ut.textContent=xn),hl=o(e),K.l(e),Dt=o(e),h(dt.$$.fragment,e),ul=o(e),wt=b(e,"P",{"data-svelte-h":!0}),f(wt)!=="svelte-s4nw2c"&&(wt.textContent=Gn),dl=o(e),h(ne.$$.fragment,e),wl=o(e),yt=b(e,"P",{"data-svelte-h":!0}),f(yt)!=="svelte-1ocnhs0"&&(yt.innerHTML=Cn),yl=o(e),h(Jt.$$.fragment,e),Jl=o(e),h(bt.$$.fragment,e),bl=o(e),Tt=b(e,"P",{"data-svelte-h":!0}),f(Tt)!=="svelte-k6zbjj"&&(Tt.textContent=Wn),Tl=o(e),h(ae.$$.fragment,e),fl=o(e),ft=b(e,"P",{"data-svelte-h":!0}),f(ft)!=="svelte-1p6apew"&&(ft.textContent=Rn),Ul=o(e),Pt=b(e,"P",{}),Il(Pt).forEach(n),this.h()},h(){Sl(s,"name","hf:doc:metadata"),Sl(s,"content",Ka),Sl(te,"class","flex justify-center"),Js.a=null,Ts.a=fs},m(e,l){_(document.head,s),a(e,i,l),a(e,t,l),a(e,c,l),u(y,e,l),a(e,U,l),u(j,e,l),a(e,Kt,l),Ut[B].m(e,l),a(e,Ct,l),a(e,oe,l),a(e,Ot,l),jt[$].m(e,l),a(e,Wt,l),u(me,e,l),a(e,es,l),a(e,pe,l),a(e,ts,l),u(ce,e,l),a(e,ss,l),u(Me,e,l),a(e,ls,l),a(e,he,l),a(e,ns,l),u(ue,e,l),a(e,as,l),u(de,e,l),a(e,is,l),a(e,we,l),a(e,rs,l),u(ye,e,l),a(e,os,l),a(e,Je,l),a(e,ms,l),It[G].m(e,l),a(e,Rt,l),a(e,be,l),a(e,ps,l),a(e,te,l),a(e,cs,l),a(e,Te,l),a(e,Ms,l),u(fe,e,l),a(e,hs,l),kt[W].m(e,l),a(e,Nt,l),a(e,Ue,l),a(e,us,l),a(e,je,l),a(e,ds,l),_t[N].m(e,l),a(e,Vt,l),a(e,Ie,l),a(e,ws,l),gt[X].m(e,l),a(e,Xt,l),a(e,ke,l),a(e,ys,l),a(e,I,l),_(I,vl),_(I,zt),_(I,Bl),_(I,Ht),_(I,Zl),_(I,Qt),_(I,$l),_(I,Et),_(I,xl),Js.m(da,I),a(e,bs,l),a(e,O,l),_(O,Gl),Ts.m(wa,O),_(O,fs),_(O,Ft),_(O,Cl),a(e,Us,l),a(e,_e,l),a(e,js,l),u(ge,e,l),a(e,Is,l),vt[H].m(e,l),a(e,Yt,l),a(e,ve,l),a(e,ks,l),u(Be,e,l),a(e,_s,l),a(e,Ze,l),a(e,gs,l),u($e,e,l),a(e,vs,l),u(se,e,l),a(e,Bs,l),a(e,xe,l),a(e,Zs,l),u(Ge,e,l),a(e,$s,l),a(e,Ce,l),a(e,xs,l),u(We,e,l),a(e,Gs,l),u(Re,e,l),a(e,Cs,l),a(e,Ne,l),a(e,Ws,l),u(le,e,l),a(e,Rs,l),u(Ve,e,l),a(e,Ns,l),a(e,Xe,l),a(e,Vs,l),u(ze,e,l),a(e,Xs,l),u(He,e,l),a(e,zs,l),a(e,Qe,l),a(e,Hs,l),u(Ee,e,l),a(e,Qs,l),u(Fe,e,l),a(e,Es,l),a(e,Ye,l),a(e,Fs,l),a(e,Se,l),a(e,Ys,l),u(Ae,e,l),a(e,Ss,l),u(qe,e,l),a(e,As,l),a(e,Le,l),a(e,qs,l),a(e,De,l),a(e,Ls,l),u(Pe,e,l),a(e,Ds,l),u(Ke,e,l),a(e,Ps,l),a(e,Oe,l),a(e,Ks,l),u(et,e,l),a(e,Os,l),u(tt,e,l),a(e,el,l),a(e,st,l),a(e,tl,l),u(lt,e,l),a(e,sl,l),a(e,nt,l),a(e,ll,l),u(at,e,l),a(e,nl,l),a(e,it,l),a(e,al,l),a(e,rt,l),a(e,il,l),u(ot,e,l),a(e,rl,l),a(e,mt,l),a(e,ol,l),Bt[E].m(e,l),a(e,St,l),a(e,pt,l),a(e,ml,l),u(ct,e,l),a(e,pl,l),Zt[Y].m(e,l),a(e,At,l),a(e,Mt,l),a(e,cl,l),$t[A].m(e,l),a(e,qt,l),a(e,ht,l),a(e,Ml,l),xt[L].m(e,l),a(e,Lt,l),a(e,ut,l),a(e,hl,l),Gt[P].m(e,l),a(e,Dt,l),u(dt,e,l),a(e,ul,l),a(e,wt,l),a(e,dl,l),u(ne,e,l),a(e,wl,l),a(e,yt,l),a(e,yl,l),u(Jt,e,l),a(e,Jl,l),u(bt,e,l),a(e,bl,l),a(e,Tt,l),a(e,Tl,l),u(ae,e,l),a(e,fl,l),a(e,ft,l),a(e,Ul,l),a(e,Pt,l),jl=!0},p(e,[l]){const k={};l&1&&(k.fw=e[0]),y.$set(k);let ee=B;B=Vn(e),B!==ee&&(v(),m(Ut[ee],1,1,()=>{Ut[ee]=null}),g(),Z=Ut[B],Z||(Z=Ut[B]=Nn[B](e),Z.c()),p(Z,1),Z.m(Ct.parentNode,Ct));let Wl=$;$=zn(e),$!==Wl&&(v(),m(jt[Wl],1,1,()=>{jt[Wl]=null}),g(),x=jt[$],x||(x=jt[$]=Xn[$](e),x.c()),p(x,1),x.m(Wt.parentNode,Wt));let Rl=G;G=Qn(e),G!==Rl&&(v(),m(It[Rl],1,1,()=>{It[Rl]=null}),g(),C=It[G],C||(C=It[G]=Hn[G](e),C.c()),p(C,1),C.m(Rt.parentNode,Rt));let Nl=W;W=Fn(e),W!==Nl&&(v(),m(kt[Nl],1,1,()=>{kt[Nl]=null}),g(),R=kt[W],R||(R=kt[W]=En[W](e),R.c()),p(R,1),R.m(Nt.parentNode,Nt));let Vl=N;N=Sn(e),N!==Vl&&(v(),m(_t[Vl],1,1,()=>{_t[Vl]=null}),g(),V=_t[N],V||(V=_t[N]=Yn[N](e),V.c()),p(V,1),V.m(Vt.parentNode,Vt));let Xl=X;X=qn(e),X!==Xl&&(v(),m(gt[Xl],1,1,()=>{gt[Xl]=null}),g(),z=gt[X],z||(z=gt[X]=An[X](e),z.c()),p(z,1),z.m(Xt.parentNode,Xt));let zl=H;H=Dn(e),H!==zl&&(v(),m(vt[zl],1,1,()=>{vt[zl]=null}),g(),Q=vt[H],Q||(Q=vt[H]=Ln[H](e),Q.c()),p(Q,1),Q.m(Yt.parentNode,Yt));const ra={};l&2&&(ra.$$scope={dirty:l,ctx:e}),se.$set(ra);const oa={};l&2&&(oa.$$scope={dirty:l,ctx:e}),le.$set(oa);let Hl=E;E=Kn(e),E!==Hl&&(v(),m(Bt[Hl],1,1,()=>{Bt[Hl]=null}),g(),F=Bt[E],F||(F=Bt[E]=Pn[E](e),F.c()),p(F,1),F.m(St.parentNode,St));let Ql=Y;Y=ea(e),Y!==Ql&&(v(),m(Zt[Ql],1,1,()=>{Zt[Ql]=null}),g(),S=Zt[Y],S||(S=Zt[Y]=On[Y](e),S.c()),p(S,1),S.m(At.parentNode,At));let El=A;A=sa(e),A!==El&&(v(),m($t[El],1,1,()=>{$t[El]=null}),g(),q=$t[A],q||(q=$t[A]=ta[A](e),q.c()),p(q,1),q.m(qt.parentNode,qt));let Fl=L;L=na(e),L!==Fl&&(v(),m(xt[Fl],1,1,()=>{xt[Fl]=null}),g(),D=xt[L],D||(D=xt[L]=la[L](e),D.c()),p(D,1),D.m(Lt.parentNode,Lt));let Yl=P;P=ia(e),P!==Yl&&(v(),m(Gt[Yl],1,1,()=>{Gt[Yl]=null}),g(),K=Gt[P],K||(K=Gt[P]=aa[P](e),K.c()),p(K,1),K.m(Dt.parentNode,Dt));const ma={};l&2&&(ma.$$scope={dirty:l,ctx:e}),ne.$set(ma);const pa={};l&2&&(pa.$$scope={dirty:l,ctx:e}),ae.$set(pa)},i(e){jl||(p(y.$$.fragment,e),p(j.$$.fragment,e),p(Z),p(x),p(me.$$.fragment,e),p(ce.$$.fragment,e),p(Me.$$.fragment,e),p(ue.$$.fragment,e),p(de.$$.fragment,e),p(ye.$$.fragment,e),p(C),p(fe.$$.fragment,e),p(R),p(V),p(z),p(ge.$$.fragment,e),p(Q),p(Be.$$.fragment,e),p($e.$$.fragment,e),p(se.$$.fragment,e),p(Ge.$$.fragment,e),p(We.$$.fragment,e),p(Re.$$.fragment,e),p(le.$$.fragment,e),p(Ve.$$.fragment,e),p(ze.$$.fragment,e),p(He.$$.fragment,e),p(Ee.$$.fragment,e),p(Fe.$$.fragment,e),p(Ae.$$.fragment,e),p(qe.$$.fragment,e),p(Pe.$$.fragment,e),p(Ke.$$.fragment,e),p(et.$$.fragment,e),p(tt.$$.fragment,e),p(lt.$$.fragment,e),p(at.$$.fragment,e),p(ot.$$.fragment,e),p(F),p(ct.$$.fragment,e),p(S),p(q),p(D),p(K),p(dt.$$.fragment,e),p(ne.$$.fragment,e),p(Jt.$$.fragment,e),p(bt.$$.fragment,e),p(ae.$$.fragment,e),jl=!0)},o(e){m(y.$$.fragment,e),m(j.$$.fragment,e),m(Z),m(x),m(me.$$.fragment,e),m(ce.$$.fragment,e),m(Me.$$.fragment,e),m(ue.$$.fragment,e),m(de.$$.fragment,e),m(ye.$$.fragment,e),m(C),m(fe.$$.fragment,e),m(R),m(V),m(z),m(ge.$$.fragment,e),m(Q),m(Be.$$.fragment,e),m($e.$$.fragment,e),m(se.$$.fragment,e),m(Ge.$$.fragment,e),m(We.$$.fragment,e),m(Re.$$.fragment,e),m(le.$$.fragment,e),m(Ve.$$.fragment,e),m(ze.$$.fragment,e),m(He.$$.fragment,e),m(Ee.$$.fragment,e),m(Fe.$$.fragment,e),m(Ae.$$.fragment,e),m(qe.$$.fragment,e),m(Pe.$$.fragment,e),m(Ke.$$.fragment,e),m(et.$$.fragment,e),m(tt.$$.fragment,e),m(lt.$$.fragment,e),m(at.$$.fragment,e),m(ot.$$.fragment,e),m(F),m(ct.$$.fragment,e),m(S),m(q),m(D),m(K),m(dt.$$.fragment,e),m(ne.$$.fragment,e),m(Jt.$$.fragment,e),m(bt.$$.fragment,e),m(ae.$$.fragment,e),jl=!1},d(e){e&&(n(i),n(t),n(c),n(U),n(Kt),n(Ct),n(oe),n(Ot),n(Wt),n(es),n(pe),n(ts),n(ss),n(ls),n(he),n(ns),n(as),n(is),n(we),n(rs),n(os),n(Je),n(ms),n(Rt),n(be),n(ps),n(te),n(cs),n(Te),n(Ms),n(hs),n(Nt),n(Ue),n(us),n(je),n(ds),n(Vt),n(Ie),n(ws),n(Xt),n(ke),n(ys),n(I),n(bs),n(O),n(Us),n(_e),n(js),n(Is),n(Yt),n(ve),n(ks),n(_s),n(Ze),n(gs),n(vs),n(Bs),n(xe),n(Zs),n($s),n(Ce),n(xs),n(Gs),n(Cs),n(Ne),n(Ws),n(Rs),n(Ns),n(Xe),n(Vs),n(Xs),n(zs),n(Qe),n(Hs),n(Qs),n(Es),n(Ye),n(Fs),n(Se),n(Ys),n(Ss),n(As),n(Le),n(qs),n(De),n(Ls),n(Ds),n(Ps),n(Oe),n(Ks),n(Os),n(el),n(st),n(tl),n(sl),n(nt),n(ll),n(nl),n(it),n(al),n(rt),n(il),n(rl),n(mt),n(ol),n(St),n(pt),n(ml),n(pl),n(At),n(Mt),n(cl),n(qt),n(ht),n(Ml),n(Lt),n(ut),n(hl),n(Dt),n(ul),n(wt),n(dl),n(wl),n(yt),n(yl),n(Jl),n(bl),n(Tt),n(Tl),n(fl),n(ft),n(Ul),n(Pt)),n(s),d(y,e),d(j,e),Ut[B].d(e),jt[$].d(e),d(me,e),d(ce,e),d(Me,e),d(ue,e),d(de,e),d(ye,e),It[G].d(e),d(fe,e),kt[W].d(e),_t[N].d(e),gt[X].d(e),d(ge,e),vt[H].d(e),d(Be,e),d($e,e),d(se,e),d(Ge,e),d(We,e),d(Re,e),d(le,e),d(Ve,e),d(ze,e),d(He,e),d(Ee,e),d(Fe,e),d(Ae,e),d(qe,e),d(Pe,e),d(Ke,e),d(et,e),d(tt,e),d(lt,e),d(at,e),d(ot,e),Bt[E].d(e),d(ct,e),Zt[Y].d(e),$t[A].d(e),xt[L].d(e),Gt[P].d(e),d(dt,e),d(ne,e),d(Jt,e),d(bt,e),d(ae,e)}}}const Ka='{"title":"Fast tokenizers in the QA pipeline","local":"fast-tokenizers-in-the-qa-pipeline","sections":[{"title":"Using the question-answering pipeline","local":"using-the-question-answering-pipeline","sections":[],"depth":2},{"title":"Using a model for question answering","local":"using-a-model-for-question-answering","sections":[],"depth":2},{"title":"Handling long contexts","local":"handling-long-contexts","sections":[],"depth":2}],"depth":1}';function Oa(w,s,i){let t="pt";return Ja(()=>{const c=new URLSearchParams(window.location.search);i(0,t=c.get("fw")||"pt")}),[t]}class oi extends ba{constructor(s){super(),Ta(this,s,Oa,Pa,ya,{})}}export{oi as component};
