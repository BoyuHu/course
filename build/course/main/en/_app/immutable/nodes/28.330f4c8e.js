import{s as Jn,o as Kn,n as Xn}from"../chunks/scheduler.37c15a92.js";import{S as Zn,i as ei,g as o,s as a,r as f,A as ti,h as s,f as n,c as r,j as Qn,u as p,x as l,k as Nn,y as ni,a as i,v as h,d as m,t as u,w as d,m as ii,n as ai}from"../chunks/index.2bf4358c.js";import{T as Vn}from"../chunks/Tip.363c041f.js";import{Q as ze}from"../chunks/Question.668688bc.js";import{H as c}from"../chunks/Heading.8ada512a.js";function ri(qe){let g;return{c(){g=ii("In this chapter, we are focusing on reinforcement learning for language models. However, reinforcement learning is a broad field with many applications beyond language models. If you're interested in learning more about reinforcement learning, you should check out the [Deep Reinforcement Learning course](https://huggingface.co/courses/deep-rl-course/en/unit1/introduction).")},l(v){g=ai(v,"In this chapter, we are focusing on reinforcement learning for language models. However, reinforcement learning is a broad field with many applications beyond language models. If you're interested in learning more about reinforcement learning, you should check out the [Deep Reinforcement Learning course](https://huggingface.co/courses/deep-rl-course/en/unit1/introduction).")},m(v,$){i(v,g,$)},d(v){v&&n(g)}}}function oi(qe){let g,v="DPO and PPO are complex reinforcement learning algorithms in their own right, which we will not cover in this course. If you’re interested in learning more about them, you can check out the following resources:",$,x,y='<li><a href="https://huggingface.co/docs/trl/main/en/ppo_trainer" rel="nofollow">Proximal Policy Optimization</a></li> <li><a href="https://huggingface.co/docs/trl/main/en/dpo_trainer" rel="nofollow">Direct Preference Optimization</a></li>';return{c(){g=o("p"),g.textContent=v,$=a(),x=o("ul"),x.innerHTML=y},l(w){g=s(w,"P",{"data-svelte-h":!0}),l(g)!=="svelte-1irn41k"&&(g.textContent=v),$=r(w),x=s(w,"UL",{"data-svelte-h":!0}),l(x)!=="svelte-1hp1bbe"&&(x.innerHTML=y)},m(w,L){i(w,g,L),i(w,$,L),i(w,x,L)},p:Xn,d(w){w&&(n(g),n($),n(x))}}}function si(qe){let g,v,$,x,y,w,L,en="Welcome to the first page!",je,R,tn="We’re going to start our journey into the exciting world of Reinforcement Learning (RL) and discover how it’s revolutionizing the way we train Language Models like the ones you might use every day.",Ae,b,Ge,P,nn="This page will give you a friendly and clear introduction to RL, even if you’ve never encountered it before. We’ll break down the core ideas and see why RL is becoming so important in the field of Large Language Models (LLMs).",De,T,Ee,k,an="Imagine you’re training a dog. You want to teach it to sit. You might say “Sit!” and then, if the dog sits, you give it a treat and praise. If it doesn’t sit, you might gently guide it or just try again. Over time, the dog learns to associate sitting with the positive reward (treat and praise) and is more likely to sit when you say “Sit!” again. In reinforcement learning, we refer to this feedback as a <strong>reward</strong>.",Se,M,rn="That, in a nutshell, is the basic idea behind Reinforcement Learning! Instead of a dog, we have a <strong>language model</strong> (in reinforcement learning, we call it an <strong>agent</strong>), and instead of you, we have the <strong>environment</strong> that gives feedback.",Ue,_,on='<img src="https://huggingface.co/reasoning-course/images/resolve/main/grpo/3.jpg" alt="RL terms Process"/>',Be,H,sn="Let’s break down the key pieces of RL:",Ye,O,Qe,F,ln="This is our learner. In the dog example, the dog is the agent. In the context of LLMs, the LLM itself becomes the agent we want to train. The agent is the one making decisions and learning from the environment and its rewards.",Ne,I,Ve,q,fn="This is the world the agent lives in and interacts with. For the dog, the environment is your house and you. For an LLM, the environment is a bit more abstract – it could be the users it interacts with, or a simulated scenario we set up for it. The environment provides feedback to the agent.",Je,W,Ke,z,pn="These are the choices the agent can make in the environment. The dog’s actions are things like “sit”, “stand”, “bark”, etc. For an LLM, actions could be generating words in a sentence, choosing which answer to give to a question, or deciding how to respond in a conversation.",Xe,j,Ze,A,hn="This is the feedback the environment gives to the agent after it takes an action. Rewards are usually numbers.",et,G,mn="<strong>Positive rewards</strong> are like treats and praise – they tell the agent “good job, you did something right!“.",tt,D,un="<strong>Negative rewards</strong> (or penalties) are like a gentle “no” – they tell the agent “that wasn’t quite right, try something else”. For the dog, the treat is the reward.",nt,E,dn="For an LLM, rewards are designed to reflect how well the LLM is doing at a specific task – maybe it’s how helpful, truthful, or harmless its response is.",it,S,at,U,gn="This is the agent’s strategy for choosing actions. It’s like the dog’s understanding of what it should do when you say “Sit!“. In RL, the policy is what we’re really trying to learn and improve. It’s a set of rules or a function that tells the agent what action to take in different situations. Initially, the policy might be random, but as the agent learns, the policy becomes better at choosing actions that lead to higher rewards.",rt,B,ot,Y,cn='<img src="https://huggingface.co/reasoning-course/images/resolve/main/grpo/1.jpg" alt="RL Process"/>',st,Q,wn="Reinforcement Learning happens through a process of trial and error:",lt,N,vn="<thead><tr><th>Step</th> <th>Process</th> <th>Description</th></tr></thead> <tbody><tr><td>1. Observation</td> <td>The agent observes the environment</td> <td>The agent takes in information about its current state and surroundings</td></tr> <tr><td>2. Action</td> <td>The agent takes an action based on its current policy</td> <td>Using its learned strategy (policy), the agent decides what to do next</td></tr> <tr><td>3. Feedback</td> <td>The environment gives the agent a reward</td> <td>The agent receives feedback on how good or bad its action was</td></tr> <tr><td>4. Learning</td> <td>The agent updates its policy based on the reward</td> <td>The agent adjusts its strategy - reinforcing actions that led to high rewards and avoiding those that led low rewards</td></tr> <tr><td>5. Iteration</td> <td>Repeat the process</td> <td>This cycle continues, allowing the agent to continuously improve its decision-making</td></tr></tbody>",ft,V,$n="Think about learning to ride a bike. You might wobble and fall at first (negative reward!). But when you manage to balance and pedal smoothly, you feel good (positive reward!). You adjust your actions based on this feedback – leaning slightly, pedaling faster, etc. – until you learn to ride well. RL is similar – it’s about learning through interaction and feedback.",pt,J,ht,K,xn="Now, why is RL so important for Large Language Models?",mt,X,Ln='Well, training really good LLMs is tricky. We can train them on massive amounts of text from the internet, and they become very good at predicting the next word in a sentence. This is how they learn to generate fluent and grammatically correct text, as we learned in <a href="/chapters/en/chapter2/1">chapter 2</a>.',ut,Z,yn="However, just being fluent isn’t enough. We want our LLMs to be more than just good at stringing words together. We want them to be:",dt,ee,bn="<li><strong>Helpful:</strong> Provide useful and relevant information.</li> <li><strong>Harmless:</strong> Avoid generating toxic, biased, or harmful content.</li> <li><strong>Aligned with Human Preferences:</strong> Respond in ways that humans find natural, helpful, and engaging.</li>",gt,te,Cn="Pre-training LLM methods, which mostly rely on predicting the next word from text data, sometimes fall short on these aspects.",ct,ne,Rn='Whilst supervised training is excellent at producing structured outputs, it can be less effective at producing helpful, harmless, and aligned responses. We explore supervised training in <a href="/chapters/en/chapter11/1">chapter 11</a>.',wt,ie,Pn="Fine-tuned models might generate fluent and structured text that is still factually incorrect, biased, or doesn’t really answer the user’s question in a helpful way.",vt,ae,Tn="<strong>Enter Reinforcement Learning!</strong> RL gives us a way to fine-tune these pre-trained LLMs to better achieve these desired qualities. It’s like giving our LLM dog extra training to become a well-behaved and helpful companion, not just a dog that knows how to bark fluently!",$t,re,xt,oe,kn="A very popular technique for aligning language models is <strong>Reinforcement Learning from Human Feedback (RLHF)</strong>. In RLHF, we use human feedback as a proxy for the “reward” signal in RL. Here’s how it works:",Lt,se,Mn="<li><p><strong>Get Human Preferences:</strong> We might ask humans to compare different responses generated by the LLM for the same input prompt and tell us which response they prefer. For example, we might show a human two different answers to the question “What is the capital of France?” and ask them “Which answer is better?“.</p></li> <li><p><strong>Train a Reward Model:</strong> We use this human preference data to train a separate model called a <strong>reward model</strong>. This reward model learns to predict what kind of responses humans will prefer. It learns to score responses based on helpfulness, harmlessness, and alignment with human preferences.</p></li> <li><p><strong>Fine-tune the LLM with RL:</strong> Now we use the reward model as the environment for our LLM agent. The LLM generates responses (actions), and the reward model scores these responses (provides rewards). In essence, we’re training the LLM to produce text that our reward model (which learned from human preferences) thinks is good.</p></li>",yt,le,_n='<img src="https://huggingface.co/reasoning-course/images/resolve/main/grpo/2.jpg" alt="RL Basic Concept"/>',bt,fe,Hn="From a general perspective, let’s look at the benefits of using RL in LLMs:",Ct,pe,On="<thead><tr><th>Benefit</th> <th>Description</th></tr></thead> <tbody><tr><td>Improved Control</td> <td>RL allows us to have more control over the kind of text LLMs generate. We can guide them to produce text that is more aligned with specific goals, like being helpful, creative, or concise.</td></tr> <tr><td>Enhanced Alignment with Human Values</td> <td>RLHF, in particular, helps us align LLMs with complex and often subjective human preferences. It’s hard to write down rules for “what makes a good answer,” but humans can easily judge and compare responses. RLHF lets the model learn from these human judgments.</td></tr> <tr><td>Mitigating Undesirable Behaviors</td> <td>RL can be used to reduce negative behaviors in LLMs, such as generating toxic language, spreading misinformation, or exhibiting biases. By designing rewards that penalize these behaviors, we can nudge the model to avoid them.</td></tr></tbody>",Rt,he,Fn="Reinforcement Learning from Human Feedback has been used to train many of the most popular LLMs today, such as OpenAI’s GPT-4, Google’s Gemini, and DeepSeek’s R1. There are a wide range of techniques for RLHF, with varying degrees of complexity and sophistication. In this chapter, we will focus on Group Relative Policy Optimization (GRPO), which is a technique for RLHF that has been shown to be effective at training LLMs that are helpful, harmless, and aligned with human preferences.",Pt,me,Tt,ue,In="There are many techniques for RLHF but this course is focused on GRPO because it represents a significant advancement in reinforcement learning for language models.",kt,de,qn="Let’s briefly consider two of other popular techniques for RLHF:",Mt,ge,Wn="<li>Proximal Policy Optimization (PPO)</li> <li>Direct Preference Optimization (DPO)</li>",_t,ce,zn="Proximal Policy Optimization (PPO) was one of the first highly effective techniques for RLHF. It uses a policy gradient method to update the policy based on the reward from a separate reward model.",Ht,we,jn="Direct Preference Optimization (DPO) was later developed as a simpler technique that eliminates the need for a separate reward model using preference data directly. Essentially, framing the problem as a classification task between the chosen and rejected responses.",Ot,C,Ft,ve,An="Unlike DPO and PPO, GRPO groups similar samples together and compares them as a group. The group-based approach provides more stable gradients and better convergence properties compared to other methods.",It,$e,Gn="GRPO does not use preference data like DPO, but instead compares groups of similar samples using a reward signal from a model or function.",qt,xe,Dn="GRPO is flexible in how it obtains reward signals - it can work with a reward model (like PPO does) but doesn’t strictly require one. This is because GRPO can incorporate reward signals from any function or model that can evaluate the quality of responses.",Wt,Le,En="For example, we could use a length function to reward shorter responses, a mathematical solver to verify solution correctness, or a factual correctness function to reward responses that are more factually accurate. This flexibility makes GRPO particularly versatile for different types of alignment tasks.",zt,jt,At,ye,Sn="Congratulations on completing Module 1! You’ve now got a solid introduction to Reinforcement Learning and its crucial role in shaping the future of Large Language Models. You understand the basic concepts of RL, why it’s used for LLMs, and you’ve been introduced to GRPO, a key algorithm in this field.",Gt,be,Un="In the next module, we’ll get our hands dirty and dive into the DeepSeek R1 paper to see these concepts in action!",Dt,Ce,Et,Re,St,Pe,Ut,Te,Bt,ke,Yt,Me,Qt,_e,Nt,He,Vt,Oe,Jt,Fe,Kt,Ie,Xt,We,Zt;return y=new c({props:{title:"Introduction to Reinforcement Learning and its Role in LLMs",local:"introduction-to-reinforcement-learning-and-its-role-in-llms",headingTag:"h1"}}),b=new Vn({props:{$$slots:{default:[ri]},$$scope:{ctx:qe}}}),T=new c({props:{title:"What is Reinforcement Learning (RL)?",local:"what-is-reinforcement-learning-rl",headingTag:"h2"}}),O=new c({props:{title:"Agent",local:"agent",headingTag:"h3"}}),I=new c({props:{title:"Environment",local:"environment",headingTag:"h3"}}),W=new c({props:{title:"Action",local:"action",headingTag:"h3"}}),j=new c({props:{title:"Reward",local:"reward",headingTag:"h3"}}),S=new c({props:{title:"Policy",local:"policy",headingTag:"h3"}}),B=new c({props:{title:"The RL Process: Trial and Error",local:"the-rl-process-trial-and-error",headingTag:"h2"}}),J=new c({props:{title:"Role of RL in Large Language Models (LLMs)",local:"role-of-rl-in-large-language-models-llms",headingTag:"h2"}}),re=new c({props:{title:"Reinforcement Learning from Human Feedback (RLHF)",local:"reinforcement-learning-from-human-feedback-rlhf",headingTag:"h2"}}),me=new c({props:{title:"Why should we care about GRPO (Group Relative Policy Optimization)?",local:"why-should-we-care-about-grpo-group-relative-policy-optimization",headingTag:"h2"}}),C=new Vn({props:{$$slots:{default:[oi]},$$scope:{ctx:qe}}}),Ce=new c({props:{title:"Quiz",local:"quiz",headingTag:"h2"}}),Re=new c({props:{title:"1. What are the key components of Reinforcement Learning?",local:"1-what-are-the-key-components-of-reinforcement-learning",headingTag:"h3"}}),Pe=new ze({props:{choices:[{text:"Agent, Environment, Action, Reward, and Policy",explain:"Correct! These are the fundamental components that make up a reinforcement learning system.",correct:!0},{text:"Model, Data, Loss Function, and Optimizer",explain:"These are components more commonly associated with supervised learning."},{text:"Input, Output, and Hidden Layers",explain:"These are components of neural network architecture, not specifically RL components."}]}}),Te=new c({props:{title:"2. What is the main advantage of RLHF for training language models?",local:"2-what-is-the-main-advantage-of-rlhf-for-training-language-models",headingTag:"h3"}}),ke=new ze({props:{choices:[{text:"It helps align models with human preferences and values",explain:"Correct! RLHF uses human feedback to guide models toward more helpful, harmless, and aligned behavior.",correct:!0},{text:"It makes models generate text faster",explain:"RLHF isn't primarily about improving generation speed."},{text:"It reduces the model's memory usage",explain:"RLHF doesn't focus on model efficiency or memory optimization."}]}}),Me=new c({props:{title:"3. In the context of RL for LLMs, what represents an “action”?",local:"3-in-the-context-of-rl-for-llms-what-represents-an-action",headingTag:"h3"}}),_e=new ze({props:{choices:[{text:"Generating words or choosing responses in a conversation",explain:"Correct! For LLMs, actions typically involve text generation decisions.",correct:!0},{text:"Updating model weights",explain:"This is part of the training process, not an action in the RL context."},{text:"Processing input tokens",explain:"This is part of the model's operation, not an action in the RL context."}]}}),He=new c({props:{title:"4. What is the role of the reward in RL training of language models?",local:"4-what-is-the-role-of-the-reward-in-rl-training-of-language-models",headingTag:"h3"}}),Oe=new ze({props:{choices:[{text:"To provide feedback on how well the model's responses align with desired behavior",explain:"Correct! Rewards guide the model toward generating more helpful, truthful, and appropriate responses.",correct:!0},{text:"To measure the model's vocabulary size",explain:"Rewards aren't used to evaluate vocabulary knowledge."},{text:"To determine the model's training speed",explain:"Rewards provide feedback on response quality, not training efficiency."}]}}),Fe=new c({props:{title:"5. What is a reward in the context of RL for LLMs?",local:"5-what-is-a-reward-in-the-context-of-rl-for-llms",headingTag:"h3"}}),Ie=new ze({props:{choices:[{text:"A numerical score that measures the quality of a response",explain:"Correct! Rewards provide feedback on response quality, guiding the model toward desired behavior.",correct:!0},{text:"A function that generates responses",explain:"Rewards are feedback on response quality, not the generation process itself."},{text:"A model that evaluates the quality of responses",explain:"Rewards are feedback on response quality, not an evaluation model."}]}}),{c(){g=o("meta"),v=a(),$=o("p"),x=a(),f(y.$$.fragment),w=a(),L=o("p"),L.textContent=en,je=a(),R=o("p"),R.textContent=tn,Ae=a(),f(b.$$.fragment),Ge=a(),P=o("p"),P.textContent=nn,De=a(),f(T.$$.fragment),Ee=a(),k=o("p"),k.innerHTML=an,Se=a(),M=o("p"),M.innerHTML=rn,Ue=a(),_=o("p"),_.innerHTML=on,Be=a(),H=o("p"),H.textContent=sn,Ye=a(),f(O.$$.fragment),Qe=a(),F=o("p"),F.textContent=ln,Ne=a(),f(I.$$.fragment),Ve=a(),q=o("p"),q.textContent=fn,Je=a(),f(W.$$.fragment),Ke=a(),z=o("p"),z.textContent=pn,Xe=a(),f(j.$$.fragment),Ze=a(),A=o("p"),A.textContent=hn,et=a(),G=o("p"),G.innerHTML=mn,tt=a(),D=o("p"),D.innerHTML=un,nt=a(),E=o("p"),E.textContent=dn,it=a(),f(S.$$.fragment),at=a(),U=o("p"),U.textContent=gn,rt=a(),f(B.$$.fragment),ot=a(),Y=o("p"),Y.innerHTML=cn,st=a(),Q=o("p"),Q.textContent=wn,lt=a(),N=o("table"),N.innerHTML=vn,ft=a(),V=o("p"),V.textContent=$n,pt=a(),f(J.$$.fragment),ht=a(),K=o("p"),K.textContent=xn,mt=a(),X=o("p"),X.innerHTML=Ln,ut=a(),Z=o("p"),Z.textContent=yn,dt=a(),ee=o("ul"),ee.innerHTML=bn,gt=a(),te=o("p"),te.textContent=Cn,ct=a(),ne=o("p"),ne.innerHTML=Rn,wt=a(),ie=o("p"),ie.textContent=Pn,vt=a(),ae=o("p"),ae.innerHTML=Tn,$t=a(),f(re.$$.fragment),xt=a(),oe=o("p"),oe.innerHTML=kn,Lt=a(),se=o("ol"),se.innerHTML=Mn,yt=a(),le=o("p"),le.innerHTML=_n,bt=a(),fe=o("p"),fe.textContent=Hn,Ct=a(),pe=o("table"),pe.innerHTML=On,Rt=a(),he=o("p"),he.textContent=Fn,Pt=a(),f(me.$$.fragment),Tt=a(),ue=o("p"),ue.textContent=In,kt=a(),de=o("p"),de.textContent=qn,Mt=a(),ge=o("ul"),ge.innerHTML=Wn,_t=a(),ce=o("p"),ce.textContent=zn,Ht=a(),we=o("p"),we.textContent=jn,Ot=a(),f(C.$$.fragment),Ft=a(),ve=o("p"),ve.textContent=An,It=a(),$e=o("p"),$e.textContent=Gn,qt=a(),xe=o("p"),xe.textContent=Dn,Wt=a(),Le=o("p"),Le.textContent=En,zt=a(),jt=o("hr"),At=a(),ye=o("p"),ye.textContent=Sn,Gt=a(),be=o("p"),be.textContent=Un,Dt=a(),f(Ce.$$.fragment),Et=a(),f(Re.$$.fragment),St=a(),f(Pe.$$.fragment),Ut=a(),f(Te.$$.fragment),Bt=a(),f(ke.$$.fragment),Yt=a(),f(Me.$$.fragment),Qt=a(),f(_e.$$.fragment),Nt=a(),f(He.$$.fragment),Vt=a(),f(Oe.$$.fragment),Jt=a(),f(Fe.$$.fragment),Kt=a(),f(Ie.$$.fragment),Xt=a(),We=o("p"),this.h()},l(e){const t=ti("svelte-u9bgzb",document.head);g=s(t,"META",{name:!0,content:!0}),t.forEach(n),v=r(e),$=s(e,"P",{}),Qn($).forEach(n),x=r(e),p(y.$$.fragment,e),w=r(e),L=s(e,"P",{"data-svelte-h":!0}),l(L)!=="svelte-cir3ne"&&(L.textContent=en),je=r(e),R=s(e,"P",{"data-svelte-h":!0}),l(R)!=="svelte-bu5wvy"&&(R.textContent=tn),Ae=r(e),p(b.$$.fragment,e),Ge=r(e),P=s(e,"P",{"data-svelte-h":!0}),l(P)!=="svelte-slvo6d"&&(P.textContent=nn),De=r(e),p(T.$$.fragment,e),Ee=r(e),k=s(e,"P",{"data-svelte-h":!0}),l(k)!=="svelte-zixz1p"&&(k.innerHTML=an),Se=r(e),M=s(e,"P",{"data-svelte-h":!0}),l(M)!=="svelte-12ibefg"&&(M.innerHTML=rn),Ue=r(e),_=s(e,"P",{"data-svelte-h":!0}),l(_)!=="svelte-sy5vgw"&&(_.innerHTML=on),Be=r(e),H=s(e,"P",{"data-svelte-h":!0}),l(H)!=="svelte-1iaj6ti"&&(H.textContent=sn),Ye=r(e),p(O.$$.fragment,e),Qe=r(e),F=s(e,"P",{"data-svelte-h":!0}),l(F)!=="svelte-yob3xl"&&(F.textContent=ln),Ne=r(e),p(I.$$.fragment,e),Ve=r(e),q=s(e,"P",{"data-svelte-h":!0}),l(q)!=="svelte-1veabrk"&&(q.textContent=fn),Je=r(e),p(W.$$.fragment,e),Ke=r(e),z=s(e,"P",{"data-svelte-h":!0}),l(z)!=="svelte-2rjcti"&&(z.textContent=pn),Xe=r(e),p(j.$$.fragment,e),Ze=r(e),A=s(e,"P",{"data-svelte-h":!0}),l(A)!=="svelte-1qcqexo"&&(A.textContent=hn),et=r(e),G=s(e,"P",{"data-svelte-h":!0}),l(G)!=="svelte-utdwbs"&&(G.innerHTML=mn),tt=r(e),D=s(e,"P",{"data-svelte-h":!0}),l(D)!=="svelte-11duu8f"&&(D.innerHTML=un),nt=r(e),E=s(e,"P",{"data-svelte-h":!0}),l(E)!=="svelte-4rbpl1"&&(E.textContent=dn),it=r(e),p(S.$$.fragment,e),at=r(e),U=s(e,"P",{"data-svelte-h":!0}),l(U)!=="svelte-1jv6l0y"&&(U.textContent=gn),rt=r(e),p(B.$$.fragment,e),ot=r(e),Y=s(e,"P",{"data-svelte-h":!0}),l(Y)!=="svelte-t8x49h"&&(Y.innerHTML=cn),st=r(e),Q=s(e,"P",{"data-svelte-h":!0}),l(Q)!=="svelte-73ben5"&&(Q.textContent=wn),lt=r(e),N=s(e,"TABLE",{"data-svelte-h":!0}),l(N)!=="svelte-4hnhaa"&&(N.innerHTML=vn),ft=r(e),V=s(e,"P",{"data-svelte-h":!0}),l(V)!=="svelte-wet5s9"&&(V.textContent=$n),pt=r(e),p(J.$$.fragment,e),ht=r(e),K=s(e,"P",{"data-svelte-h":!0}),l(K)!=="svelte-2nnkb3"&&(K.textContent=xn),mt=r(e),X=s(e,"P",{"data-svelte-h":!0}),l(X)!=="svelte-59sj0k"&&(X.innerHTML=Ln),ut=r(e),Z=s(e,"P",{"data-svelte-h":!0}),l(Z)!=="svelte-1f12e0u"&&(Z.textContent=yn),dt=r(e),ee=s(e,"UL",{"data-svelte-h":!0}),l(ee)!=="svelte-12mjlhm"&&(ee.innerHTML=bn),gt=r(e),te=s(e,"P",{"data-svelte-h":!0}),l(te)!=="svelte-fggtyf"&&(te.textContent=Cn),ct=r(e),ne=s(e,"P",{"data-svelte-h":!0}),l(ne)!=="svelte-ao9cj7"&&(ne.innerHTML=Rn),wt=r(e),ie=s(e,"P",{"data-svelte-h":!0}),l(ie)!=="svelte-l992q4"&&(ie.textContent=Pn),vt=r(e),ae=s(e,"P",{"data-svelte-h":!0}),l(ae)!=="svelte-13h0geo"&&(ae.innerHTML=Tn),$t=r(e),p(re.$$.fragment,e),xt=r(e),oe=s(e,"P",{"data-svelte-h":!0}),l(oe)!=="svelte-vbayee"&&(oe.innerHTML=kn),Lt=r(e),se=s(e,"OL",{"data-svelte-h":!0}),l(se)!=="svelte-13rt60o"&&(se.innerHTML=Mn),yt=r(e),le=s(e,"P",{"data-svelte-h":!0}),l(le)!=="svelte-20078t"&&(le.innerHTML=_n),bt=r(e),fe=s(e,"P",{"data-svelte-h":!0}),l(fe)!=="svelte-1ljlbgz"&&(fe.textContent=Hn),Ct=r(e),pe=s(e,"TABLE",{"data-svelte-h":!0}),l(pe)!=="svelte-omrei2"&&(pe.innerHTML=On),Rt=r(e),he=s(e,"P",{"data-svelte-h":!0}),l(he)!=="svelte-cocpof"&&(he.textContent=Fn),Pt=r(e),p(me.$$.fragment,e),Tt=r(e),ue=s(e,"P",{"data-svelte-h":!0}),l(ue)!=="svelte-1toktt4"&&(ue.textContent=In),kt=r(e),de=s(e,"P",{"data-svelte-h":!0}),l(de)!=="svelte-1kk40pt"&&(de.textContent=qn),Mt=r(e),ge=s(e,"UL",{"data-svelte-h":!0}),l(ge)!=="svelte-19pdi9y"&&(ge.innerHTML=Wn),_t=r(e),ce=s(e,"P",{"data-svelte-h":!0}),l(ce)!=="svelte-1uprfeu"&&(ce.textContent=zn),Ht=r(e),we=s(e,"P",{"data-svelte-h":!0}),l(we)!=="svelte-gtn9pd"&&(we.textContent=jn),Ot=r(e),p(C.$$.fragment,e),Ft=r(e),ve=s(e,"P",{"data-svelte-h":!0}),l(ve)!=="svelte-1r71chc"&&(ve.textContent=An),It=r(e),$e=s(e,"P",{"data-svelte-h":!0}),l($e)!=="svelte-172rnsv"&&($e.textContent=Gn),qt=r(e),xe=s(e,"P",{"data-svelte-h":!0}),l(xe)!=="svelte-azw8h4"&&(xe.textContent=Dn),Wt=r(e),Le=s(e,"P",{"data-svelte-h":!0}),l(Le)!=="svelte-hlmg2x"&&(Le.textContent=En),zt=r(e),jt=s(e,"HR",{}),At=r(e),ye=s(e,"P",{"data-svelte-h":!0}),l(ye)!=="svelte-ax6i43"&&(ye.textContent=Sn),Gt=r(e),be=s(e,"P",{"data-svelte-h":!0}),l(be)!=="svelte-1k1hoix"&&(be.textContent=Un),Dt=r(e),p(Ce.$$.fragment,e),Et=r(e),p(Re.$$.fragment,e),St=r(e),p(Pe.$$.fragment,e),Ut=r(e),p(Te.$$.fragment,e),Bt=r(e),p(ke.$$.fragment,e),Yt=r(e),p(Me.$$.fragment,e),Qt=r(e),p(_e.$$.fragment,e),Nt=r(e),p(He.$$.fragment,e),Vt=r(e),p(Oe.$$.fragment,e),Jt=r(e),p(Fe.$$.fragment,e),Kt=r(e),p(Ie.$$.fragment,e),Xt=r(e),We=s(e,"P",{}),Qn(We).forEach(n),this.h()},h(){Nn(g,"name","hf:doc:metadata"),Nn(g,"content",li)},m(e,t){ni(document.head,g),i(e,v,t),i(e,$,t),i(e,x,t),h(y,e,t),i(e,w,t),i(e,L,t),i(e,je,t),i(e,R,t),i(e,Ae,t),h(b,e,t),i(e,Ge,t),i(e,P,t),i(e,De,t),h(T,e,t),i(e,Ee,t),i(e,k,t),i(e,Se,t),i(e,M,t),i(e,Ue,t),i(e,_,t),i(e,Be,t),i(e,H,t),i(e,Ye,t),h(O,e,t),i(e,Qe,t),i(e,F,t),i(e,Ne,t),h(I,e,t),i(e,Ve,t),i(e,q,t),i(e,Je,t),h(W,e,t),i(e,Ke,t),i(e,z,t),i(e,Xe,t),h(j,e,t),i(e,Ze,t),i(e,A,t),i(e,et,t),i(e,G,t),i(e,tt,t),i(e,D,t),i(e,nt,t),i(e,E,t),i(e,it,t),h(S,e,t),i(e,at,t),i(e,U,t),i(e,rt,t),h(B,e,t),i(e,ot,t),i(e,Y,t),i(e,st,t),i(e,Q,t),i(e,lt,t),i(e,N,t),i(e,ft,t),i(e,V,t),i(e,pt,t),h(J,e,t),i(e,ht,t),i(e,K,t),i(e,mt,t),i(e,X,t),i(e,ut,t),i(e,Z,t),i(e,dt,t),i(e,ee,t),i(e,gt,t),i(e,te,t),i(e,ct,t),i(e,ne,t),i(e,wt,t),i(e,ie,t),i(e,vt,t),i(e,ae,t),i(e,$t,t),h(re,e,t),i(e,xt,t),i(e,oe,t),i(e,Lt,t),i(e,se,t),i(e,yt,t),i(e,le,t),i(e,bt,t),i(e,fe,t),i(e,Ct,t),i(e,pe,t),i(e,Rt,t),i(e,he,t),i(e,Pt,t),h(me,e,t),i(e,Tt,t),i(e,ue,t),i(e,kt,t),i(e,de,t),i(e,Mt,t),i(e,ge,t),i(e,_t,t),i(e,ce,t),i(e,Ht,t),i(e,we,t),i(e,Ot,t),h(C,e,t),i(e,Ft,t),i(e,ve,t),i(e,It,t),i(e,$e,t),i(e,qt,t),i(e,xe,t),i(e,Wt,t),i(e,Le,t),i(e,zt,t),i(e,jt,t),i(e,At,t),i(e,ye,t),i(e,Gt,t),i(e,be,t),i(e,Dt,t),h(Ce,e,t),i(e,Et,t),h(Re,e,t),i(e,St,t),h(Pe,e,t),i(e,Ut,t),h(Te,e,t),i(e,Bt,t),h(ke,e,t),i(e,Yt,t),h(Me,e,t),i(e,Qt,t),h(_e,e,t),i(e,Nt,t),h(He,e,t),i(e,Vt,t),h(Oe,e,t),i(e,Jt,t),h(Fe,e,t),i(e,Kt,t),h(Ie,e,t),i(e,Xt,t),i(e,We,t),Zt=!0},p(e,[t]){const Bn={};t&2&&(Bn.$$scope={dirty:t,ctx:e}),b.$set(Bn);const Yn={};t&2&&(Yn.$$scope={dirty:t,ctx:e}),C.$set(Yn)},i(e){Zt||(m(y.$$.fragment,e),m(b.$$.fragment,e),m(T.$$.fragment,e),m(O.$$.fragment,e),m(I.$$.fragment,e),m(W.$$.fragment,e),m(j.$$.fragment,e),m(S.$$.fragment,e),m(B.$$.fragment,e),m(J.$$.fragment,e),m(re.$$.fragment,e),m(me.$$.fragment,e),m(C.$$.fragment,e),m(Ce.$$.fragment,e),m(Re.$$.fragment,e),m(Pe.$$.fragment,e),m(Te.$$.fragment,e),m(ke.$$.fragment,e),m(Me.$$.fragment,e),m(_e.$$.fragment,e),m(He.$$.fragment,e),m(Oe.$$.fragment,e),m(Fe.$$.fragment,e),m(Ie.$$.fragment,e),Zt=!0)},o(e){u(y.$$.fragment,e),u(b.$$.fragment,e),u(T.$$.fragment,e),u(O.$$.fragment,e),u(I.$$.fragment,e),u(W.$$.fragment,e),u(j.$$.fragment,e),u(S.$$.fragment,e),u(B.$$.fragment,e),u(J.$$.fragment,e),u(re.$$.fragment,e),u(me.$$.fragment,e),u(C.$$.fragment,e),u(Ce.$$.fragment,e),u(Re.$$.fragment,e),u(Pe.$$.fragment,e),u(Te.$$.fragment,e),u(ke.$$.fragment,e),u(Me.$$.fragment,e),u(_e.$$.fragment,e),u(He.$$.fragment,e),u(Oe.$$.fragment,e),u(Fe.$$.fragment,e),u(Ie.$$.fragment,e),Zt=!1},d(e){e&&(n(v),n($),n(x),n(w),n(L),n(je),n(R),n(Ae),n(Ge),n(P),n(De),n(Ee),n(k),n(Se),n(M),n(Ue),n(_),n(Be),n(H),n(Ye),n(Qe),n(F),n(Ne),n(Ve),n(q),n(Je),n(Ke),n(z),n(Xe),n(Ze),n(A),n(et),n(G),n(tt),n(D),n(nt),n(E),n(it),n(at),n(U),n(rt),n(ot),n(Y),n(st),n(Q),n(lt),n(N),n(ft),n(V),n(pt),n(ht),n(K),n(mt),n(X),n(ut),n(Z),n(dt),n(ee),n(gt),n(te),n(ct),n(ne),n(wt),n(ie),n(vt),n(ae),n($t),n(xt),n(oe),n(Lt),n(se),n(yt),n(le),n(bt),n(fe),n(Ct),n(pe),n(Rt),n(he),n(Pt),n(Tt),n(ue),n(kt),n(de),n(Mt),n(ge),n(_t),n(ce),n(Ht),n(we),n(Ot),n(Ft),n(ve),n(It),n($e),n(qt),n(xe),n(Wt),n(Le),n(zt),n(jt),n(At),n(ye),n(Gt),n(be),n(Dt),n(Et),n(St),n(Ut),n(Bt),n(Yt),n(Qt),n(Nt),n(Vt),n(Jt),n(Kt),n(Xt),n(We)),n(g),d(y,e),d(b,e),d(T,e),d(O,e),d(I,e),d(W,e),d(j,e),d(S,e),d(B,e),d(J,e),d(re,e),d(me,e),d(C,e),d(Ce,e),d(Re,e),d(Pe,e),d(Te,e),d(ke,e),d(Me,e),d(_e,e),d(He,e),d(Oe,e),d(Fe,e),d(Ie,e)}}}const li='{"title":"Introduction to Reinforcement Learning and its Role in LLMs","local":"introduction-to-reinforcement-learning-and-its-role-in-llms","sections":[{"title":"What is Reinforcement Learning (RL)?","local":"what-is-reinforcement-learning-rl","sections":[{"title":"Agent","local":"agent","sections":[],"depth":3},{"title":"Environment","local":"environment","sections":[],"depth":3},{"title":"Action","local":"action","sections":[],"depth":3},{"title":"Reward","local":"reward","sections":[],"depth":3},{"title":"Policy","local":"policy","sections":[],"depth":3}],"depth":2},{"title":"The RL Process: Trial and Error","local":"the-rl-process-trial-and-error","sections":[],"depth":2},{"title":"Role of RL in Large Language Models (LLMs)","local":"role-of-rl-in-large-language-models-llms","sections":[],"depth":2},{"title":"Reinforcement Learning from Human Feedback (RLHF)","local":"reinforcement-learning-from-human-feedback-rlhf","sections":[],"depth":2},{"title":"Why should we care about GRPO (Group Relative Policy Optimization)?","local":"why-should-we-care-about-grpo-group-relative-policy-optimization","sections":[],"depth":2},{"title":"Quiz","local":"quiz","sections":[{"title":"1. What are the key components of Reinforcement Learning?","local":"1-what-are-the-key-components-of-reinforcement-learning","sections":[],"depth":3},{"title":"2. What is the main advantage of RLHF for training language models?","local":"2-what-is-the-main-advantage-of-rlhf-for-training-language-models","sections":[],"depth":3},{"title":"3. In the context of RL for LLMs, what represents an “action”?","local":"3-in-the-context-of-rl-for-llms-what-represents-an-action","sections":[],"depth":3},{"title":"4. What is the role of the reward in RL training of language models?","local":"4-what-is-the-role-of-the-reward-in-rl-training-of-language-models","sections":[],"depth":3},{"title":"5. What is a reward in the context of RL for LLMs?","local":"5-what-is-a-reward-in-the-context-of-rl-for-llms","sections":[],"depth":3}],"depth":2}],"depth":1}';function fi(qe){return Kn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class gi extends Zn{constructor(g){super(),ei(this,g,fi,si,Jn,{})}}export{gi as component};
