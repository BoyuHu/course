import{s as Bt,f as lt,o as Zt}from"../chunks/scheduler.37c15a92.js";import{S as Rt,i as $t,g as i,s as a,r as o,A as kt,h as r,f as l,c as n,j as oe,u as m,x as p,k as M,y as At,a as s,v as y,d as w,t as c,w as d,m as vt,n as _t}from"../chunks/index.2bf4358c.js";import{T as xt}from"../chunks/Tip.363c041f.js";import{C as pe}from"../chunks/CodeBlock.4f5fc1ad.js";import{H as u}from"../chunks/Heading.8ada512a.js";function Xt(me){let U;return{c(){U=vt("This chapter is aimed at TRL beginners. If you are already familiar with TRL, you might want to also check out the [Open R1 implementation](https://github.com/huggingface/open-r1/blob/main/src/open_r1/grpo.py) of GRPO.")},l(h){U=_t(h,"This chapter is aimed at TRL beginners. If you are already familiar with TRL, you might want to also check out the [Open R1 implementation](https://github.com/huggingface/open-r1/blob/main/src/open_r1/grpo.py) of GRPO.")},m(h,C){s(h,U,C)},d(h){h&&l(U)}}}function Wt(me){let U,h,C,ye,I,we,b,st="In this page, we’ll learn how to implement Group Relative Policy Optimization (GRPO) using the Transformer Reinforcement Learning (TRL) library. We’ll focus on practical implementation with minimal code.",ce,g,at="We’ll explore the core concepts of GRPO as they are embodied in TRL’s GRPOTrainer, using snippets from the official TRL documentation to guide us.",de,f,Ue,G,nt="First, let’s remind ourselves of some of the important concepts of GRPO algorithm:",Je,B,it="<li>Group Formation: The model generates multiple completions for each prompt.</li> <li>Preference Learning: The model learns from a reward function that compares groups of completions.</li> <li>Training Configuration: The model uses a configuration to control the training process.</li>",Te,Z,rt="What do we need to do to implement GRPO?",je,R,pt="<li>Define a dataset of prompts.</li> <li>Define a reward function that takes a list of completions and returns a list of rewards.</li> <li>Configure the training process with a GRPOConfig.</li> <li>Train the model using the GRPOTrainer.</li>",ue,$,Mt="Here’s a minimal example to get started with GRPO training:",he,k,fe,A,Ce,v,Ie,_,ot="Your dataset should contain prompts that the model will respond to. The GRPO trainer will generate multiple completions for each prompt and use the reward function to compare them.",be,x,ge,X,mt="The reward function is crucial - it determines how the model learns. Here are two practical examples:",Ge,W,Be,E,Ze,F,yt="Key parameters to consider in <code>GRPOConfig</code>:",Re,Y,$e,S,wt="The <code>num_generation</code> parameter is particularly important for GRPO as it defines the group size - how many different completions the model will generate for each prompt. This is a key differentiator from other RL methods:",ke,V,ct="<li>Too small (e.g., 2-3): May not provide enough diversity for meaningful comparisons</li> <li>Recommended (4-16): Provides good balance between diversity and computational efficiency</li> <li>Larger values: May improve learning but significantly increases computational cost</li>",Ae,Q,dt="The group size should be chosen based on your computational resources and the complexity of your task. For simple tasks, smaller groups (4-8) may be sufficient, while more complex reasoning tasks might benefit from larger groups (8-16).",ve,z,_e,N,Ut="<li><strong>Memory Management</strong>: Adjust <code>per_device_train_batch_size</code> and <code>gradient_accumulation_steps</code> based on your GPU memory.</li> <li><strong>Speed</strong>: Enable <code>use_vllm=True</code> for faster generation if your model is supported.</li> <li><strong>Monitoring</strong>: Watch the logged metrics during training:<ul><li><code>reward</code>: Average reward across completions</li> <li><code>reward_std</code>: Standard deviation within reward groups</li> <li><code>kl</code>: KL divergence from reference model</li></ul></li>",xe,H,Xe,L,Jt="The DeepSeek R1 paper demonstrates several effective approaches to reward function design that you can adapt for your own GRPO implementation:",We,P,Ee,q,Tt="One of the easiest rewards to implement is a length-based reward. You can reward longer completions:",Fe,D,Ye,K,jt="This reward function penalizes completions that are too short or too long, encouraging the model to generate completions that are close to the ideal length of 20 tokens.",Se,J,ut,Ve,O,Qe,ee,ht="For tasks with objectively correct answers (like mathematics or coding), you can implement rule-based reward functions:",ze,te,Ne,T,ft,He,le,Le,se,Ct="You can also reward proper formatting, which was important in the DeepSeek R1 training:",Pe,ae,qe,j,It,De,ne,bt="These examples demonstrate how you can implement reward functions inspired by the DeepSeek R1 training process, focusing on correctness, formatting, and combined signals.",Ke,ie,Oe,re,gt="In the next section, you will follow an exercise to implement GRPO in TRL.",et,Me,tt;return I=new u({props:{title:"Implementing GRPO in TRL",local:"implementing-grpo-in-trl",headingTag:"h1"}}),f=new xt({props:{$$slots:{default:[Xt]},$$scope:{ctx:me}}}),k=new pe({props:{code:"ZnJvbSUyMHRybCUyMGltcG9ydCUyMEdSUE9UcmFpbmVyJTJDJTIwR1JQT0NvbmZpZyUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQSUyMyUyMDEuJTIwTG9hZCUyMHlvdXIlMjBkYXRhc2V0JTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJ5b3VyX2RhdGFzZXQlMjIlMkMlMjBzcGxpdCUzRCUyMnRyYWluJTIyKSUwQSUwQSUwQSUyMyUyMDIuJTIwRGVmaW5lJTIwYSUyMHNpbXBsZSUyMHJld2FyZCUyMGZ1bmN0aW9uJTBBZGVmJTIwcmV3YXJkX2Z1bmMoY29tcGxldGlvbnMlMkMlMjAqKmt3YXJncyklM0ElMEElMjAlMjAlMjAlMjAlMjIlMjIlMjJFeGFtcGxlJTNBJTIwUmV3YXJkJTIwbG9uZ2VyJTIwY29tcGxldGlvbnMlMjIlMjIlMjIlMEElMjAlMjAlMjAlMjByZXR1cm4lMjAlNUJmbG9hdChsZW4oY29tcGxldGlvbikpJTIwZm9yJTIwY29tcGxldGlvbiUyMGluJTIwY29tcGxldGlvbnMlNUQlMEElMEElMEElMjMlMjAzLiUyMENvbmZpZ3VyZSUyMHRyYWluaW5nJTBBdHJhaW5pbmdfYXJncyUyMCUzRCUyMEdSUE9Db25maWcoJTBBJTIwJTIwJTIwJTIwb3V0cHV0X2RpciUzRCUyMm91dHB1dCUyMiUyQyUwQSUyMCUyMCUyMCUyMG51bV90cmFpbl9lcG9jaHMlM0QzJTJDJTBBJTIwJTIwJTIwJTIwcGVyX2RldmljZV90cmFpbl9iYXRjaF9zaXplJTNENCUyQyUwQSUyMCUyMCUyMCUyMGdyYWRpZW50X2FjY3VtdWxhdGlvbl9zdGVwcyUzRDIlMkMlMEElMjAlMjAlMjAlMjBsb2dnaW5nX3N0ZXBzJTNEMTAlMkMlMEEpJTBBJTBBJTIzJTIwNC4lMjBJbml0aWFsaXplJTIwYW5kJTIwdHJhaW4lMEF0cmFpbmVyJTIwJTNEJTIwR1JQT1RyYWluZXIoJTBBJTIwJTIwJTIwJTIwbW9kZWwlM0QlMjJ5b3VyX21vZGVsJTIyJTJDJTIwJTIwJTIzJTIwZS5nLiUyMCUyMlF3ZW4lMkZRd2VuMi0wLjVCLUluc3RydWN0JTIyJTBBJTIwJTIwJTIwJTIwYXJncyUzRHRyYWluaW5nX2FyZ3MlMkMlMEElMjAlMjAlMjAlMjB0cmFpbl9kYXRhc2V0JTNEZGF0YXNldCUyQyUwQSUyMCUyMCUyMCUyMHJld2FyZF9mdW5jcyUzRHJld2FyZF9mdW5jJTJDJTBBKSUwQXRyYWluZXIudHJhaW4oKQ==",highlighted:`<span class="hljs-keyword">from</span> trl <span class="hljs-keyword">import</span> GRPOTrainer, GRPOConfig
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># 1. Load your dataset</span>
dataset = load_dataset(<span class="hljs-string">&quot;your_dataset&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)


<span class="hljs-comment"># 2. Define a simple reward function</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">reward_func</span>(<span class="hljs-params">completions, **kwargs</span>):
    <span class="hljs-string">&quot;&quot;&quot;Example: Reward longer completions&quot;&quot;&quot;</span>
    <span class="hljs-keyword">return</span> [<span class="hljs-built_in">float</span>(<span class="hljs-built_in">len</span>(completion)) <span class="hljs-keyword">for</span> completion <span class="hljs-keyword">in</span> completions]


<span class="hljs-comment"># 3. Configure training</span>
training_args = GRPOConfig(
    output_dir=<span class="hljs-string">&quot;output&quot;</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    per_device_train_batch_size=<span class="hljs-number">4</span>,
    gradient_accumulation_steps=<span class="hljs-number">2</span>,
    logging_steps=<span class="hljs-number">10</span>,
)

<span class="hljs-comment"># 4. Initialize and train</span>
trainer = GRPOTrainer(
    model=<span class="hljs-string">&quot;your_model&quot;</span>,  <span class="hljs-comment"># e.g. &quot;Qwen/Qwen2-0.5B-Instruct&quot;</span>
    args=training_args,
    train_dataset=dataset,
    reward_funcs=reward_func,
)
trainer.train()`,wrap:!1}}),A=new u({props:{title:"Key Components",local:"key-components",headingTag:"h2"}}),v=new u({props:{title:"1. Dataset Format",local:"1-dataset-format",headingTag:"h3"}}),x=new u({props:{title:"2. Reward Function",local:"2-reward-function",headingTag:"h3"}}),W=new pe({props:{code:"JTIzJTIwRXhhbXBsZSUyMDElM0ElMjBSZXdhcmQlMjBiYXNlZCUyMG9uJTIwY29tcGxldGlvbiUyMGxlbmd0aCUwQWRlZiUyMHJld2FyZF9sZW5ndGgoY29tcGxldGlvbnMlMkMlMjAqKmt3YXJncyklM0ElMEElMjAlMjAlMjAlMjByZXR1cm4lMjAlNUJmbG9hdChsZW4oY29tcGxldGlvbikpJTIwZm9yJTIwY29tcGxldGlvbiUyMGluJTIwY29tcGxldGlvbnMlNUQlMEElMEElMEElMjMlMjBFeGFtcGxlJTIwMiUzQSUyMFJld2FyZCUyMGJhc2VkJTIwb24lMjBtYXRjaGluZyUyMGElMjBwYXR0ZXJuJTBBaW1wb3J0JTIwcmUlMEElMEElMEFkZWYlMjByZXdhcmRfZm9ybWF0KGNvbXBsZXRpb25zJTJDJTIwKiprd2FyZ3MpJTNBJTBBJTIwJTIwJTIwJTIwcGF0dGVybiUyMCUzRCUyMHIlMjIlNUUlM0N0aGluayUzRS4qJTNGJTNDJTJGdGhpbmslM0UlM0NhbnN3ZXIlM0UuKiUzRiUzQyUyRmFuc3dlciUzRSUyNCUyMiUwQSUyMCUyMCUyMCUyMHJldHVybiUyMCU1QjEuMCUyMGlmJTIwcmUubWF0Y2gocGF0dGVybiUyQyUyMGMpJTIwZWxzZSUyMDAuMCUyMGZvciUyMGMlMjBpbiUyMGNvbXBsZXRpb25zJTVE",highlighted:`<span class="hljs-comment"># Example 1: Reward based on completion length</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">reward_length</span>(<span class="hljs-params">completions, **kwargs</span>):
    <span class="hljs-keyword">return</span> [<span class="hljs-built_in">float</span>(<span class="hljs-built_in">len</span>(completion)) <span class="hljs-keyword">for</span> completion <span class="hljs-keyword">in</span> completions]


<span class="hljs-comment"># Example 2: Reward based on matching a pattern</span>
<span class="hljs-keyword">import</span> re


<span class="hljs-keyword">def</span> <span class="hljs-title function_">reward_format</span>(<span class="hljs-params">completions, **kwargs</span>):
    pattern = <span class="hljs-string">r&quot;^&lt;think&gt;.*?&lt;/think&gt;&lt;answer&gt;.*?&lt;/answer&gt;$&quot;</span>
    <span class="hljs-keyword">return</span> [<span class="hljs-number">1.0</span> <span class="hljs-keyword">if</span> re.<span class="hljs-keyword">match</span>(pattern, c) <span class="hljs-keyword">else</span> <span class="hljs-number">0.0</span> <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> completions]`,wrap:!1}}),E=new u({props:{title:"3. Training Configuration",local:"3-training-configuration",headingTag:"h3"}}),Y=new pe({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMEdSUE9Db25maWcoJTBBJTIwJTIwJTIwJTIwJTIzJTIwRXNzZW50aWFsJTIwcGFyYW1ldGVycyUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjJvdXRwdXQlMjIlMkMlMEElMjAlMjAlMjAlMjBudW1fdHJhaW5fZXBvY2hzJTNEMyUyQyUwQSUyMCUyMCUyMCUyMG51bV9nZW5lcmF0aW9uJTNENCUyQyUyMCUyMCUyMyUyME51bWJlciUyMG9mJTIwY29tcGxldGlvbnMlMjB0byUyMGdlbmVyYXRlJTIwZm9yJTIwZWFjaCUyMHByb21wdCUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfdHJhaW5fYmF0Y2hfc2l6ZSUzRDQlMkMlMjAlMjAlMjMlMjBXZSUyMHdhbnQlMjB0byUyMGdldCUyMGFsbCUyMGdlbmVyYXRpb25zJTIwaW4lMjBvbmUlMjBkZXZpY2UlMjBiYXRjaCUwQSUyMCUyMCUyMCUyMCUyMyUyME9wdGlvbmFsJTIwYnV0JTIwdXNlZnVsJTBBJTIwJTIwJTIwJTIwZ3JhZGllbnRfYWNjdW11bGF0aW9uX3N0ZXBzJTNEMiUyQyUwQSUyMCUyMCUyMCUyMGxlYXJuaW5nX3JhdGUlM0QxZS01JTJDJTBBJTIwJTIwJTIwJTIwbG9nZ2luZ19zdGVwcyUzRDEwJTJDJTBBJTIwJTIwJTIwJTIwJTIzJTIwR1JQTyUyMHNwZWNpZmljJTIwKG9wdGlvbmFsKSUwQSUyMCUyMCUyMCUyMHVzZV92bGxtJTNEVHJ1ZSUyQyUyMCUyMCUyMyUyMFNwZWVkJTIwdXAlMjBnZW5lcmF0aW9uJTBBKQ==",highlighted:`training_args = GRPOConfig(
    <span class="hljs-comment"># Essential parameters</span>
    output_dir=<span class="hljs-string">&quot;output&quot;</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    num_generation=<span class="hljs-number">4</span>,  <span class="hljs-comment"># Number of completions to generate for each prompt</span>
    per_device_train_batch_size=<span class="hljs-number">4</span>,  <span class="hljs-comment"># We want to get all generations in one device batch</span>
    <span class="hljs-comment"># Optional but useful</span>
    gradient_accumulation_steps=<span class="hljs-number">2</span>,
    learning_rate=<span class="hljs-number">1e-5</span>,
    logging_steps=<span class="hljs-number">10</span>,
    <span class="hljs-comment"># GRPO specific (optional)</span>
    use_vllm=<span class="hljs-literal">True</span>,  <span class="hljs-comment"># Speed up generation</span>
)`,wrap:!1}}),z=new u({props:{title:"Tips for Success",local:"tips-for-success",headingTag:"h2"}}),H=new u({props:{title:"Reward Function Design",local:"reward-function-design",headingTag:"h2"}}),P=new u({props:{title:"1. Length-Based Rewards",local:"1-length-based-rewards",headingTag:"h3"}}),D=new pe({props:{code:"ZGVmJTIwcmV3YXJkX2xlbihjb21wbGV0aW9ucyUyQyUyMCoqa3dhcmdzKSUzQSUwQSUyMCUyMCUyMCUyMGlkZWFsX2xlbmd0aCUyMCUzRCUyMDIwJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwJTVCLWFicyhpZGVhbF9sZW5ndGglMjAtJTIwbGVuKGNvbXBsZXRpb24pKSUyMGZvciUyMGNvbXBsZXRpb24lMjBpbiUyMGNvbXBsZXRpb25zJTVE",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">reward_len</span>(<span class="hljs-params">completions, **kwargs</span>):
    ideal_length = <span class="hljs-number">20</span>
    <span class="hljs-keyword">return</span> [-<span class="hljs-built_in">abs</span>(ideal_length - <span class="hljs-built_in">len</span>(completion)) <span class="hljs-keyword">for</span> completion <span class="hljs-keyword">in</span> completions]`,wrap:!1}}),O=new u({props:{title:"2. Rule-Based Rewards for Verifiable Tasks",local:"2-rule-based-rewards-for-verifiable-tasks",headingTag:"h2"}}),te=new pe({props:{code:"ZGVmJTIwcHJvYmxlbV9yZXdhcmQoY29tcGxldGlvbnMlMkMlMjBhbnN3ZXJzJTJDJTIwKiprd2FyZ3MpJTNBJTBBJTIwJTIwJTIwJTIwJTIyJTIyJTIyUmV3YXJkJTIwZnVuY3Rpb24lMjBmb3IlMjBtYXRoJTIwcHJvYmxlbXMlMjB3aXRoJTIwdmVyaWZpYWJsZSUyMGFuc3dlcnMlMEElMjAlMjAlMjAlMjBjb21wbGV0aW9ucyUzQSUyMGxpc3QlMjBvZiUyMGNvbXBsZXRpb25zJTIwdG8lMjBldmFsdWF0ZSUwQSUyMCUyMCUyMCUyMGFuc3dlcnMlM0ElMjBsaXN0JTIwb2YlMjBhbnN3ZXJzJTIwdG8lMjB0aGUlMjBwcm9ibGVtcyUyMGZyb20lMjB0aGUlMjBkYXRhc2V0JTBBJTIwJTIwJTIwJTIwJTIyJTIyJTIyJTBBJTBBJTIwJTIwJTIwJTIwcmV3YXJkcyUyMCUzRCUyMCU1QiU1RCUwQSUyMCUyMCUyMCUyMGZvciUyMGNvbXBsZXRpb24lMkMlMjBjb3JyZWN0X2Fuc3dlciUyMGluJTIwemlwKGNvbXBsZXRpb25zJTJDJTIwYW5zd2VycyklM0ElMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjMlMjBFeHRyYWN0JTIwdGhlJTIwYW5zd2VyJTIwZnJvbSUyMHRoZSUyMGNvbXBsZXRpb24lMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjB0cnklM0ElMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjMlMjBUaGlzJTIwaXMlMjBhJTIwc2ltcGxpZmllZCUyMGV4YW1wbGUlMjAtJTIweW91J2QlMjBuZWVkJTIwcHJvcGVyJTIwcGFyc2luZyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGFuc3dlciUyMCUzRCUyMGV4dHJhY3RfZmluYWxfYW5zd2VyKGNvbXBsZXRpb24pJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIzJTIwQmluYXJ5JTIwcmV3YXJkJTNBJTIwMSUyMGZvciUyMGNvcnJlY3QlMkMlMjAwJTIwZm9yJTIwaW5jb3JyZWN0JTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwcmV3YXJkJTIwJTNEJTIwMS4wJTIwaWYlMjBhbnN3ZXIlMjAlM0QlM0QlMjBjb3JyZWN0X2Fuc3dlciUyMGVsc2UlMjAwLjAlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjByZXdhcmRzLmFwcGVuZChyZXdhcmQpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwZXhjZXB0JTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIzJTIwSWYlMjB3ZSUyMGNhbid0JTIwcGFyc2UlMjBhbiUyMGFuc3dlciUyQyUyMGdpdmUlMjBhJTIwbG93JTIwcmV3YXJkJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwcmV3YXJkcy5hcHBlbmQoMC4wKSUwQSUwQSUyMCUyMCUyMCUyMHJldHVybiUyMHJld2FyZHM=",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">problem_reward</span>(<span class="hljs-params">completions, answers, **kwargs</span>):
    <span class="hljs-string">&quot;&quot;&quot;Reward function for math problems with verifiable answers
    completions: list of completions to evaluate
    answers: list of answers to the problems from the dataset
    &quot;&quot;&quot;</span>

    rewards = []
    <span class="hljs-keyword">for</span> completion, correct_answer <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(completions, answers):
        <span class="hljs-comment"># Extract the answer from the completion</span>
        <span class="hljs-keyword">try</span>:
            <span class="hljs-comment"># This is a simplified example - you&#x27;d need proper parsing</span>
            answer = extract_final_answer(completion)
            <span class="hljs-comment"># Binary reward: 1 for correct, 0 for incorrect</span>
            reward = <span class="hljs-number">1.0</span> <span class="hljs-keyword">if</span> answer == correct_answer <span class="hljs-keyword">else</span> <span class="hljs-number">0.0</span>
            rewards.append(reward)
        <span class="hljs-keyword">except</span>:
            <span class="hljs-comment"># If we can&#x27;t parse an answer, give a low reward</span>
            rewards.append(<span class="hljs-number">0.0</span>)

    <span class="hljs-keyword">return</span> rewards`,wrap:!1}}),le=new u({props:{title:"3. Format-Based Rewards",local:"3-format-based-rewards",headingTag:"h2"}}),ae=new pe({props:{code:"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",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">format_reward</span>(<span class="hljs-params">completions, **kwargs</span>):
    <span class="hljs-string">&quot;&quot;&quot;Reward completions that follow the desired format&quot;&quot;&quot;</span>
    <span class="hljs-comment"># Example: Check if the completion follows a think-then-answer format</span>
    pattern = <span class="hljs-string">r&quot;&lt;think&gt;(.*?)&lt;/think&gt;\\s*&lt;answer&gt;(.*?)&lt;/answer&gt;&quot;</span>

    rewards = []
    <span class="hljs-keyword">for</span> completion <span class="hljs-keyword">in</span> completions:
        <span class="hljs-keyword">match</span> = re.search(pattern, completion, re.DOTALL)
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">match</span>:
            <span class="hljs-comment"># Check if there&#x27;s substantial content in both sections</span>
            think_content = <span class="hljs-keyword">match</span>.group(<span class="hljs-number">1</span>).strip()
            answer_content = <span class="hljs-keyword">match</span>.group(<span class="hljs-number">2</span>).strip()

            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(think_content) &gt; <span class="hljs-number">20</span> <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(answer_content) &gt; <span class="hljs-number">0</span>:
                rewards.append(<span class="hljs-number">1.0</span>)
            <span class="hljs-keyword">else</span>:
                rewards.append(
                    <span class="hljs-number">0.5</span>
                )  <span class="hljs-comment"># Partial reward for correct format but limited content</span>
        <span class="hljs-keyword">else</span>:
            rewards.append(<span class="hljs-number">0.0</span>)  <span class="hljs-comment"># No reward for incorrect format</span>

    <span class="hljs-keyword">return</span> rewards`,wrap:!1}}),ie=new u({props:{title:"That’s it!",local:"thats-it",headingTag:"h2"}}),{c(){U=i("meta"),h=a(),C=i("p"),ye=a(),o(I.$$.fragment),we=a(),b=i("p"),b.textContent=st,ce=a(),g=i("p"),g.textContent=at,de=a(),o(f.$$.fragment),Ue=a(),G=i("p"),G.textContent=nt,Je=a(),B=i("ul"),B.innerHTML=it,Te=a(),Z=i("p"),Z.textContent=rt,je=a(),R=i("ul"),R.innerHTML=pt,ue=a(),$=i("p"),$.textContent=Mt,he=a(),o(k.$$.fragment),fe=a(),o(A.$$.fragment),Ce=a(),o(v.$$.fragment),Ie=a(),_=i("p"),_.textContent=ot,be=a(),o(x.$$.fragment),ge=a(),X=i("p"),X.textContent=mt,Ge=a(),o(W.$$.fragment),Be=a(),o(E.$$.fragment),Ze=a(),F=i("p"),F.innerHTML=yt,Re=a(),o(Y.$$.fragment),$e=a(),S=i("p"),S.innerHTML=wt,ke=a(),V=i("ul"),V.innerHTML=ct,Ae=a(),Q=i("p"),Q.textContent=dt,ve=a(),o(z.$$.fragment),_e=a(),N=i("ol"),N.innerHTML=Ut,xe=a(),o(H.$$.fragment),Xe=a(),L=i("p"),L.textContent=Jt,We=a(),o(P.$$.fragment),Ee=a(),q=i("p"),q.textContent=Tt,Fe=a(),o(D.$$.fragment),Ye=a(),K=i("p"),K.textContent=jt,Se=a(),J=i("iframe"),Ve=a(),o(O.$$.fragment),Qe=a(),ee=i("p"),ee.textContent=ht,ze=a(),o(te.$$.fragment),Ne=a(),T=i("iframe"),He=a(),o(le.$$.fragment),Le=a(),se=i("p"),se.textContent=Ct,Pe=a(),o(ae.$$.fragment),qe=a(),j=i("iframe"),De=a(),ne=i("p"),ne.textContent=bt,Ke=a(),o(ie.$$.fragment),Oe=a(),re=i("p"),re.textContent=gt,et=a(),Me=i("p"),this.h()},l(e){const t=kt("svelte-u9bgzb",document.head);U=r(t,"META",{name:!0,content:!0}),t.forEach(l),h=n(e),C=r(e,"P",{}),oe(C).forEach(l),ye=n(e),m(I.$$.fragment,e),we=n(e),b=r(e,"P",{"data-svelte-h":!0}),p(b)!=="svelte-v9iq5e"&&(b.textContent=st),ce=n(e),g=r(e,"P",{"data-svelte-h":!0}),p(g)!=="svelte-iqdzey"&&(g.textContent=at),de=n(e),m(f.$$.fragment,e),Ue=n(e),G=r(e,"P",{"data-svelte-h":!0}),p(G)!=="svelte-14hk95q"&&(G.textContent=nt),Je=n(e),B=r(e,"UL",{"data-svelte-h":!0}),p(B)!=="svelte-1vd6wz6"&&(B.innerHTML=it),Te=n(e),Z=r(e,"P",{"data-svelte-h":!0}),p(Z)!=="svelte-pzoe2y"&&(Z.textContent=rt),je=n(e),R=r(e,"UL",{"data-svelte-h":!0}),p(R)!=="svelte-1hif2as"&&(R.innerHTML=pt),ue=n(e),$=r(e,"P",{"data-svelte-h":!0}),p($)!=="svelte-ac9u4w"&&($.textContent=Mt),he=n(e),m(k.$$.fragment,e),fe=n(e),m(A.$$.fragment,e),Ce=n(e),m(v.$$.fragment,e),Ie=n(e),_=r(e,"P",{"data-svelte-h":!0}),p(_)!=="svelte-1pnmwnq"&&(_.textContent=ot),be=n(e),m(x.$$.fragment,e),ge=n(e),X=r(e,"P",{"data-svelte-h":!0}),p(X)!=="svelte-ths948"&&(X.textContent=mt),Ge=n(e),m(W.$$.fragment,e),Be=n(e),m(E.$$.fragment,e),Ze=n(e),F=r(e,"P",{"data-svelte-h":!0}),p(F)!=="svelte-1uukcnx"&&(F.innerHTML=yt),Re=n(e),m(Y.$$.fragment,e),$e=n(e),S=r(e,"P",{"data-svelte-h":!0}),p(S)!=="svelte-1tdjaxe"&&(S.innerHTML=wt),ke=n(e),V=r(e,"UL",{"data-svelte-h":!0}),p(V)!=="svelte-1b1t69f"&&(V.innerHTML=ct),Ae=n(e),Q=r(e,"P",{"data-svelte-h":!0}),p(Q)!=="svelte-7howml"&&(Q.textContent=dt),ve=n(e),m(z.$$.fragment,e),_e=n(e),N=r(e,"OL",{"data-svelte-h":!0}),p(N)!=="svelte-4ukx5l"&&(N.innerHTML=Ut),xe=n(e),m(H.$$.fragment,e),Xe=n(e),L=r(e,"P",{"data-svelte-h":!0}),p(L)!=="svelte-5lvuy8"&&(L.textContent=Jt),We=n(e),m(P.$$.fragment,e),Ee=n(e),q=r(e,"P",{"data-svelte-h":!0}),p(q)!=="svelte-jhf03w"&&(q.textContent=Tt),Fe=n(e),m(D.$$.fragment,e),Ye=n(e),K=r(e,"P",{"data-svelte-h":!0}),p(K)!=="svelte-jlmna3"&&(K.textContent=jt),Se=n(e),J=r(e,"IFRAME",{src:!0,title:!0,width:!0,height:!0,frameborder:!0,allow:!0}),oe(J).forEach(l),Ve=n(e),m(O.$$.fragment,e),Qe=n(e),ee=r(e,"P",{"data-svelte-h":!0}),p(ee)!=="svelte-mrpax"&&(ee.textContent=ht),ze=n(e),m(te.$$.fragment,e),Ne=n(e),T=r(e,"IFRAME",{src:!0,title:!0,width:!0,height:!0,frameborder:!0,allow:!0}),oe(T).forEach(l),He=n(e),m(le.$$.fragment,e),Le=n(e),se=r(e,"P",{"data-svelte-h":!0}),p(se)!=="svelte-18k4wdv"&&(se.textContent=Ct),Pe=n(e),m(ae.$$.fragment,e),qe=n(e),j=r(e,"IFRAME",{src:!0,title:!0,width:!0,height:!0,frameborder:!0,allow:!0}),oe(j).forEach(l),De=n(e),ne=r(e,"P",{"data-svelte-h":!0}),p(ne)!=="svelte-fzl1fe"&&(ne.textContent=bt),Ke=n(e),m(ie.$$.fragment,e),Oe=n(e),re=r(e,"P",{"data-svelte-h":!0}),p(re)!=="svelte-chd5s6"&&(re.textContent=gt),et=n(e),Me=r(e,"P",{}),oe(Me).forEach(l),this.h()},h(){M(U,"name","hf:doc:metadata"),M(U,"content",Et),lt(J.src,ut="https://marimo.app/gh/huggingface/notebooks/main/e?entrypoint=course%2Fen%2Fchapter13%2Fgrpo_length.py&embed=true&show-chrome=false")||M(J,"src",ut),M(J,"title","Marimo Notebook"),M(J,"width","100%"),M(J,"height","800px"),M(J,"frameborder","0"),M(J,"allow","clipboard-write"),lt(T.src,ft="https://marimo.app/gh/huggingface/notebooks/main/e?entrypoint=course%2Fen%2Fchapter13%2Fgrpo_math.py&embed=true&show-chrome=false")||M(T,"src",ft),M(T,"title","Marimo Notebook"),M(T,"width","100%"),M(T,"height","800px"),M(T,"frameborder","0"),M(T,"allow","clipboard-write"),lt(j.src,It="https://marimo.app/gh/huggingface/notebooks/main/e?entrypoint=course%2Fen%2Fchapter13%2Fgrpo_format.py&embed=true&show-chrome=false")||M(j,"src",It),M(j,"title","Marimo Notebook"),M(j,"width","100%"),M(j,"height","800px"),M(j,"frameborder","0"),M(j,"allow","clipboard-write")},m(e,t){At(document.head,U),s(e,h,t),s(e,C,t),s(e,ye,t),y(I,e,t),s(e,we,t),s(e,b,t),s(e,ce,t),s(e,g,t),s(e,de,t),y(f,e,t),s(e,Ue,t),s(e,G,t),s(e,Je,t),s(e,B,t),s(e,Te,t),s(e,Z,t),s(e,je,t),s(e,R,t),s(e,ue,t),s(e,$,t),s(e,he,t),y(k,e,t),s(e,fe,t),y(A,e,t),s(e,Ce,t),y(v,e,t),s(e,Ie,t),s(e,_,t),s(e,be,t),y(x,e,t),s(e,ge,t),s(e,X,t),s(e,Ge,t),y(W,e,t),s(e,Be,t),y(E,e,t),s(e,Ze,t),s(e,F,t),s(e,Re,t),y(Y,e,t),s(e,$e,t),s(e,S,t),s(e,ke,t),s(e,V,t),s(e,Ae,t),s(e,Q,t),s(e,ve,t),y(z,e,t),s(e,_e,t),s(e,N,t),s(e,xe,t),y(H,e,t),s(e,Xe,t),s(e,L,t),s(e,We,t),y(P,e,t),s(e,Ee,t),s(e,q,t),s(e,Fe,t),y(D,e,t),s(e,Ye,t),s(e,K,t),s(e,Se,t),s(e,J,t),s(e,Ve,t),y(O,e,t),s(e,Qe,t),s(e,ee,t),s(e,ze,t),y(te,e,t),s(e,Ne,t),s(e,T,t),s(e,He,t),y(le,e,t),s(e,Le,t),s(e,se,t),s(e,Pe,t),y(ae,e,t),s(e,qe,t),s(e,j,t),s(e,De,t),s(e,ne,t),s(e,Ke,t),y(ie,e,t),s(e,Oe,t),s(e,re,t),s(e,et,t),s(e,Me,t),tt=!0},p(e,[t]){const Gt={};t&2&&(Gt.$$scope={dirty:t,ctx:e}),f.$set(Gt)},i(e){tt||(w(I.$$.fragment,e),w(f.$$.fragment,e),w(k.$$.fragment,e),w(A.$$.fragment,e),w(v.$$.fragment,e),w(x.$$.fragment,e),w(W.$$.fragment,e),w(E.$$.fragment,e),w(Y.$$.fragment,e),w(z.$$.fragment,e),w(H.$$.fragment,e),w(P.$$.fragment,e),w(D.$$.fragment,e),w(O.$$.fragment,e),w(te.$$.fragment,e),w(le.$$.fragment,e),w(ae.$$.fragment,e),w(ie.$$.fragment,e),tt=!0)},o(e){c(I.$$.fragment,e),c(f.$$.fragment,e),c(k.$$.fragment,e),c(A.$$.fragment,e),c(v.$$.fragment,e),c(x.$$.fragment,e),c(W.$$.fragment,e),c(E.$$.fragment,e),c(Y.$$.fragment,e),c(z.$$.fragment,e),c(H.$$.fragment,e),c(P.$$.fragment,e),c(D.$$.fragment,e),c(O.$$.fragment,e),c(te.$$.fragment,e),c(le.$$.fragment,e),c(ae.$$.fragment,e),c(ie.$$.fragment,e),tt=!1},d(e){e&&(l(h),l(C),l(ye),l(we),l(b),l(ce),l(g),l(de),l(Ue),l(G),l(Je),l(B),l(Te),l(Z),l(je),l(R),l(ue),l($),l(he),l(fe),l(Ce),l(Ie),l(_),l(be),l(ge),l(X),l(Ge),l(Be),l(Ze),l(F),l(Re),l($e),l(S),l(ke),l(V),l(Ae),l(Q),l(ve),l(_e),l(N),l(xe),l(Xe),l(L),l(We),l(Ee),l(q),l(Fe),l(Ye),l(K),l(Se),l(J),l(Ve),l(Qe),l(ee),l(ze),l(Ne),l(T),l(He),l(Le),l(se),l(Pe),l(qe),l(j),l(De),l(ne),l(Ke),l(Oe),l(re),l(et),l(Me)),l(U),d(I,e),d(f,e),d(k,e),d(A,e),d(v,e),d(x,e),d(W,e),d(E,e),d(Y,e),d(z,e),d(H,e),d(P,e),d(D,e),d(O,e),d(te,e),d(le,e),d(ae,e),d(ie,e)}}}const Et='{"title":"Implementing GRPO in TRL","local":"implementing-grpo-in-trl","sections":[{"title":"Key Components","local":"key-components","sections":[{"title":"1. Dataset Format","local":"1-dataset-format","sections":[],"depth":3},{"title":"2. Reward Function","local":"2-reward-function","sections":[],"depth":3},{"title":"3. Training Configuration","local":"3-training-configuration","sections":[],"depth":3}],"depth":2},{"title":"Tips for Success","local":"tips-for-success","sections":[],"depth":2},{"title":"Reward Function Design","local":"reward-function-design","sections":[{"title":"1. Length-Based Rewards","local":"1-length-based-rewards","sections":[],"depth":3}],"depth":2},{"title":"2. Rule-Based Rewards for Verifiable Tasks","local":"2-rule-based-rewards-for-verifiable-tasks","sections":[],"depth":2},{"title":"3. Format-Based Rewards","local":"3-format-based-rewards","sections":[],"depth":2},{"title":"That’s it!","local":"thats-it","sections":[],"depth":2}],"depth":1}';function Ft(me){return Zt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Nt extends Rt{constructor(U){super(),$t(this,U,Ft,Wt,Bt,{})}}export{Nt as component};
