import{s as pi,f as mi,o as Mi,n as ts}from"../chunks/scheduler.37c15a92.js";import{S as ci,i as hi,g as M,s as t,r as h,m as ei,H as si,A as yi,h as c,f as s,c as n,j as ta,u as y,x as u,n as li,E as ai,k as Be,y as Dt,a as l,v as d,t as m,b as Pl,d as p,w,p as Ol}from"../chunks/index.2bf4358c.js";import{T as as}from"../chunks/Tip.363c041f.js";import{Y as Lt}from"../chunks/Youtube.1e50a667.js";import{C as U}from"../chunks/CodeBlock.4f5fc1ad.js";import{C as ii}from"../chunks/CourseFloatingBanner.15ba07e6.js";import{F as di}from"../chunks/FrameworkSwitchCourse.8d4d4ab6.js";import{H as fe}from"../chunks/Heading.8ada512a.js";function wi(Z){let r,j;return r=new ii({props:{chapter:7,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"}]}}),{c(){h(r.$$.fragment)},l(o){y(r.$$.fragment,o)},m(o,I){d(r,o,I),j=!0},i(o){j||(p(r.$$.fragment,o),j=!0)},o(o){m(r.$$.fragment,o),j=!1},d(o){w(r,o)}}}function ui(Z){let r,j;return r=new ii({props:{chapter:7,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"}]}}),{c(){h(r.$$.fragment)},l(o){y(r.$$.fragment,o)},m(o,I){d(r,o,I),j=!0},i(o){j||(p(r.$$.fragment,o),j=!0)},o(o){m(r.$$.fragment,o),j=!1},d(o){w(r,o)}}}function Ji(Z){let r,j="‚úèÔ∏è <strong>Try it out!</strong> Change the random seed in the <code>Dataset.shuffle()</code> command to explore other reviews in the corpus. If you‚Äôre a Spanish speaker, take a look at some of the reviews in <code>spanish_dataset</code> to see if the titles also seem like reasonable summaries.";return{c(){r=M("p"),r.innerHTML=j},l(o){r=c(o,"P",{"data-svelte-h":!0}),u(r)!=="svelte-1oj9qcf"&&(r.innerHTML=j)},m(o,I){l(o,r,I)},p:ts,d(o){o&&s(r)}}}function Ti(Z){let r,j="‚úèÔ∏è <strong>Try it out!</strong> Once you‚Äôve worked through this section, see how well mT5 compares to mBART by fine-tuning the latter with the same techniques. For bonus points, you can also try fine-tuning T5 on just the English reviews. Since T5 has a special prefix prompt, you‚Äôll need to prepend <code>summarize:</code> to the input examples in the preprocessing steps below.";return{c(){r=M("p"),r.innerHTML=j},l(o){r=c(o,"P",{"data-svelte-h":!0}),u(r)!=="svelte-rgei27"&&(r.innerHTML=j)},m(o,I){l(o,r,I)},p:ts,d(o){o&&s(r)}}}function Ui(Z){let r,j="üí° In the early stages of your NLP projects, a good practice is to train a class of ‚Äúsmall‚Äù models on a small sample of data. This allows you to debug and iterate faster toward an end-to-end workflow. Once you are confident in the results, you can always scale up the model by simply changing the model checkpoint!";return{c(){r=M("p"),r.textContent=j},l(o){r=c(o,"P",{"data-svelte-h":!0}),u(r)!=="svelte-gxjsqb"&&(r.textContent=j)},m(o,I){l(o,r,I)},p:ts,d(o){o&&s(r)}}}function ji(Z){let r,j="üí° You may have noticed that we used <code>batched=True</code> in our <code>Dataset.map()</code> function above. This encodes the examples in batches of 1,000 (the default) and allows you to make use of the multithreading capabilities of the fast tokenizers in ü§ó Transformers. Where possible, try using <code>batched=True</code> to get the most out of your preprocessing!";return{c(){r=M("p"),r.innerHTML=j},l(o){r=c(o,"P",{"data-svelte-h":!0}),u(r)!=="svelte-1m43pbb"&&(r.innerHTML=j)},m(o,I){l(o,r,I)},p:ts,d(o){o&&s(r)}}}function bi(Z){let r,j='üôã Don‚Äôt worry if this is the first time you‚Äôve heard of precision and recall ‚Äî we‚Äôll go through some explicit examples together to make it all clear. These metrics are usually encountered in classification tasks, so if you want to understand how precision and recall are defined in that context, we recommend checking out the <code>scikit-learn</code> <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html" rel="nofollow">guides</a>.';return{c(){r=M("p"),r.innerHTML=j},l(o){r=c(o,"P",{"data-svelte-h":!0}),u(r)!=="svelte-1ggbwee"&&(r.innerHTML=j)},m(o,I){l(o,r,I)},p:ts,d(o){o&&s(r)}}}function fi(Z){let r,j="‚úèÔ∏è <strong>Try it out!</strong> Create your own example of a generated and reference summary and see if the resulting ROUGE scores agree with a manual calculation based on the formulas for precision and recall. For bonus points, split the text into bigrams and compare the precision and recall for the <code>rouge2</code> metric.";return{c(){r=M("p"),r.innerHTML=j},l(o){r=c(o,"P",{"data-svelte-h":!0}),u(r)!=="svelte-1e616sx"&&(r.innerHTML=j)},m(o,I){l(o,r,I)},p:ts,d(o){o&&s(r)}}}function Ii(Z){let r,j,o,I="Fine-tuning a model for summarization is very similar to the other tasks we‚Äôve covered in this chapter. The first thing we need to do is load the pretrained model from the <code>mt5-small</code> checkpoint. Since summarization is a sequence-to-sequence task, we can load the model with the <code>TFAutoModelForSeq2SeqLM</code> class, which will automatically download and cache the weights:",v,C,$;return r=new fe({props:{title:"Fine-tuning mT5 with Keras",local:"fine-tuning-mt5-with-keras",headingTag:"h2"}}),C=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxMlNlcUxNJTBBJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcTJTZXFMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfY2hlY2twb2ludCk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`,wrap:!1}}),{c(){h(r.$$.fragment),j=t(),o=M("p"),o.innerHTML=I,v=t(),h(C.$$.fragment)},l(f){y(r.$$.fragment,f),j=n(f),o=c(f,"P",{"data-svelte-h":!0}),u(o)!=="svelte-1a7war5"&&(o.innerHTML=I),v=n(f),y(C.$$.fragment,f)},m(f,k){d(r,f,k),l(f,j,k),l(f,o,k),l(f,v,k),d(C,f,k),$=!0},i(f){$||(p(r.$$.fragment,f),p(C.$$.fragment,f),$=!0)},o(f){m(r.$$.fragment,f),m(C.$$.fragment,f),$=!1},d(f){f&&(s(j),s(o),s(v)),w(r,f),w(C,f)}}}function gi(Z){let r,j,o,I="Fine-tuning a model for summarization is very similar to the other tasks we‚Äôve covered in this chapter. The first thing we need to do is load the pretrained model from the <code>mt5-small</code> checkpoint. Since summarization is a sequence-to-sequence task, we can load the model with the <code>AutoModelForSeq2SeqLM</code> class, which will automatically download and cache the weights:",v,C,$;return r=new fe({props:{title:"Fine-tuning mT5 with the Trainer API",local:"fine-tuning-mt5-with-the-trainer-api",headingTag:"h2"}}),C=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcTJTZXFMTSUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZChtb2RlbF9jaGVja3BvaW50KQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`,wrap:!1}}),{c(){h(r.$$.fragment),j=t(),o=M("p"),o.innerHTML=I,v=t(),h(C.$$.fragment)},l(f){y(r.$$.fragment,f),j=n(f),o=c(f,"P",{"data-svelte-h":!0}),u(o)!=="svelte-13fz6t3"&&(o.innerHTML=I),v=n(f),y(C.$$.fragment,f)},m(f,k){d(r,f,k),l(f,j,k),l(f,o,k),l(f,v,k),d(C,f,k),$=!0},i(f){$||(p(r.$$.fragment,f),p(C.$$.fragment,f),$=!0)},o(f){m(r.$$.fragment,f),m(C.$$.fragment,f),$=!1},d(f){f&&(s(j),s(o),s(v)),w(r,f),w(C,f)}}}function Ci(Z){let r,j='üí° If you‚Äôre wondering why you don‚Äôt see any warnings about fine-tuning the model on a downstream task, that‚Äôs because for sequence-to-sequence tasks we keep all the weights of the network. Compare this to our text classification model in <a href="/course/chapter3">Chapter 3</a>, where the head of the pretrained model was replaced with a randomly initialized network.';return{c(){r=M("p"),r.innerHTML=j},l(o){r=c(o,"P",{"data-svelte-h":!0}),u(r)!=="svelte-1dyy1at"&&(r.innerHTML=j)},m(o,I){l(o,r,I)},p:ts,d(o){o&&s(r)}}}function ti(Z){let r,j="We‚Äôll need to generate summaries in order to compute ROUGE scores during training. Fortunately, ü§ó Transformers provides dedicated <code>Seq2SeqTrainingArguments</code> and <code>Seq2SeqTrainer</code> classes that can do this for us automatically! To see how this works, let‚Äôs first define the hyperparameters and other arguments for our experiments:",o,I,v,C,$='Here, the <code>predict_with_generate</code> argument has been set to indicate that we should generate summaries during evaluation so that we can compute ROUGE scores for each epoch. As discussed in <a href="/course/chapter1">Chapter 1</a>, the decoder performs inference by predicting tokens one by one, and this is implemented by the model‚Äôs <code>generate()</code> method. Setting <code>predict_with_generate=True</code> tells the <code>Seq2SeqTrainer</code> to use that method for evaluation. We‚Äôve also adjusted some of the default hyperparameters, like the learning rate, number of epochs, and weight decay, and we‚Äôve set the <code>save_total_limit</code> option to only save up to 3 checkpoints during training ‚Äî this is because even the ‚Äúsmall‚Äù version of mT5 uses around a GB of hard drive space, and we can save a bit of room by limiting the number of copies we save.',f,k,R='The <code>push_to_hub=True</code> argument will allow us to push the model to the Hub after training; you‚Äôll find the repository under your user profile in the location defined by <code>output_dir</code>. Note that you can specify the name of the repository you want to push to with the <code>hub_model_id</code> argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the <a href="https://huggingface.co/huggingface-course" rel="nofollow"><code>huggingface-course</code> organization</a>, we added <code>hub_model_id=&quot;huggingface-course/mt5-finetuned-amazon-en-es&quot;</code> to <code>Seq2SeqTrainingArguments</code>.',G,z,S="The next thing we need to do is provide the trainer with a <code>compute_metrics()</code> function so that we can evaluate our model during training. For summarization this is a bit more involved than simply calling <code>rouge_score.compute()</code> on the model‚Äôs predictions, since we need to <em>decode</em> the outputs and labels into text before we can compute the ROUGE scores. The following function does exactly that, and also makes use of the <code>sent_tokenize()</code> function from <code>nltk</code> to separate the summary sentences with newlines:",N,_,B;return I=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFNlcTJTZXFUcmFpbmluZ0FyZ3VtZW50cyUwQSUwQWJhdGNoX3NpemUlMjAlM0QlMjA4JTBBbnVtX3RyYWluX2Vwb2NocyUyMCUzRCUyMDglMEElMjMlMjBTaG93JTIwdGhlJTIwdHJhaW5pbmclMjBsb3NzJTIwd2l0aCUyMGV2ZXJ5JTIwZXBvY2glMEFsb2dnaW5nX3N0ZXBzJTIwJTNEJTIwbGVuKHRva2VuaXplZF9kYXRhc2V0cyU1QiUyMnRyYWluJTIyJTVEKSUyMCUyRiUyRiUyMGJhdGNoX3NpemUlMEFtb2RlbF9uYW1lJTIwJTNEJTIwbW9kZWxfY2hlY2twb2ludC5zcGxpdCglMjIlMkYlMjIpJTVCLTElNUQlMEElMEFhcmdzJTIwJTNEJTIwU2VxMlNlcVRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0RmJTIyJTdCbW9kZWxfbmFtZSU3RC1maW5ldHVuZWQtYW1hem9uLWVuLWVzJTIyJTJDJTBBJTIwJTIwJTIwJTIwZXZhbHVhdGlvbl9zdHJhdGVneSUzRCUyMmVwb2NoJTIyJTJDJTBBJTIwJTIwJTIwJTIwbGVhcm5pbmdfcmF0ZSUzRDUuNmUtNSUyQyUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfdHJhaW5fYmF0Y2hfc2l6ZSUzRGJhdGNoX3NpemUlMkMlMEElMjAlMjAlMjAlMjBwZXJfZGV2aWNlX2V2YWxfYmF0Y2hfc2l6ZSUzRGJhdGNoX3NpemUlMkMlMEElMjAlMjAlMjAlMjB3ZWlnaHRfZGVjYXklM0QwLjAxJTJDJTBBJTIwJTIwJTIwJTIwc2F2ZV90b3RhbF9saW1pdCUzRDMlMkMlMEElMjAlMjAlMjAlMjBudW1fdHJhaW5fZXBvY2hzJTNEbnVtX3RyYWluX2Vwb2NocyUyQyUwQSUyMCUyMCUyMCUyMHByZWRpY3Rfd2l0aF9nZW5lcmF0ZSUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBsb2dnaW5nX3N0ZXBzJTNEbG9nZ2luZ19zdGVwcyUyQyUwQSUyMCUyMCUyMCUyMHB1c2hfdG9faHViJTNEVHJ1ZSUyQyUwQSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Seq2SeqTrainingArguments

batch_size = <span class="hljs-number">8</span>
num_train_epochs = <span class="hljs-number">8</span>
<span class="hljs-comment"># Show the training loss with every epoch</span>
logging_steps = <span class="hljs-built_in">len</span>(tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
model_name = model_checkpoint.split(<span class="hljs-string">&quot;/&quot;</span>)[-<span class="hljs-number">1</span>]

args = Seq2SeqTrainingArguments(
    output_dir=<span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-finetuned-amazon-en-es&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">5.6e-5</span>,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=<span class="hljs-number">0.01</span>,
    save_total_limit=<span class="hljs-number">3</span>,
    num_train_epochs=num_train_epochs,
    predict_with_generate=<span class="hljs-literal">True</span>,
    logging_steps=logging_steps,
    push_to_hub=<span class="hljs-literal">True</span>,
)`,wrap:!1}}),_=new U({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBJTBBJTBBZGVmJTIwY29tcHV0ZV9tZXRyaWNzKGV2YWxfcHJlZCklM0ElMEElMjAlMjAlMjAlMjBwcmVkaWN0aW9ucyUyQyUyMGxhYmVscyUyMCUzRCUyMGV2YWxfcHJlZCUwQSUyMCUyMCUyMCUyMCUyMyUyMERlY29kZSUyMGdlbmVyYXRlZCUyMHN1bW1hcmllcyUyMGludG8lMjB0ZXh0JTBBJTIwJTIwJTIwJTIwZGVjb2RlZF9wcmVkcyUyMCUzRCUyMHRva2VuaXplci5iYXRjaF9kZWNvZGUocHJlZGljdGlvbnMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEElMjAlMjAlMjAlMjAlMjMlMjBSZXBsYWNlJTIwLTEwMCUyMGluJTIwdGhlJTIwbGFiZWxzJTIwYXMlMjB3ZSUyMGNhbid0JTIwZGVjb2RlJTIwdGhlbSUwQSUyMCUyMCUyMCUyMGxhYmVscyUyMCUzRCUyMG5wLndoZXJlKGxhYmVscyUyMCElM0QlMjAtMTAwJTJDJTIwbGFiZWxzJTJDJTIwdG9rZW5pemVyLnBhZF90b2tlbl9pZCklMEElMjAlMjAlMjAlMjAlMjMlMjBEZWNvZGUlMjByZWZlcmVuY2UlMjBzdW1tYXJpZXMlMjBpbnRvJTIwdGV4dCUwQSUyMCUyMCUyMCUyMGRlY29kZWRfbGFiZWxzJTIwJTNEJTIwdG9rZW5pemVyLmJhdGNoX2RlY29kZShsYWJlbHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEElMjAlMjAlMjAlMjAlMjMlMjBST1VHRSUyMGV4cGVjdHMlMjBhJTIwbmV3bGluZSUyMGFmdGVyJTIwZWFjaCUyMHNlbnRlbmNlJTBBJTIwJTIwJTIwJTIwZGVjb2RlZF9wcmVkcyUyMCUzRCUyMCU1QiUyMiU1Q24lMjIuam9pbihzZW50X3Rva2VuaXplKHByZWQuc3RyaXAoKSkpJTIwZm9yJTIwcHJlZCUyMGluJTIwZGVjb2RlZF9wcmVkcyU1RCUwQSUyMCUyMCUyMCUyMGRlY29kZWRfbGFiZWxzJTIwJTNEJTIwJTVCJTIyJTVDbiUyMi5qb2luKHNlbnRfdG9rZW5pemUobGFiZWwuc3RyaXAoKSkpJTIwZm9yJTIwbGFiZWwlMjBpbiUyMGRlY29kZWRfbGFiZWxzJTVEJTBBJTIwJTIwJTIwJTIwJTIzJTIwQ29tcHV0ZSUyMFJPVUdFJTIwc2NvcmVzJTBBJTIwJTIwJTIwJTIwcmVzdWx0JTIwJTNEJTIwcm91Z2Vfc2NvcmUuY29tcHV0ZSglMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBwcmVkaWN0aW9ucyUzRGRlY29kZWRfcHJlZHMlMkMlMjByZWZlcmVuY2VzJTNEZGVjb2RlZF9sYWJlbHMlMkMlMjB1c2Vfc3RlbW1lciUzRFRydWUlMEElMjAlMjAlMjAlMjApJTBBJTIwJTIwJTIwJTIwJTIzJTIwRXh0cmFjdCUyMHRoZSUyMG1lZGlhbiUyMHNjb3JlcyUwQSUyMCUyMCUyMCUyMHJlc3VsdCUyMCUzRCUyMCU3QmtleSUzQSUyMHZhbHVlLm1pZC5mbWVhc3VyZSUyMColMjAxMDAlMjBmb3IlMjBrZXklMkMlMjB2YWx1ZSUyMGluJTIwcmVzdWx0Lml0ZW1zKCklN0QlMEElMjAlMjAlMjAlMjByZXR1cm4lMjAlN0JrJTNBJTIwcm91bmQodiUyQyUyMDQpJTIwZm9yJTIwayUyQyUyMHYlMjBpbiUyMHJlc3VsdC5pdGVtcygpJTdE",highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-comment"># Decode generated summaries into text</span>
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># Replace -100 in the labels as we can&#x27;t decode them</span>
    labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
    <span class="hljs-comment"># Decode reference summaries into text</span>
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># ROUGE expects a newline after each sentence</span>
    decoded_preds = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(pred.strip())) <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> decoded_preds]
    decoded_labels = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(label.strip())) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> decoded_labels]
    <span class="hljs-comment"># Compute ROUGE scores</span>
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=<span class="hljs-literal">True</span>
    )
    <span class="hljs-comment"># Extract the median scores</span>
    result = {key: value.mid.fmeasure * <span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> result.items()}
    <span class="hljs-keyword">return</span> {k: <span class="hljs-built_in">round</span>(v, <span class="hljs-number">4</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> result.items()}`,wrap:!1}}),{c(){r=M("p"),r.innerHTML=j,o=t(),h(I.$$.fragment),v=t(),C=M("p"),C.innerHTML=$,f=t(),k=M("p"),k.innerHTML=R,G=t(),z=M("p"),z.innerHTML=S,N=t(),h(_.$$.fragment)},l(g){r=c(g,"P",{"data-svelte-h":!0}),u(r)!=="svelte-lra7fb"&&(r.innerHTML=j),o=n(g),y(I.$$.fragment,g),v=n(g),C=c(g,"P",{"data-svelte-h":!0}),u(C)!=="svelte-1kfg486"&&(C.innerHTML=$),f=n(g),k=c(g,"P",{"data-svelte-h":!0}),u(k)!=="svelte-9f4al4"&&(k.innerHTML=R),G=n(g),z=c(g,"P",{"data-svelte-h":!0}),u(z)!=="svelte-1w4eq0d"&&(z.innerHTML=S),N=n(g),y(_.$$.fragment,g)},m(g,x){l(g,r,x),l(g,o,x),d(I,g,x),l(g,v,x),l(g,C,x),l(g,f,x),l(g,k,x),l(g,G,x),l(g,z,x),l(g,N,x),d(_,g,x),B=!0},i(g){B||(p(I.$$.fragment,g),p(_.$$.fragment,g),B=!0)},o(g){m(I.$$.fragment,g),m(_.$$.fragment,g),B=!1},d(g){g&&(s(r),s(o),s(v),s(C),s(f),s(k),s(G),s(z),s(N)),w(I,g),w(_,g)}}}function ki(Z){let r,j;return r=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvckZvclNlcTJTZXElMEElMEFkYXRhX2NvbGxhdG9yJTIwJTNEJTIwRGF0YUNvbGxhdG9yRm9yU2VxMlNlcSh0b2tlbml6ZXIlMkMlMjBtb2RlbCUzRG1vZGVsJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJ0ZiUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`,wrap:!1}}),{c(){h(r.$$.fragment)},l(o){y(r.$$.fragment,o)},m(o,I){d(r,o,I),j=!0},i(o){j||(p(r.$$.fragment,o),j=!0)},o(o){m(r.$$.fragment,o),j=!1},d(o){w(r,o)}}}function Zi(Z){let r,j;return r=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvckZvclNlcTJTZXElMEElMEFkYXRhX2NvbGxhdG9yJTIwJTNEJTIwRGF0YUNvbGxhdG9yRm9yU2VxMlNlcSh0b2tlbml6ZXIlMkMlMjBtb2RlbCUzRG1vZGVsKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)`,wrap:!1}}),{c(){h(r.$$.fragment)},l(o){y(r.$$.fragment,o)},m(o,I){d(r,o,I),j=!0},i(o){j||(p(r.$$.fragment,o),j=!0)},o(o){m(r.$$.fragment,o),j=!1},d(o){w(r,o)}}}function vi(Z){let r,j="We‚Äôre almost ready to train! We just need to convert our datasets to <code>tf.data.Dataset</code>s using the data collator we defined above, and then <code>compile()</code> and <code>fit()</code> the model. First, the datasets:",o,I,v,C,$="Now, we define our training hyperparameters and compile:",f,k,R,G,z="And finally, we fit the model. We use a <code>PushToHubCallback</code> to save the model to the Hub after each epoch, which will allow us to use it for inference later:",S,N,_,B,g='We got some loss values during training, but really we‚Äôd like to see the ROUGE metrics we computed earlier. To get those metrics, we‚Äôll need to generate outputs from the model and convert them to strings. Let‚Äôs build some lists of labels and predictions for the ROUGE metric to compare (note that if you get import errors for this section, you may need to<code>!pip install tqdm</code>). We‚Äôre also going to use a trick that dramatically increases performance - compiling our generation code with <a href="https://www.tensorflow.org/xla" rel="nofollow">XLA</a>, TensorFlow‚Äôs accelerated linear algebra compiler. XLA applies various optimizations to the model‚Äôs computation graph, and results in significant improvements to speed and memory usage. As described in the Hugging Face <a href="https://huggingface.co/blog/tf-xla-generate" rel="nofollow">blog</a>, XLA works best when our input shapes don‚Äôt vary too much. To handle this, we‚Äôll pad our inputs to multiples of 128, and make a new dataset with the padding collator, and then we‚Äôll apply the <code>@tf.function(jit_compile=True)</code> decorator to our generation function, which marks the whole function for compilation with XLA.',x,L,A,Q,K="Once we have our lists of label and prediction strings, computing the ROUGE score is easy:",X,q,E,Y,F;return I=new U({props:{code:"dGZfdHJhaW5fZGF0YXNldCUyMCUzRCUyMG1vZGVsLnByZXBhcmVfdGZfZGF0YXNldCglMEElMjAlMjAlMjAlMjB0b2tlbml6ZWRfZGF0YXNldHMlNUIlMjJ0cmFpbiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGNvbGxhdGVfZm4lM0RkYXRhX2NvbGxhdG9yJTJDJTBBJTIwJTIwJTIwJTIwc2h1ZmZsZSUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBiYXRjaF9zaXplJTNEOCUyQyUwQSklMEF0Zl9ldmFsX2RhdGFzZXQlMjAlM0QlMjBtb2RlbC5wcmVwYXJlX3RmX2RhdGFzZXQoJTBBJTIwJTIwJTIwJTIwdG9rZW5pemVkX2RhdGFzZXRzJTVCJTIydmFsaWRhdGlvbiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGNvbGxhdGVfZm4lM0RkYXRhX2NvbGxhdG9yJTJDJTBBJTIwJTIwJTIwJTIwc2h1ZmZsZSUzREZhbHNlJTJDJTBBJTIwJTIwJTIwJTIwYmF0Y2hfc2l6ZSUzRDglMkMlMEEp",highlighted:`tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">8</span>,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">8</span>,
)`,wrap:!1}}),k=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMGNyZWF0ZV9vcHRpbWl6ZXIlMEFpbXBvcnQlMjB0ZW5zb3JmbG93JTIwYXMlMjB0ZiUwQSUwQSUyMyUyMFRoZSUyMG51bWJlciUyMG9mJTIwdHJhaW5pbmclMjBzdGVwcyUyMGlzJTIwdGhlJTIwbnVtYmVyJTIwb2YlMjBzYW1wbGVzJTIwaW4lMjB0aGUlMjBkYXRhc2V0JTJDJTIwZGl2aWRlZCUyMGJ5JTIwdGhlJTIwYmF0Y2glMjBzaXplJTIwdGhlbiUyMG11bHRpcGxpZWQlMEElMjMlMjBieSUyMHRoZSUyMHRvdGFsJTIwbnVtYmVyJTIwb2YlMjBlcG9jaHMuJTIwTm90ZSUyMHRoYXQlMjB0aGUlMjB0Zl90cmFpbl9kYXRhc2V0JTIwaGVyZSUyMGlzJTIwYSUyMGJhdGNoZWQlMjB0Zi5kYXRhLkRhdGFzZXQlMkMlMEElMjMlMjBub3QlMjB0aGUlMjBvcmlnaW5hbCUyMEh1Z2dpbmclMjBGYWNlJTIwRGF0YXNldCUyQyUyMHNvJTIwaXRzJTIwbGVuKCklMjBpcyUyMGFscmVhZHklMjBudW1fc2FtcGxlcyUyMCUyRiUyRiUyMGJhdGNoX3NpemUuJTBBbnVtX3RyYWluX2Vwb2NocyUyMCUzRCUyMDglMEFudW1fdHJhaW5fc3RlcHMlMjAlM0QlMjBsZW4odGZfdHJhaW5fZGF0YXNldCklMjAqJTIwbnVtX3RyYWluX2Vwb2NocyUwQW1vZGVsX25hbWUlMjAlM0QlMjBtb2RlbF9jaGVja3BvaW50LnNwbGl0KCUyMiUyRiUyMiklNUItMSU1RCUwQSUwQW9wdGltaXplciUyQyUyMHNjaGVkdWxlJTIwJTNEJTIwY3JlYXRlX29wdGltaXplciglMEElMjAlMjAlMjAlMjBpbml0X2xyJTNENS42ZS01JTJDJTBBJTIwJTIwJTIwJTIwbnVtX3dhcm11cF9zdGVwcyUzRDAlMkMlMEElMjAlMjAlMjAlMjBudW1fdHJhaW5fc3RlcHMlM0RudW1fdHJhaW5fc3RlcHMlMkMlMEElMjAlMjAlMjAlMjB3ZWlnaHRfZGVjYXlfcmF0ZSUzRDAuMDElMkMlMEEpJTBBJTBBbW9kZWwuY29tcGlsZShvcHRpbWl6ZXIlM0RvcHRpbWl6ZXIpJTBBJTBBJTIzJTIwVHJhaW4lMjBpbiUyMG1peGVkLXByZWNpc2lvbiUyMGZsb2F0MTYlMEF0Zi5rZXJhcy5taXhlZF9wcmVjaXNpb24uc2V0X2dsb2JhbF9wb2xpY3koJTIybWl4ZWRfZmxvYXQxNiUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-comment"># The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied</span>
<span class="hljs-comment"># by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,</span>
<span class="hljs-comment"># not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.</span>
num_train_epochs = <span class="hljs-number">8</span>
num_train_steps = <span class="hljs-built_in">len</span>(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split(<span class="hljs-string">&quot;/&quot;</span>)[-<span class="hljs-number">1</span>]

optimizer, schedule = create_optimizer(
    init_lr=<span class="hljs-number">5.6e-5</span>,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_train_steps=num_train_steps,
    weight_decay_rate=<span class="hljs-number">0.01</span>,
)

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)

<span class="hljs-comment"># Train in mixed-precision float16</span>
tf.keras.mixed_precision.set_global_policy(<span class="hljs-string">&quot;mixed_float16&quot;</span>)`,wrap:!1}}),N=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5rZXJhc19jYWxsYmFja3MlMjBpbXBvcnQlMjBQdXNoVG9IdWJDYWxsYmFjayUwQSUwQWNhbGxiYWNrJTIwJTNEJTIwUHVzaFRvSHViQ2FsbGJhY2soJTBBJTIwJTIwJTIwJTIwb3V0cHV0X2RpciUzRGYlMjIlN0Jtb2RlbF9uYW1lJTdELWZpbmV0dW5lZC1hbWF6b24tZW4tZXMlMjIlMkMlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIlMEEpJTBBJTBBbW9kZWwuZml0KCUwQSUyMCUyMCUyMCUyMHRmX3RyYWluX2RhdGFzZXQlMkMlMjB2YWxpZGF0aW9uX2RhdGElM0R0Zl9ldmFsX2RhdGFzZXQlMkMlMjBjYWxsYmFja3MlM0QlNUJjYWxsYmFjayU1RCUyQyUyMGVwb2NocyUzRDglMEEp",highlighted:`<span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback

callback = PushToHubCallback(
    output_dir=<span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-finetuned-amazon-en-es&quot;</span>, tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=<span class="hljs-number">8</span>
)`,wrap:!1}}),L=new U({props:{code:"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",highlighted:`<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, pad_to_multiple_of=<span class="hljs-number">320</span>
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>],
    collate_fn=generation_data_collator,
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">8</span>,
    drop_remainder=<span class="hljs-literal">True</span>,
)


<span class="hljs-meta">@tf.function(<span class="hljs-params">jit_compile=<span class="hljs-literal">True</span></span>)</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_with_xla</span>(<span class="hljs-params">batch</span>):
    <span class="hljs-keyword">return</span> model.generate(
        input_ids=batch[<span class="hljs-string">&quot;input_ids&quot;</span>],
        attention_mask=batch[<span class="hljs-string">&quot;attention_mask&quot;</span>],
        max_new_tokens=<span class="hljs-number">32</span>,
    )


all_preds = []
all_labels = []
<span class="hljs-keyword">for</span> batch, labels <span class="hljs-keyword">in</span> tqdm(tf_generate_dataset):
    predictions = generate_with_xla(batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=<span class="hljs-literal">True</span>)
    labels = labels.numpy()
    labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)
    decoded_preds = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(pred.strip())) <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> decoded_preds]
    decoded_labels = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(label.strip())) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)`,wrap:!1}}),q=new U({props:{code:"cmVzdWx0JTIwJTNEJTIwcm91Z2Vfc2NvcmUuY29tcHV0ZSglMEElMjAlMjAlMjAlMjBwcmVkaWN0aW9ucyUzRGRlY29kZWRfcHJlZHMlMkMlMjByZWZlcmVuY2VzJTNEZGVjb2RlZF9sYWJlbHMlMkMlMjB1c2Vfc3RlbW1lciUzRFRydWUlMEEpJTBBcmVzdWx0JTIwJTNEJTIwJTdCa2V5JTNBJTIwdmFsdWUubWlkLmZtZWFzdXJlJTIwKiUyMDEwMCUyMGZvciUyMGtleSUyQyUyMHZhbHVlJTIwaW4lMjByZXN1bHQuaXRlbXMoKSU3RCUwQSU3QmslM0ElMjByb3VuZCh2JTJDJTIwNCklMjBmb3IlMjBrJTJDJTIwdiUyMGluJTIwcmVzdWx0Lml0ZW1zKCklN0Q=",highlighted:`result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=<span class="hljs-literal">True</span>
)
result = {key: value.mid.fmeasure * <span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> result.items()}
{k: <span class="hljs-built_in">round</span>(v, <span class="hljs-number">4</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> result.items()}`,wrap:!1}}),Y=new U({props:{code:"JTdCJ3JvdWdlMSclM0ElMjAzMS40ODE1JTJDJTIwJ3JvdWdlMiclM0ElMjAyNS40Mzg2JTJDJTIwJ3JvdWdlTCclM0ElMjAzMS40ODE1JTJDJTIwJ3JvdWdlTHN1bSclM0ElMjAzMS40ODE1JTdE",highlighted:'{&#x27;rouge1&#x27;: <span class="hljs-number">31.4815</span>, &#x27;rouge2&#x27;: <span class="hljs-number">25.4386</span>, &#x27;rougeL&#x27;: <span class="hljs-number">31.4815</span>, &#x27;rougeLsum&#x27;: <span class="hljs-number">31.4815</span>}',wrap:!1}}),{c(){r=M("p"),r.innerHTML=j,o=t(),h(I.$$.fragment),v=t(),C=M("p"),C.textContent=$,f=t(),h(k.$$.fragment),R=t(),G=M("p"),G.innerHTML=z,S=t(),h(N.$$.fragment),_=t(),B=M("p"),B.innerHTML=g,x=t(),h(L.$$.fragment),A=t(),Q=M("p"),Q.textContent=K,X=t(),h(q.$$.fragment),E=t(),h(Y.$$.fragment)},l(J){r=c(J,"P",{"data-svelte-h":!0}),u(r)!=="svelte-168i1"&&(r.innerHTML=j),o=n(J),y(I.$$.fragment,J),v=n(J),C=c(J,"P",{"data-svelte-h":!0}),u(C)!=="svelte-zonfr1"&&(C.textContent=$),f=n(J),y(k.$$.fragment,J),R=n(J),G=c(J,"P",{"data-svelte-h":!0}),u(G)!=="svelte-1v5wuv1"&&(G.innerHTML=z),S=n(J),y(N.$$.fragment,J),_=n(J),B=c(J,"P",{"data-svelte-h":!0}),u(B)!=="svelte-17pc15p"&&(B.innerHTML=g),x=n(J),y(L.$$.fragment,J),A=n(J),Q=c(J,"P",{"data-svelte-h":!0}),u(Q)!=="svelte-1elu3aa"&&(Q.textContent=K),X=n(J),y(q.$$.fragment,J),E=n(J),y(Y.$$.fragment,J)},m(J,V){l(J,r,V),l(J,o,V),d(I,J,V),l(J,v,V),l(J,C,V),l(J,f,V),d(k,J,V),l(J,R,V),l(J,G,V),l(J,S,V),d(N,J,V),l(J,_,V),l(J,B,V),l(J,x,V),d(L,J,V),l(J,A,V),l(J,Q,V),l(J,X,V),d(q,J,V),l(J,E,V),d(Y,J,V),F=!0},i(J){F||(p(I.$$.fragment,J),p(k.$$.fragment,J),p(N.$$.fragment,J),p(L.$$.fragment,J),p(q.$$.fragment,J),p(Y.$$.fragment,J),F=!0)},o(J){m(I.$$.fragment,J),m(k.$$.fragment,J),m(N.$$.fragment,J),m(L.$$.fragment,J),m(q.$$.fragment,J),m(Y.$$.fragment,J),F=!1},d(J){J&&(s(r),s(o),s(v),s(C),s(f),s(R),s(G),s(S),s(_),s(B),s(x),s(A),s(Q),s(X),s(E)),w(I,J),w(k,J),w(N,J),w(L,J),w(q,J),w(Y,J)}}}function Gi(Z){let r,j="We finally have all the ingredients we need to train with! We now simply need to instantiate the trainer with the standard arguments:",o,I,v,C,$="and launch our training run:",f,k,R,G,z="During training, you should see the training loss decrease and the ROUGE scores increase with each epoch. Once the training is complete, you can see the final ROUGE scores by running <code>Trainer.evaluate()</code>:",S,N,_,B,g,x,L="From the scores we can see that our model has handily outperformed our lead-3 baseline ‚Äî nice! The final thing to do is push the model weights to the Hub, as follows:",A,Q,K,X,q,E,Y='This will save the checkpoint and configuration files to <code>output_dir</code>, before uploading all the files to the Hub. By specifying the <code>tags</code> argument, we also ensure that the widget on the Hub will be one for a summarization pipeline instead of the default text generation one associated with the mT5 architecture (for more information about model tags, see the <a href="https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined" rel="nofollow">ü§ó Hub documentation</a>). The output from <code>trainer.push_to_hub()</code> is a URL to the Git commit hash, so you can easily see the changes that were made to the model repository!',F,J,V="To wrap up this section, let‚Äôs take a look at how we can also fine-tune mT5 using the low-level features provided by ü§ó Accelerate.",Me;return I=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFNlcTJTZXFUcmFpbmVyJTBBJTBBdHJhaW5lciUyMCUzRCUyMFNlcTJTZXFUcmFpbmVyKCUwQSUyMCUyMCUyMCUyMG1vZGVsJTJDJTBBJTIwJTIwJTIwJTIwYXJncyUyQyUwQSUyMCUyMCUyMCUyMHRyYWluX2RhdGFzZXQlM0R0b2tlbml6ZWRfZGF0YXNldHMlNUIlMjJ0cmFpbiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGV2YWxfZGF0YXNldCUzRHRva2VuaXplZF9kYXRhc2V0cyU1QiUyMnZhbGlkYXRpb24lMjIlNUQlMkMlMEElMjAlMjAlMjAlMjBkYXRhX2NvbGxhdG9yJTNEZGF0YV9jb2xsYXRvciUyQyUwQSUyMCUyMCUyMCUyMHRva2VuaXplciUzRHRva2VuaXplciUyQyUwQSUyMCUyMCUyMCUyMGNvbXB1dGVfbWV0cmljcyUzRGNvbXB1dGVfbWV0cmljcyUyQyUwQSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)`,wrap:!1}}),k=new U({props:{code:"dHJhaW5lci50cmFpbigp",highlighted:"trainer.train()",wrap:!1}}),N=new U({props:{code:"dHJhaW5lci5ldmFsdWF0ZSgp",highlighted:"trainer.evaluate()",wrap:!1}}),B=new U({props:{code:"JTdCJ2V2YWxfbG9zcyclM0ElMjAzLjAyODUyNDM5ODgwMzcxMSUyQyUwQSUyMCdldmFsX3JvdWdlMSclM0ElMjAxNi45NzI4JTJDJTBBJTIwJ2V2YWxfcm91Z2UyJyUzQSUyMDguMjk2OSUyQyUwQSUyMCdldmFsX3JvdWdlTCclM0ElMjAxNi44MzY2JTJDJTBBJTIwJ2V2YWxfcm91Z2VMc3VtJyUzQSUyMDE2Ljg1MSUyQyUwQSUyMCdldmFsX2dlbl9sZW4nJTNBJTIwMTAuMTU5NyUyQyUwQSUyMCdldmFsX3J1bnRpbWUnJTNBJTIwNi4xMDU0JTJDJTBBJTIwJ2V2YWxfc2FtcGxlc19wZXJfc2Vjb25kJyUzQSUyMDM4Ljk4MiUyQyUwQSUyMCdldmFsX3N0ZXBzX3Blcl9zZWNvbmQnJTNBJTIwNC45MTQlN0Q=",highlighted:`{<span class="hljs-string">&#x27;eval_loss&#x27;</span>: <span class="hljs-number">3.028524398803711</span>,
 <span class="hljs-string">&#x27;eval_rouge1&#x27;</span>: <span class="hljs-number">16.9728</span>,
 <span class="hljs-string">&#x27;eval_rouge2&#x27;</span>: <span class="hljs-number">8.2969</span>,
 <span class="hljs-string">&#x27;eval_rougeL&#x27;</span>: <span class="hljs-number">16.8366</span>,
 <span class="hljs-string">&#x27;eval_rougeLsum&#x27;</span>: <span class="hljs-number">16.851</span>,
 <span class="hljs-string">&#x27;eval_gen_len&#x27;</span>: <span class="hljs-number">10.1597</span>,
 <span class="hljs-string">&#x27;eval_runtime&#x27;</span>: <span class="hljs-number">6.1054</span>,
 <span class="hljs-string">&#x27;eval_samples_per_second&#x27;</span>: <span class="hljs-number">38.982</span>,
 <span class="hljs-string">&#x27;eval_steps_per_second&#x27;</span>: <span class="hljs-number">4.914</span>}`,wrap:!1}}),Q=new U({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yihjb21taXRfbWVzc2FnZSUzRCUyMlRyYWluaW5nJTIwY29tcGxldGUlMjIlMkMlMjB0YWdzJTNEJTIyc3VtbWFyaXphdGlvbiUyMik=",highlighted:'trainer.push_to_hub(<span class="hljs-attribute">commit_message</span>=<span class="hljs-string">&quot;Training complete&quot;</span>, <span class="hljs-attribute">tags</span>=<span class="hljs-string">&quot;summarization&quot;</span>)',wrap:!1}}),X=new U({props:{code:"J2h0dHBzJTNBJTJGJTJGaHVnZ2luZ2ZhY2UuY28lMkZodWdnaW5nZmFjZS1jb3Vyc2UlMkZtdDUtZmluZXR1bmVkLWFtYXpvbi1lbi1lcyUyRmNvbW1pdCUyRmFhMDUzNmI4MjliMjhlNzNlMWU0Yjk0YjhhNWFhY2VjNDIwZDQwZTAn",highlighted:'<span class="hljs-string">&#x27;https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0&#x27;</span>',wrap:!1}}),{c(){r=M("p"),r.textContent=j,o=t(),h(I.$$.fragment),v=t(),C=M("p"),C.textContent=$,f=t(),h(k.$$.fragment),R=t(),G=M("p"),G.innerHTML=z,S=t(),h(N.$$.fragment),_=t(),h(B.$$.fragment),g=t(),x=M("p"),x.textContent=L,A=t(),h(Q.$$.fragment),K=t(),h(X.$$.fragment),q=t(),E=M("p"),E.innerHTML=Y,F=t(),J=M("p"),J.textContent=V},l(T){r=c(T,"P",{"data-svelte-h":!0}),u(r)!=="svelte-14wouu"&&(r.textContent=j),o=n(T),y(I.$$.fragment,T),v=n(T),C=c(T,"P",{"data-svelte-h":!0}),u(C)!=="svelte-1gr4lzn"&&(C.textContent=$),f=n(T),y(k.$$.fragment,T),R=n(T),G=c(T,"P",{"data-svelte-h":!0}),u(G)!=="svelte-8gu1qg"&&(G.innerHTML=z),S=n(T),y(N.$$.fragment,T),_=n(T),y(B.$$.fragment,T),g=n(T),x=c(T,"P",{"data-svelte-h":!0}),u(x)!=="svelte-c5gs0a"&&(x.textContent=L),A=n(T),y(Q.$$.fragment,T),K=n(T),y(X.$$.fragment,T),q=n(T),E=c(T,"P",{"data-svelte-h":!0}),u(E)!=="svelte-euvgvz"&&(E.innerHTML=Y),F=n(T),J=c(T,"P",{"data-svelte-h":!0}),u(J)!=="svelte-thah2z"&&(J.textContent=V)},m(T,W){l(T,r,W),l(T,o,W),d(I,T,W),l(T,v,W),l(T,C,W),l(T,f,W),d(k,T,W),l(T,R,W),l(T,G,W),l(T,S,W),d(N,T,W),l(T,_,W),d(B,T,W),l(T,g,W),l(T,x,W),l(T,A,W),d(Q,T,W),l(T,K,W),d(X,T,W),l(T,q,W),l(T,E,W),l(T,F,W),l(T,J,W),Me=!0},i(T){Me||(p(I.$$.fragment,T),p(k.$$.fragment,T),p(N.$$.fragment,T),p(B.$$.fragment,T),p(Q.$$.fragment,T),p(X.$$.fragment,T),Me=!0)},o(T){m(I.$$.fragment,T),m(k.$$.fragment,T),m(N.$$.fragment,T),m(B.$$.fragment,T),m(Q.$$.fragment,T),m(X.$$.fragment,T),Me=!1},d(T){T&&(s(r),s(o),s(v),s(C),s(f),s(R),s(G),s(S),s(_),s(g),s(x),s(A),s(K),s(q),s(E),s(F),s(J)),w(I,T),w(k,T),w(N,T),w(B,T),w(Q,T),w(X,T)}}}function ni(Z){let r,j,o,I='Fine-tuning our model with ü§ó Accelerate is very similar to the text classification example we encountered in <a href="/course/chapter3">Chapter 3</a>. The main differences will be the need to explicitly generate our summaries during training and define how we compute the ROUGE scores (recall that the <code>Seq2SeqTrainer</code> took care of the generation for us). Let‚Äôs take a look how we can implement these two requirements within ü§ó Accelerate!',v,C,$,f,k="The first thing we need to do is create a <code>DataLoader</code> for each of our splits. Since the PyTorch dataloaders expect batches of tensors, we need to set the format to <code>&quot;torch&quot;</code> in our datasets:",R,G,z,S,N="Now that we‚Äôve got datasets consisting of just tensors, the next thing to do is instantiate the <code>DataCollatorForSeq2Seq</code> again. For this we need to provide a fresh version of the model, so let‚Äôs load it again from our cache:",_,B,g,x,L="We can then instantiate the data collator and use this to define our dataloaders:",A,Q,K,X,q="The next thing to do is define the optimizer we want to use. As in our other examples, we‚Äôll use <code>AdamW</code>, which works well for most problems:",E,Y,F,J,V="Finally, we feed our model, optimizer, and dataloaders to the <code>accelerator.prepare()</code> method:",Me,T,W,P,Ne,O,ea="Now that we‚Äôve prepared our objects, there are three remaining things to do:",Ae,ee,ns="<li>Define the learning rate schedule.</li> <li>Implement a function to post-process the summaries for evaluation.</li> <li>Create a repository on the Hub that we can push our model to.</li>",ce,he,Ie="For the learning rate schedule, we‚Äôll use the standard linear one from previous sections:",Xe,se,is,ye,We="For post-processing, we need a function that splits the generated summaries into sentences that are separated by newlines. This is the format the ROUGE metric expects, and we can achieve this with the following snippet of code:",ze,le,Qe,ae,sa="This should look familiar to you if you recall how we defined the <code>compute_metrics()</code> function of the <code>Seq2SeqTrainer</code>.",Ye,te,rs="Finally, we need to create a model repository on the Hugging Face Hub. For this, we can use the appropriately titled ü§ó Hub library. We just need to define a name for our repository, and the library has a utility function to combine the repository ID with the user profile:",de,$e,Se,ne,Ee,ie,la="Now we can use this repository name to clone a local version to our results directory that will store the training artifacts:",Fe,re,He,oe,os="This will allow us to push the artifacts back to the Hub by calling the <code>repo.push_to_hub()</code> method during training! Let‚Äôs now wrap up our analysis by writing out the training loop.",we,_e,De,pe,ps="The training loop for summarization is quite similar to the other ü§ó Accelerate examples that we‚Äôve encountered and is roughly split into four main steps:",ue,Je,Ve="<li>Train the model by iterating over all the examples in <code>train_dataloader</code> for each epoch.</li> <li>Generate model summaries at the end of each epoch, by first generating the tokens and then decoding them (and the reference summaries) into text.</li> <li>Compute the ROUGE scores using the same techniques we saw earlier.</li> <li>Save the checkpoints and push everything to the Hub. Here we rely on the nifty <code>blocking=False</code> argument of the <code>Repository</code> object so that we can push the checkpoints per epoch <em>asynchronously</em>. This allows us to continue training without having to wait for the somewhat slow upload associated with a GB-sized model!</li>",ms,Te,ge="These steps can be seen in the following block of code:",Ms,Ue,je,Re,Le,me,cs="And that‚Äôs it! Once you run this, you‚Äôll have a model and results that are pretty similar to the ones we obtained with the <code>Trainer</code>.",be;return r=new fe({props:{title:"Fine-tuning mT5 with ü§ó Accelerate",local:"fine-tuning-mt5-with-accelerate",headingTag:"h2"}}),C=new fe({props:{title:"Preparing everything for training",local:"preparing-everything-for-training",headingTag:"h3"}}),G=new U({props:{code:"dG9rZW5pemVkX2RhdGFzZXRzLnNldF9mb3JtYXQoJTIydG9yY2glMjIp",highlighted:'tokenized_datasets.set_format(<span class="hljs-string">&quot;torch&quot;</span>)',wrap:!1}}),B=new U({props:{code:"bW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2NoZWNrcG9pbnQp",highlighted:"model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)",wrap:!1}}),Q=new U({props:{code:"ZnJvbSUyMHRvcmNoLnV0aWxzLmRhdGElMjBpbXBvcnQlMjBEYXRhTG9hZGVyJTBBJTBBYmF0Y2hfc2l6ZSUyMCUzRCUyMDglMEF0cmFpbl9kYXRhbG9hZGVyJTIwJTNEJTIwRGF0YUxvYWRlciglMEElMjAlMjAlMjAlMjB0b2tlbml6ZWRfZGF0YXNldHMlNUIlMjJ0cmFpbiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHNodWZmbGUlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwY29sbGF0ZV9mbiUzRGRhdGFfY29sbGF0b3IlMkMlMEElMjAlMjAlMjAlMjBiYXRjaF9zaXplJTNEYmF0Y2hfc2l6ZSUyQyUwQSklMEFldmFsX2RhdGFsb2FkZXIlMjAlM0QlMjBEYXRhTG9hZGVyKCUwQSUyMCUyMCUyMCUyMHRva2VuaXplZF9kYXRhc2V0cyU1QiUyMnZhbGlkYXRpb24lMjIlNUQlMkMlMjBjb2xsYXRlX2ZuJTNEZGF0YV9jb2xsYXRvciUyQyUyMGJhdGNoX3NpemUlM0RiYXRjaF9zaXplJTBBKQ==",highlighted:`<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

batch_size = <span class="hljs-number">8</span>
train_dataloader = DataLoader(
    tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>], collate_fn=data_collator, batch_size=batch_size
)`,wrap:!1}}),Y=new U({props:{code:"ZnJvbSUyMHRvcmNoLm9wdGltJTIwaW1wb3J0JTIwQWRhbVclMEElMEFvcHRpbWl6ZXIlMjAlM0QlMjBBZGFtVyhtb2RlbC5wYXJhbWV0ZXJzKCklMkMlMjBsciUzRDJlLTUp",highlighted:`<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW

optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">2e-5</span>)`,wrap:!1}}),T=new U({props:{code:"ZnJvbSUyMGFjY2VsZXJhdGUlMjBpbXBvcnQlMjBBY2NlbGVyYXRvciUwQSUwQWFjY2VsZXJhdG9yJTIwJTNEJTIwQWNjZWxlcmF0b3IoKSUwQW1vZGVsJTJDJTIwb3B0aW1pemVyJTJDJTIwdHJhaW5fZGF0YWxvYWRlciUyQyUyMGV2YWxfZGF0YWxvYWRlciUyMCUzRCUyMGFjY2VsZXJhdG9yLnByZXBhcmUoJTBBJTIwJTIwJTIwJTIwbW9kZWwlMkMlMjBvcHRpbWl6ZXIlMkMlMjB0cmFpbl9kYXRhbG9hZGVyJTJDJTIwZXZhbF9kYXRhbG9hZGVyJTBBKQ==",highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`,wrap:!1}}),P=new as({props:{$$slots:{default:[xi]},$$scope:{ctx:Z}}}),se=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMGdldF9zY2hlZHVsZXIlMEElMEFudW1fdHJhaW5fZXBvY2hzJTIwJTNEJTIwMTAlMEFudW1fdXBkYXRlX3N0ZXBzX3Blcl9lcG9jaCUyMCUzRCUyMGxlbih0cmFpbl9kYXRhbG9hZGVyKSUwQW51bV90cmFpbmluZ19zdGVwcyUyMCUzRCUyMG51bV90cmFpbl9lcG9jaHMlMjAqJTIwbnVtX3VwZGF0ZV9zdGVwc19wZXJfZXBvY2glMEElMEFscl9zY2hlZHVsZXIlMjAlM0QlMjBnZXRfc2NoZWR1bGVyKCUwQSUyMCUyMCUyMCUyMCUyMmxpbmVhciUyMiUyQyUwQSUyMCUyMCUyMCUyMG9wdGltaXplciUzRG9wdGltaXplciUyQyUwQSUyMCUyMCUyMCUyMG51bV93YXJtdXBfc3RlcHMlM0QwJTJDJTBBJTIwJTIwJTIwJTIwbnVtX3RyYWluaW5nX3N0ZXBzJTNEbnVtX3RyYWluaW5nX3N0ZXBzJTJDJTBBKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler

num_train_epochs = <span class="hljs-number">10</span>
num_update_steps_per_epoch = <span class="hljs-built_in">len</span>(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    <span class="hljs-string">&quot;linear&quot;</span>,
    optimizer=optimizer,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_training_steps=num_training_steps,
)`,wrap:!1}}),le=new U({props:{code:"ZGVmJTIwcG9zdHByb2Nlc3NfdGV4dChwcmVkcyUyQyUyMGxhYmVscyklM0ElMEElMjAlMjAlMjAlMjBwcmVkcyUyMCUzRCUyMCU1QnByZWQuc3RyaXAoKSUyMGZvciUyMHByZWQlMjBpbiUyMHByZWRzJTVEJTBBJTIwJTIwJTIwJTIwbGFiZWxzJTIwJTNEJTIwJTVCbGFiZWwuc3RyaXAoKSUyMGZvciUyMGxhYmVsJTIwaW4lMjBsYWJlbHMlNUQlMEElMEElMjAlMjAlMjAlMjAlMjMlMjBST1VHRSUyMGV4cGVjdHMlMjBhJTIwbmV3bGluZSUyMGFmdGVyJTIwZWFjaCUyMHNlbnRlbmNlJTBBJTIwJTIwJTIwJTIwcHJlZHMlMjAlM0QlMjAlNUIlMjIlNUNuJTIyLmpvaW4obmx0ay5zZW50X3Rva2VuaXplKHByZWQpKSUyMGZvciUyMHByZWQlMjBpbiUyMHByZWRzJTVEJTBBJTIwJTIwJTIwJTIwbGFiZWxzJTIwJTNEJTIwJTVCJTIyJTVDbiUyMi5qb2luKG5sdGsuc2VudF90b2tlbml6ZShsYWJlbCkpJTIwZm9yJTIwbGFiZWwlMjBpbiUyMGxhYmVscyU1RCUwQSUwQSUyMCUyMCUyMCUyMHJldHVybiUyMHByZWRzJTJDJTIwbGFiZWxz",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">postprocess_text</span>(<span class="hljs-params">preds, labels</span>):
    preds = [pred.strip() <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds]
    labels = [label.strip() <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels]

    <span class="hljs-comment"># ROUGE expects a newline after each sentence</span>
    preds = [<span class="hljs-string">&quot;\\n&quot;</span>.join(nltk.sent_tokenize(pred)) <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds]
    labels = [<span class="hljs-string">&quot;\\n&quot;</span>.join(nltk.sent_tokenize(label)) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels]

    <span class="hljs-keyword">return</span> preds, labels`,wrap:!1}}),$e=new U({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMGdldF9mdWxsX3JlcG9fbmFtZSUwQSUwQW1vZGVsX25hbWUlMjAlM0QlMjAlMjJ0ZXN0LWJlcnQtZmluZXR1bmVkLXNxdWFkLWFjY2VsZXJhdGUlMjIlMEFyZXBvX25hbWUlMjAlM0QlMjBnZXRfZnVsbF9yZXBvX25hbWUobW9kZWxfbmFtZSklMEFyZXBvX25hbWU=",highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> get_full_repo_name

model_name = <span class="hljs-string">&quot;test-bert-finetuned-squad-accelerate&quot;</span>
repo_name = get_full_repo_name(model_name)
repo_name`,wrap:!1}}),ne=new U({props:{code:"J2xld3R1biUyRm10NS1maW5ldHVuZWQtYW1hem9uLWVuLWVzLWFjY2VsZXJhdGUn",highlighted:'<span class="hljs-string">&#x27;lewtun/mt5-finetuned-amazon-en-es-accelerate&#x27;</span>',wrap:!1}}),re=new U({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMFJlcG9zaXRvcnklMEElMEFvdXRwdXRfZGlyJTIwJTNEJTIwJTIycmVzdWx0cy1tdDUtZmluZXR1bmVkLXNxdWFkLWFjY2VsZXJhdGUlMjIlMEFyZXBvJTIwJTNEJTIwUmVwb3NpdG9yeShvdXRwdXRfZGlyJTJDJTIwY2xvbmVfZnJvbSUzRHJlcG9fbmFtZSk=",highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> Repository

output_dir = <span class="hljs-string">&quot;results-mt5-finetuned-squad-accelerate&quot;</span>
repo = Repository(output_dir, clone_from=repo_name)`,wrap:!1}}),_e=new fe({props:{title:"Training loop",local:"training-loop",headingTag:"h3"}}),Ue=new U({props:{code:"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",highlighted:`<span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))

<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train_epochs):
    <span class="hljs-comment"># Training</span>
    model.train()
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)

    <span class="hljs-comment"># Evaluation</span>
    model.<span class="hljs-built_in">eval</span>()
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(eval_dataloader):
        <span class="hljs-keyword">with</span> torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch[<span class="hljs-string">&quot;input_ids&quot;</span>],
                attention_mask=batch[<span class="hljs-string">&quot;attention_mask&quot;</span>],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=<span class="hljs-number">1</span>, pad_index=tokenizer.pad_token_id
            )
            labels = batch[<span class="hljs-string">&quot;labels&quot;</span>]

            <span class="hljs-comment"># If we did not pad to max length, we need to pad the labels too</span>
            labels = accelerator.pad_across_processes(
                batch[<span class="hljs-string">&quot;labels&quot;</span>], dim=<span class="hljs-number">1</span>, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            <span class="hljs-comment"># Replace -100 in the labels as we can&#x27;t decode them</span>
            labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(generated_tokens, <span class="hljs-built_in">tuple</span>):
                generated_tokens = generated_tokens[<span class="hljs-number">0</span>]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=<span class="hljs-literal">True</span>
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    <span class="hljs-comment"># Compute metrics</span>
    result = rouge_score.compute()
    <span class="hljs-comment"># Extract the median ROUGE scores</span>
    result = {key: value.mid.fmeasure * <span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> result.items()}
    result = {k: <span class="hljs-built_in">round</span>(v, <span class="hljs-number">4</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> result.items()}
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">{epoch}</span>:&quot;</span>, result)

    <span class="hljs-comment"># Save and upload</span>
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    <span class="hljs-keyword">if</span> accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=<span class="hljs-string">f&quot;Training in progress epoch <span class="hljs-subst">{epoch}</span>&quot;</span>, blocking=<span class="hljs-literal">False</span>
        )`,wrap:!1}}),Re=new U({props:{code:"RXBvY2glMjAwJTNBJTIwJTdCJ3JvdWdlMSclM0ElMjA1LjYzNTElMkMlMjAncm91Z2UyJyUzQSUyMDEuMTYyNSUyQyUyMCdyb3VnZUwnJTNBJTIwNS40ODY2JTJDJTIwJ3JvdWdlTHN1bSclM0ElMjA1LjUwMDUlN0QlMEFFcG9jaCUyMDElM0ElMjAlN0Incm91Z2UxJyUzQSUyMDkuODY0NiUyQyUyMCdyb3VnZTInJTNBJTIwMy40MTA2JTJDJTIwJ3JvdWdlTCclM0ElMjA5Ljk0MzklMkMlMjAncm91Z2VMc3VtJyUzQSUyMDkuOTMwNiU3RCUwQUVwb2NoJTIwMiUzQSUyMCU3Qidyb3VnZTEnJTNBJTIwMTEuMDg3MiUyQyUyMCdyb3VnZTInJTNBJTIwMy4zMjczJTJDJTIwJ3JvdWdlTCclM0ElMjAxMS4wNTA4JTJDJTIwJ3JvdWdlTHN1bSclM0ElMjAxMC45NDY4JTdEJTBBRXBvY2glMjAzJTNBJTIwJTdCJ3JvdWdlMSclM0ElMjAxMS44NTg3JTJDJTIwJ3JvdWdlMiclM0ElMjA0LjgxNjclMkMlMjAncm91Z2VMJyUzQSUyMDExLjc5ODYlMkMlMjAncm91Z2VMc3VtJyUzQSUyMDExLjc1MTglN0QlMEFFcG9jaCUyMDQlM0ElMjAlN0Incm91Z2UxJyUzQSUyMDEyLjk4NDIlMkMlMjAncm91Z2UyJyUzQSUyMDUuNTg4NyUyQyUyMCdyb3VnZUwnJTNBJTIwMTIuNzU0NiUyQyUyMCdyb3VnZUxzdW0nJTNBJTIwMTIuNzAyOSU3RCUwQUVwb2NoJTIwNSUzQSUyMCU3Qidyb3VnZTEnJTNBJTIwMTMuNDYyOCUyQyUyMCdyb3VnZTInJTNBJTIwNi40NTk4JTJDJTIwJ3JvdWdlTCclM0ElMjAxMy4zMTIlMkMlMjAncm91Z2VMc3VtJyUzQSUyMDEzLjI5MTMlN0QlMEFFcG9jaCUyMDYlM0ElMjAlN0Incm91Z2UxJyUzQSUyMDEyLjkxMzElMkMlMjAncm91Z2UyJyUzQSUyMDUuODkxNCUyQyUyMCdyb3VnZUwnJTNBJTIwMTIuNjg5NiUyQyUyMCdyb3VnZUxzdW0nJTNBJTIwMTIuNTcwMSU3RCUwQUVwb2NoJTIwNyUzQSUyMCU3Qidyb3VnZTEnJTNBJTIwMTMuMzA3OSUyQyUyMCdyb3VnZTInJTNBJTIwNi4yOTk0JTJDJTIwJ3JvdWdlTCclM0ElMjAxMy4xNTM2JTJDJTIwJ3JvdWdlTHN1bSclM0ElMjAxMy4xMTk0JTdEJTBBRXBvY2glMjA4JTNBJTIwJTdCJ3JvdWdlMSclM0ElMjAxMy45NiUyQyUyMCdyb3VnZTInJTNBJTIwNi41OTk4JTJDJTIwJ3JvdWdlTCclM0ElMjAxMy45MTIzJTJDJTIwJ3JvdWdlTHN1bSclM0ElMjAxMy43NzQ0JTdEJTBBRXBvY2glMjA5JTNBJTIwJTdCJ3JvdWdlMSclM0ElMjAxNC4xMTkyJTJDJTIwJ3JvdWdlMiclM0ElMjA3LjAwNTklMkMlMjAncm91Z2VMJyUzQSUyMDE0LjExNzIlMkMlMjAncm91Z2VMc3VtJyUzQSUyMDEzLjk1MDklN0Q=",highlighted:`Epoch <span class="hljs-number">0</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">5.6351</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">1.1625</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">5.4866</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">5.5005</span>}
Epoch <span class="hljs-number">1</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">9.8646</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">3.4106</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">9.9439</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">9.9306</span>}
Epoch <span class="hljs-number">2</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">11.0872</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">3.3273</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">11.0508</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">10.9468</span>}
Epoch <span class="hljs-number">3</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">11.8587</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">4.8167</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">11.7986</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">11.7518</span>}
Epoch <span class="hljs-number">4</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">12.9842</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">5.5887</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">12.7546</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">12.7029</span>}
Epoch <span class="hljs-number">5</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">13.4628</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">6.4598</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">13.312</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.2913</span>}
Epoch <span class="hljs-number">6</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">12.9131</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">5.8914</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">12.6896</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">12.5701</span>}
Epoch <span class="hljs-number">7</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">13.3079</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">6.2994</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">13.1536</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.1194</span>}
Epoch <span class="hljs-number">8</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">13.96</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">6.5998</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">13.9123</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.7744</span>}
Epoch <span class="hljs-number">9</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">14.1192</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">7.0059</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">14.1172</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.9509</span>}`,wrap:!1}}),{c(){h(r.$$.fragment),j=t(),o=M("p"),o.innerHTML=I,v=t(),h(C.$$.fragment),$=t(),f=M("p"),f.innerHTML=k,R=t(),h(G.$$.fragment),z=t(),S=M("p"),S.innerHTML=N,_=t(),h(B.$$.fragment),g=t(),x=M("p"),x.textContent=L,A=t(),h(Q.$$.fragment),K=t(),X=M("p"),X.innerHTML=q,E=t(),h(Y.$$.fragment),F=t(),J=M("p"),J.innerHTML=V,Me=t(),h(T.$$.fragment),W=t(),h(P.$$.fragment),Ne=t(),O=M("p"),O.textContent=ea,Ae=t(),ee=M("ul"),ee.innerHTML=ns,ce=t(),he=M("p"),he.textContent=Ie,Xe=t(),h(se.$$.fragment),is=t(),ye=M("p"),ye.textContent=We,ze=t(),h(le.$$.fragment),Qe=t(),ae=M("p"),ae.innerHTML=sa,Ye=t(),te=M("p"),te.textContent=rs,de=t(),h($e.$$.fragment),Se=t(),h(ne.$$.fragment),Ee=t(),ie=M("p"),ie.textContent=la,Fe=t(),h(re.$$.fragment),He=t(),oe=M("p"),oe.innerHTML=os,we=t(),h(_e.$$.fragment),De=t(),pe=M("p"),pe.textContent=ps,ue=t(),Je=M("ol"),Je.innerHTML=Ve,ms=t(),Te=M("p"),Te.textContent=ge,Ms=t(),h(Ue.$$.fragment),je=t(),h(Re.$$.fragment),Le=t(),me=M("p"),me.innerHTML=cs},l(i){y(r.$$.fragment,i),j=n(i),o=c(i,"P",{"data-svelte-h":!0}),u(o)!=="svelte-uwq75m"&&(o.innerHTML=I),v=n(i),y(C.$$.fragment,i),$=n(i),f=c(i,"P",{"data-svelte-h":!0}),u(f)!=="svelte-1raz9tt"&&(f.innerHTML=k),R=n(i),y(G.$$.fragment,i),z=n(i),S=c(i,"P",{"data-svelte-h":!0}),u(S)!=="svelte-ymev3i"&&(S.innerHTML=N),_=n(i),y(B.$$.fragment,i),g=n(i),x=c(i,"P",{"data-svelte-h":!0}),u(x)!=="svelte-vgcsz6"&&(x.textContent=L),A=n(i),y(Q.$$.fragment,i),K=n(i),X=c(i,"P",{"data-svelte-h":!0}),u(X)!=="svelte-41m32h"&&(X.innerHTML=q),E=n(i),y(Y.$$.fragment,i),F=n(i),J=c(i,"P",{"data-svelte-h":!0}),u(J)!=="svelte-1fp4o9v"&&(J.innerHTML=V),Me=n(i),y(T.$$.fragment,i),W=n(i),y(P.$$.fragment,i),Ne=n(i),O=c(i,"P",{"data-svelte-h":!0}),u(O)!=="svelte-h8aj3b"&&(O.textContent=ea),Ae=n(i),ee=c(i,"UL",{"data-svelte-h":!0}),u(ee)!=="svelte-1odn6yw"&&(ee.innerHTML=ns),ce=n(i),he=c(i,"P",{"data-svelte-h":!0}),u(he)!=="svelte-12m7glt"&&(he.textContent=Ie),Xe=n(i),y(se.$$.fragment,i),is=n(i),ye=c(i,"P",{"data-svelte-h":!0}),u(ye)!=="svelte-1l4rcdk"&&(ye.textContent=We),ze=n(i),y(le.$$.fragment,i),Qe=n(i),ae=c(i,"P",{"data-svelte-h":!0}),u(ae)!=="svelte-17mpp9"&&(ae.innerHTML=sa),Ye=n(i),te=c(i,"P",{"data-svelte-h":!0}),u(te)!=="svelte-1wkjrqf"&&(te.textContent=rs),de=n(i),y($e.$$.fragment,i),Se=n(i),y(ne.$$.fragment,i),Ee=n(i),ie=c(i,"P",{"data-svelte-h":!0}),u(ie)!=="svelte-1yghzy"&&(ie.textContent=la),Fe=n(i),y(re.$$.fragment,i),He=n(i),oe=c(i,"P",{"data-svelte-h":!0}),u(oe)!=="svelte-1vbolln"&&(oe.innerHTML=os),we=n(i),y(_e.$$.fragment,i),De=n(i),pe=c(i,"P",{"data-svelte-h":!0}),u(pe)!=="svelte-1cqv7vl"&&(pe.textContent=ps),ue=n(i),Je=c(i,"OL",{"data-svelte-h":!0}),u(Je)!=="svelte-yr5mj5"&&(Je.innerHTML=Ve),ms=n(i),Te=c(i,"P",{"data-svelte-h":!0}),u(Te)!=="svelte-14dai24"&&(Te.textContent=ge),Ms=n(i),y(Ue.$$.fragment,i),je=n(i),y(Re.$$.fragment,i),Le=n(i),me=c(i,"P",{"data-svelte-h":!0}),u(me)!=="svelte-172kpng"&&(me.innerHTML=cs)},m(i,b){d(r,i,b),l(i,j,b),l(i,o,b),l(i,v,b),d(C,i,b),l(i,$,b),l(i,f,b),l(i,R,b),d(G,i,b),l(i,z,b),l(i,S,b),l(i,_,b),d(B,i,b),l(i,g,b),l(i,x,b),l(i,A,b),d(Q,i,b),l(i,K,b),l(i,X,b),l(i,E,b),d(Y,i,b),l(i,F,b),l(i,J,b),l(i,Me,b),d(T,i,b),l(i,W,b),d(P,i,b),l(i,Ne,b),l(i,O,b),l(i,Ae,b),l(i,ee,b),l(i,ce,b),l(i,he,b),l(i,Xe,b),d(se,i,b),l(i,is,b),l(i,ye,b),l(i,ze,b),d(le,i,b),l(i,Qe,b),l(i,ae,b),l(i,Ye,b),l(i,te,b),l(i,de,b),d($e,i,b),l(i,Se,b),d(ne,i,b),l(i,Ee,b),l(i,ie,b),l(i,Fe,b),d(re,i,b),l(i,He,b),l(i,oe,b),l(i,we,b),d(_e,i,b),l(i,De,b),l(i,pe,b),l(i,ue,b),l(i,Je,b),l(i,ms,b),l(i,Te,b),l(i,Ms,b),d(Ue,i,b),l(i,je,b),d(Re,i,b),l(i,Le,b),l(i,me,b),be=!0},i(i){be||(p(r.$$.fragment,i),p(C.$$.fragment,i),p(G.$$.fragment,i),p(B.$$.fragment,i),p(Q.$$.fragment,i),p(Y.$$.fragment,i),p(T.$$.fragment,i),p(P.$$.fragment,i),p(se.$$.fragment,i),p(le.$$.fragment,i),p($e.$$.fragment,i),p(ne.$$.fragment,i),p(re.$$.fragment,i),p(_e.$$.fragment,i),p(Ue.$$.fragment,i),p(Re.$$.fragment,i),be=!0)},o(i){m(r.$$.fragment,i),m(C.$$.fragment,i),m(G.$$.fragment,i),m(B.$$.fragment,i),m(Q.$$.fragment,i),m(Y.$$.fragment,i),m(T.$$.fragment,i),m(P.$$.fragment,i),m(se.$$.fragment,i),m(le.$$.fragment,i),m($e.$$.fragment,i),m(ne.$$.fragment,i),m(re.$$.fragment,i),m(_e.$$.fragment,i),m(Ue.$$.fragment,i),m(Re.$$.fragment,i),be=!1},d(i){i&&(s(j),s(o),s(v),s($),s(f),s(R),s(z),s(S),s(_),s(g),s(x),s(A),s(K),s(X),s(E),s(F),s(J),s(Me),s(W),s(Ne),s(O),s(Ae),s(ee),s(ce),s(he),s(Xe),s(is),s(ye),s(ze),s(Qe),s(ae),s(Ye),s(te),s(de),s(Se),s(Ee),s(ie),s(Fe),s(He),s(oe),s(we),s(De),s(pe),s(ue),s(Je),s(ms),s(Te),s(Ms),s(je),s(Le),s(me)),w(r,i),w(C,i),w(G,i),w(B,i),w(Q,i),w(Y,i),w(T,i),w(P,i),w(se,i),w(le,i),w($e,i),w(ne,i),w(re,i),w(_e,i),w(Ue,i),w(Re,i)}}}function xi(Z){let r,j='üö® If you‚Äôre training on a TPU, you‚Äôll need to move all the code above into a dedicated training function. See <a href="/course/chapter3">Chapter 3</a> for more details.';return{c(){r=M("p"),r.innerHTML=j},l(o){r=c(o,"P",{"data-svelte-h":!0}),u(r)!=="svelte-1h0m1lz"&&(r.innerHTML=j)},m(o,I){l(o,r,I)},p:ts,d(o){o&&s(r)}}}function Bi(Z){let r,j,o,I,v,C,$,f,k,R,G,z,S="In this section we‚Äôll take a look at how Transformer models can be used to condense long documents into summaries, a task known as <em>text summarization</em>. This is one of the most challenging NLP tasks as it requires a range of abilities, such as understanding long passages and generating coherent text that captures the main topics in a document. However, when done well, text summarization is a powerful tool that can speed up various business processes by relieving the burden of domain experts to read long documents in detail.",N,_,B,g,x='Although there already exist various fine-tuned models for summarization on the <a href="https://huggingface.co/models?pipeline_tag=summarization&amp;sort=downloads" rel="nofollow">Hugging Face Hub</a>, almost all of these are only suitable for English documents. So, to add a twist in this section, we‚Äôll train a bilingual model for English and Spanish. By the end of this section, you‚Äôll have a <a href="https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es" rel="nofollow">model</a> that can summarize customer reviews like the one shown here:',L,A,Q,K,X,q="As we‚Äôll see, these summaries are concise because they‚Äôre learned from the titles that customers provide in their product reviews. Let‚Äôs start by putting together a suitable bilingual corpus for this task.",E,Y,F,J,V='We‚Äôll use the <a href="https://huggingface.co/datasets/amazon_reviews_multi" rel="nofollow">Multilingual Amazon Reviews Corpus</a> to create our bilingual summarizer. This corpus consists of Amazon product reviews in six languages and is typically used to benchmark multilingual classifiers. However, since each review is accompanied by a short title, we can use the titles as the target summaries for our model to learn from! To get started, let‚Äôs download the English and Spanish subsets from the Hugging Face Hub:',Me,T,W,P,Ne,O,ea='As you can see, for each language there are 200,000 reviews for the <code>train</code> split, and 5,000 reviews for each of the <code>validation</code> and <code>test</code> splits. The review information we are interested in is contained in the <code>review_body</code> and <code>review_title</code> columns. Let‚Äôs take a look at a few examples by creating a simple function that takes a random sample from the training set with the techniques we learned in <a href="/course/chapter5">Chapter 5</a>:',Ae,ee,ns,ce,he,Ie,Xe,se,is="This sample shows the diversity of reviews one typically finds online, ranging from positive to negative (and everything in between!). Although the example with the ‚Äúmeh‚Äù title is not very informative, the other titles look like decent summaries of the reviews themselves. Training a summarization model on all 400,000 reviews would take far too long on a single GPU, so instead we‚Äôll focus on generating summaries for a single domain of products. To get a feel for what domains we can choose from, let‚Äôs convert <code>english_dataset</code> to a <code>pandas.DataFrame</code> and compute the number of reviews per product category:",ye,We,ze,le,Qe,ae,sa='The most popular products in the English dataset are about household items, clothing, and wireless electronics. To stick with the Amazon theme, though, let‚Äôs focus on summarizing book reviews ‚Äî after all, this is what the company was founded on! We can see two product categories that fit the bill (<code>book</code> and <code>digital_ebook_purchase</code>), so let‚Äôs filter the datasets in both languages for just these products. As we saw in <a href="/course/chapter5">Chapter 5</a>, the <code>Dataset.filter()</code> function allows us to slice a dataset very efficiently, so we can define a simple function to do this:',Ye,te,rs,de,$e="Now when we apply this function to <code>english_dataset</code> and <code>spanish_dataset</code>, the result will contain just those rows involving the book categories. Before applying the filter, let‚Äôs switch the format of <code>english_dataset</code> from <code>&quot;pandas&quot;</code> back to <code>&quot;arrow&quot;</code>:",Se,ne,Ee,ie,la="We can then apply the filter function, and as a sanity check let‚Äôs inspect a sample of reviews to see if they are indeed about books:",Fe,re,He,oe,os,we,_e="Okay, we can see that the reviews are not strictly about books and might refer to things like calendars and electronic applications such as OneNote. Nevertheless, the domain seems about right to train a summarization model on. Before we look at various models that are suitable for this task, we have one last bit of data preparation to do: combining the English and Spanish reviews as a single <code>DatasetDict</code> object. ü§ó Datasets provides a handy <code>concatenate_datasets()</code> function that (as the name suggests) will stack two <code>Dataset</code> objects on top of each other. So, to create our bilingual dataset, we‚Äôll loop over each split, concatenate the datasets for that split, and shuffle the result to ensure our model doesn‚Äôt overfit to a single language:",De,pe,ps,ue,Je,Ve,ms="This certainly looks like a mix of English and Spanish reviews! Now that we have a training corpus, one final thing to check is the distribution of words in the reviews and their titles. This is especially important for summarization tasks, where short reference summaries in the data can bias the model to only output one or two words in the generated summaries. The plots below show the word distributions, and we can see that the titles are heavily skewed toward just 1-2 words:",Te,ge,Ms='<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg" alt="Word count distributions for the review titles and texts."/> <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg" alt="Word count distributions for the review titles and texts."/>',Ue,je,Re="To deal with this, we‚Äôll filter out the examples with very short titles so that our model can produce more interesting summaries. Since we‚Äôre dealing with English and Spanish texts, we can use a rough heuristic to split the titles on whitespace and then use our trusty <code>Dataset.filter()</code> method as follows:",Le,me,cs,be,i="Now that we‚Äôve prepared our corpus, let‚Äôs take a look at a few possible Transformer models that one might fine-tune on it!",b,hs,na,ys,qt='If you think about it, text summarization is a similar sort of task to machine translation: we have a body of text like a review that we‚Äôd like to ‚Äútranslate‚Äù into a shorter version that captures the salient features of the input. Accordingly, most Transformer models for summarization adopt the encoder-decoder architecture that we first encountered in <a href="/course/chapter1">Chapter 1</a>, although there are some exceptions like the GPT family of models which can also be used for summarization in few-shot settings. The following table lists some popular pretrained models that can be fine-tuned for summarization.',ia,ds,Kt='<thead><tr><th align="center">Transformer model</th> <th>Description</th> <th align="center">Multilingual?</th></tr></thead> <tbody><tr><td align="center"><a href="https://huggingface.co/gpt2-xl" rel="nofollow">GPT-2</a></td> <td>Although trained as an auto-regressive language model, you can make GPT-2 generate summaries by appending ‚ÄúTL;DR‚Äù at the end of the input text.</td> <td align="center">‚ùå</td></tr> <tr><td align="center"><a href="https://huggingface.co/google/pegasus-large" rel="nofollow">PEGASUS</a></td> <td>Uses a pretraining objective to predict masked sentences in multi-sentence texts. This pretraining objective is closer to summarization than vanilla language modeling and scores highly on popular benchmarks.</td> <td align="center">‚ùå</td></tr> <tr><td align="center"><a href="https://huggingface.co/t5-base" rel="nofollow">T5</a></td> <td>A universal Transformer architecture that formulates all tasks in a text-to-text framework; e.g., the input format for the model to summarize a document is <code>summarize: ARTICLE</code>.</td> <td align="center">‚ùå</td></tr> <tr><td align="center"><a href="https://huggingface.co/google/mt5-base" rel="nofollow">mT5</a></td> <td>A multilingual version of T5, pretrained on the multilingual Common Crawl corpus (mC4), covering 101 languages.</td> <td align="center">‚úÖ</td></tr> <tr><td align="center"><a href="https://huggingface.co/facebook/bart-base" rel="nofollow">BART</a></td> <td>A novel Transformer architecture with both an encoder and a decoder stack trained to reconstruct corrupted input that combines the pretraining schemes of BERT and GPT-2.</td> <td align="center">‚ùå</td></tr> <tr><td align="center"><a href="https://huggingface.co/facebook/mbart-large-50" rel="nofollow">mBART-50</a></td> <td>A multilingual version of BART, pretrained on 50 languages.</td> <td align="center">‚úÖ</td></tr></tbody>',ra,ws,Pt="As you can see from this table, the majority of Transformer models for summarization (and indeed most NLP tasks) are monolingual. This is great if your task is in a ‚Äúhigh-resource‚Äù language like English or German, but less so for the thousands of other languages in use across the world. Fortunately, there is a class of multilingual Transformer models, like mT5 and mBART, that come to the rescue. These models are pretrained using language modeling, but with a twist: instead of training on a corpus of one language, they are trained jointly on texts in over 50 languages at once!",oa,us,Ot="We‚Äôll focus on mT5, an interesting architecture based on T5 that was pretrained in a text-to-text framework. In T5, every NLP task is formulated in terms of a prompt prefix like <code>summarize:</code> which conditions the model to adapt the generated text to the prompt. As shown in the figure below, this makes T5 extremely versatile, as you can solve many tasks with a single model!",pa,qe,en='<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg" alt="Different tasks performed by the T5 architecture."/> <img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg" alt="Different tasks performed by the T5 architecture."/>',ma,Js,sn="mT5 doesn‚Äôt use prefixes, but shares much of the versatility of T5 and has the advantage of being multilingual. Now that we‚Äôve picked a model, let‚Äôs take a look at preparing our data for training.",Ma,Ke,ca,Ts,ha,Us,ya,js,ln="Our next task is to tokenize and encode our reviews and their titles. As usual, we begin by loading the tokenizer associated with the pretrained model checkpoint. We‚Äôll use <code>mt5-small</code> as our checkpoint so we can fine-tune the model in a reasonable amount of time:",da,bs,wa,Pe,ua,fs,an="Let‚Äôs test out the mT5 tokenizer on a small example:",Ja,Is,Ta,gs,Ua,Cs,tn='Here we can see the familiar <code>input_ids</code> and <code>attention_mask</code> that we encountered in our first fine-tuning experiments back in <a href="/course/chapter3">Chapter 3</a>. Let‚Äôs decode these input IDs with the tokenizer‚Äôs <code>convert_ids_to_tokens()</code> function to see what kind of tokenizer we‚Äôre dealing with:',ja,ks,ba,Zs,fa,vs,nn='The special Unicode character <code>‚ñÅ</code> and end-of-sequence token <code>&lt;/s&gt;</code> indicate that we‚Äôre dealing with the SentencePiece tokenizer, which is based on the Unigram segmentation algorithm discussed in <a href="/course/chapter6">Chapter 6</a>. Unigram is especially useful for multilingual corpora since it allows SentencePiece to be agnostic about accents, punctuation, and the fact that many languages, like Japanese, do not have whitespace characters.',Ia,Gs,rn="To tokenize our corpus, we have to deal with a subtlety associated with summarization: because our labels are also text, it is possible that they exceed the model‚Äôs maximum context size. This means we need to apply truncation to both the reviews and their titles to ensure we don‚Äôt pass excessively long inputs to our model. The tokenizers in ü§ó Transformers provide a nifty <code>text_target</code> argument that allows you to tokenize the labels in parallel to the inputs. Here is an example of how the inputs and targets are processed for mT5:",ga,xs,Ca,Bs,on="Let‚Äôs walk through this code to understand what‚Äôs happening. The first thing we‚Äôve done is define values for <code>max_input_length</code> and <code>max_target_length</code>, which set the upper limits for how long our reviews and titles can be. Since the review body is typically much larger than the title, we‚Äôve scaled these values accordingly.",ka,Ws,pn="With <code>preprocess_function()</code>, it is then a simple matter to tokenize the whole corpus using the handy <code>Dataset.map()</code> function we‚Äôve used extensively throughout this course:",Za,$s,va,_s,mn="Now that the corpus has been preprocessed, let‚Äôs take a look at some metrics that are commonly used for summarization. As we‚Äôll see, there is no silver bullet when it comes to measuring the quality of machine-generated text.",Ga,Oe,xa,Vs,Ba,Rs,Wa,Ns,Mn="In comparison to most of the other tasks we‚Äôve covered in this course, measuring the performance of text generation tasks like summarization or translation is not as straightforward. For example, given a review like ‚ÄúI loved reading the Hunger Games‚Äù, there are multiple valid summaries, like ‚ÄúI loved the Hunger Games‚Äù or ‚ÄúHunger Games is a great read‚Äù. Clearly, applying some sort of exact match between the generated summary and the label is not a good solution ‚Äî even humans would fare poorly under such a metric, because we all have our own writing style.",$a,As,cn='For summarization, one of the most commonly used metrics is the <a href="https://en.wikipedia.org/wiki/ROUGE_(metric)" rel="nofollow">ROUGE score</a> (short for Recall-Oriented Understudy for Gisting Evaluation). The basic idea behind this metric is to compare a generated summary against a set of reference summaries that are typically created by humans. To make this more precise, suppose we want to compare the following two summaries:',_a,Xs,Va,zs,hn="One way to compare them could be to count the number of overlapping words, which in this case would be 6. However, this is a bit crude, so instead ROUGE is based on computing the <em>precision</em> and <em>recall</em> scores for the overlap.",Ra,es,Na,Qs,Yt,Aa,ri='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">R</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">l</mi></mrow><mo>=</mo><mfrac><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">v</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">g</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi></mrow><mrow><mi mathvariant="normal">T</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">n</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">e</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">y</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex"> \\mathrm{Recall} = \\frac{\\mathrm{Number\\,of\\,overlapping\\, words}}{\\mathrm{Total\\, number\\, of\\, words\\, in\\, reference\\, summary}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord"><span class="mord mathrm">Recall</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2519em;vertical-align:-0.8804em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Total</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">number</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.07778em;">of</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">words</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">in</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">reference</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.01389em;">summary</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Number</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.07778em;">of</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.01389em;">overlapping</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">words</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>',Xa,Ys,St,za,oi='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">P</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mrow><mo>=</mo><mfrac><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">v</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">g</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi></mrow><mrow><mi mathvariant="normal">T</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">n</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">g</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">d</mi><mtext>‚Äâ</mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">y</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex"> \\mathrm{Precision} = \\frac{\\mathrm{Number\\,of\\,overlapping\\, words}}{\\mathrm{Total\\, number\\, of\\, words\\, in\\, generated\\, summary}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord mathrm">Precision</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2519em;vertical-align:-0.8804em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Total</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">number</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.07778em;">of</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">words</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">in</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">generated</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.01389em;">summary</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Number</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.07778em;">of</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.01389em;">overlapping</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">words</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>',Qa,Ss,yn="Applying this to our verbose summary gives a precision of 6/10  = 0.6, which is considerably worse than the precision of 6/7 = 0.86 obtained by our shorter one. In practice, both precision and recall are usually computed, and then the F1-score (the harmonic mean of precision and recall) is reported. We can do this easily in ü§ó Datasets by first installing the <code>rouge_score</code> package:",Ya,Es,Sa,Fs,dn="and then loading the ROUGE metric as follows:",Ea,Hs,Fa,Ds,wn="Then we can use the <code>rouge_score.compute()</code> function to calculate all the metrics at once:",Ha,Ls,Da,qs,La,Ks,un="Whoa, there‚Äôs a lot of information in that output ‚Äî what does it all mean? First, ü§ó Datasets actually computes confidence intervals for precision, recall, and F1-score; these are the <code>low</code>, <code>mid</code>, and <code>high</code> attributes you can see here. Moreover, ü§ó Datasets computes a variety of ROUGE scores which are based on different types of text granularity when comparing the generated and reference summaries. The <code>rouge1</code> variant is the overlap of unigrams ‚Äî this is just a fancy way of saying the overlap of words and is exactly the metric we‚Äôve discussed above. To verify this, let‚Äôs pull out the <code>mid</code> value of our scores:",qa,Ps,Ka,Os,Pa,el,Jn="Great, the precision and recall numbers match up! Now what about those other ROUGE scores? <code>rouge2</code> measures the overlap between bigrams (think the overlap of pairs of words), while <code>rougeL</code> and <code>rougeLsum</code> measure the longest matching sequences of words by looking for the longest common substrings in the generated and reference summaries. The ‚Äúsum‚Äù in <code>rougeLsum</code> refers to the fact that this metric is computed over a whole summary, while <code>rougeL</code> is computed as the average over individual sentences.",Oa,ss,et,sl,Tn="We‚Äôll use these ROUGE scores to track the performance of our model, but before doing that let‚Äôs do something every good NLP practitioner should do: create a strong, yet simple baseline!",st,ll,lt,al,Un="A common baseline for text summarization is to simply take the first three sentences of an article, often called the <em>lead-3</em> baseline. We could use full stops to track the sentence boundaries, but this will fail on acronyms like ‚ÄúU.S.‚Äù or ‚ÄúU.N.‚Äù ‚Äî so instead we‚Äôll use the <code>nltk</code> library, which includes a better algorithm to handle these cases. You can install the package using <code>pip</code> as follows:",at,tl,tt,nl,jn="and then download the punctuation rules:",nt,il,it,rl,bn="Next, we import the sentence tokenizer from <code>nltk</code> and create a simple function to extract the first three sentences in a review. The convention in text summarization is to separate each summary with a newline, so let‚Äôs also include this and test it on a training example:",rt,ol,ot,pl,pt,ml,fn="This seems to work, so let‚Äôs now implement a function that extracts these ‚Äúsummaries‚Äù from a dataset and computes the ROUGE scores for the baseline:",mt,Ml,Mt,cl,In="We can then use this function to compute the ROUGE scores over the validation set and prettify them a bit using Pandas:",ct,hl,ht,yl,yt,dl,gn="We can see that the <code>rouge2</code> score is significantly lower than the rest; this likely reflects the fact that review titles are typically concise and so the lead-3 baseline is too verbose. Now that we have a good baseline to work from, let‚Äôs turn our attention toward fine-tuning mT5!",dt,Ce,ke,Fl,ls,wt,wl,Cn="The next thing we need to do is log in to the Hugging Face Hub. If you‚Äôre running this code in a notebook, you can do so with the following utility function:",ut,ul,Jt,Jl,kn="which will display a widget where you can enter your credentials. Alternatively, you can run this command in your terminal and log in there:",Tt,Tl,Ut,Hl,Ul,Zn='Next, we need to define a data collator for our sequence-to-sequence task. Since mT5 is an encoder-decoder Transformer model, one subtlety with preparing our batches is that during decoding we need to shift the labels to the right by one. This is required to ensure that the decoder only sees the previous ground truth labels and not the current or future ones, which would be easy for the model to memorize. This is similar to how masked self-attention is applied to the inputs in a task like <a href="/course/chapter7/6">causal language modeling</a>.',jt,jl,vn="Luckily, ü§ó Transformers provides a <code>DataCollatorForSeq2Seq</code> collator that will dynamically pad the inputs and the labels for us. To instantiate this collator, we simply need to provide the <code>tokenizer</code> and <code>model</code>:",bt,Ze,ve,Dl,bl,Gn="Let‚Äôs see what this collator produces when fed a small batch of examples. First, we need to remove the columns with strings because the collator won‚Äôt know how to pad these elements:",ft,fl,It,Il,xn="Since the collator expects a list of <code>dict</code>s, where each <code>dict</code> represents a single example in the dataset, we also need to wrangle the data into the expected format before passing it to the data collator:",gt,gl,Ct,Cl,kt,kl,Bn="The main thing to notice here is that the first example is longer than the second one, so the <code>input_ids</code> and <code>attention_mask</code> of the second example have been padded on the right with a <code>[PAD]</code> token (whose ID is <code>0</code>). Similarly, we can see that the <code>labels</code> have been padded with <code>-100</code>s, to make sure the padding tokens are ignored by the loss function. And finally, we can see a new <code>decoder_input_ids</code> which has shifted the labels to the right by inserting a <code>[PAD]</code> token in the first entry.",Zt,Ge,xe,Ll,ql,Zl,vt,vl,Wn="Once you‚Äôve pushed the model to the Hub, you can play with it either via the inference widget or with a <code>pipeline</code> object, as follows:",Gt,Gl,xt,xl,$n="We can feed some examples from the test set (which the model has not seen) to our pipeline to get a feel for the quality of the summaries. First let‚Äôs implement a simple function to show the review, title, and generated summary together:",Bt,Bl,Wt,Wl,_n="Let‚Äôs take a look at one of the English examples we get:",$t,$l,_t,_l,Vt,Vl,Vn="This is not too bad! We can see that our model has actually been able to perform <em>abstractive</em> summarization by augmenting parts of the review with new words. And perhaps the coolest aspect of our model is that it is bilingual, so we can also generate summaries of Spanish reviews:",Rt,Rl,Nt,Nl,At,Al,Rn="The summary translates into ‚ÄúVery easy to read‚Äù in English, which we can see in this case was extracted directly from the review. Nevertheless, this shows the versatility of the mT5 model and has given you a taste of what it‚Äôs like to deal with a multilingual corpus!",Xt,Xl,Nn="Next, we‚Äôll turn our attention to a slightly more complex task: training a language model from scratch.",zt,aa,Qt;v=new di({props:{fw:Z[0]}}),$=new fe({props:{title:"Summarization",local:"summarization",headingTag:"h1"}});const An=[ui,wi],zl=[];function Xn(e,a){return e[0]==="pt"?0:1}k=Xn(Z),R=zl[k]=An[k](Z),_=new Lt({props:{id:"yHnr5Dk2zCI"}}),Y=new fe({props:{title:"Preparing a multilingual corpus",local:"preparing-a-multilingual-corpus",headingTag:"h2"}}),T=new U({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBc3BhbmlzaF9kYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmFtYXpvbl9yZXZpZXdzX211bHRpJTIyJTJDJTIwJTIyZXMlMjIpJTBBZW5nbGlzaF9kYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmFtYXpvbl9yZXZpZXdzX211bHRpJTIyJTJDJTIwJTIyZW4lMjIpJTBBZW5nbGlzaF9kYXRhc2V0",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

spanish_dataset = load_dataset(<span class="hljs-string">&quot;amazon_reviews_multi&quot;</span>, <span class="hljs-string">&quot;es&quot;</span>)
english_dataset = load_dataset(<span class="hljs-string">&quot;amazon_reviews_multi&quot;</span>, <span class="hljs-string">&quot;en&quot;</span>)
english_dataset`,wrap:!1}}),P=new U({props:{code:"RGF0YXNldERpY3QoJTdCJTBBJTIwJTIwJTIwJTIwdHJhaW4lM0ElMjBEYXRhc2V0KCU3QiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGZlYXR1cmVzJTNBJTIwJTVCJ3Jldmlld19pZCclMkMlMjAncHJvZHVjdF9pZCclMkMlMjAncmV2aWV3ZXJfaWQnJTJDJTIwJ3N0YXJzJyUyQyUyMCdyZXZpZXdfYm9keSclMkMlMjAncmV2aWV3X3RpdGxlJyUyQyUyMCdsYW5ndWFnZSclMkMlMjAncHJvZHVjdF9jYXRlZ29yeSclNUQlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBudW1fcm93cyUzQSUyMDIwMDAwMCUwQSUyMCUyMCUyMCUyMCU3RCklMEElMjAlMjAlMjAlMjB2YWxpZGF0aW9uJTNBJTIwRGF0YXNldCglN0IlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBmZWF0dXJlcyUzQSUyMCU1QidyZXZpZXdfaWQnJTJDJTIwJ3Byb2R1Y3RfaWQnJTJDJTIwJ3Jldmlld2VyX2lkJyUyQyUyMCdzdGFycyclMkMlMjAncmV2aWV3X2JvZHknJTJDJTIwJ3Jldmlld190aXRsZSclMkMlMjAnbGFuZ3VhZ2UnJTJDJTIwJ3Byb2R1Y3RfY2F0ZWdvcnknJTVEJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbnVtX3Jvd3MlM0ElMjA1MDAwJTBBJTIwJTIwJTIwJTIwJTdEKSUwQSUyMCUyMCUyMCUyMHRlc3QlM0ElMjBEYXRhc2V0KCU3QiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGZlYXR1cmVzJTNBJTIwJTVCJ3Jldmlld19pZCclMkMlMjAncHJvZHVjdF9pZCclMkMlMjAncmV2aWV3ZXJfaWQnJTJDJTIwJ3N0YXJzJyUyQyUyMCdyZXZpZXdfYm9keSclMkMlMjAncmV2aWV3X3RpdGxlJyUyQyUyMCdsYW5ndWFnZSclMkMlMjAncHJvZHVjdF9jYXRlZ29yeSclNUQlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBudW1fcm93cyUzQSUyMDUwMDAlMEElMjAlMjAlMjAlMjAlN0QpJTBBJTdEKQ==",highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;review_id&#x27;</span>, <span class="hljs-string">&#x27;product_id&#x27;</span>, <span class="hljs-string">&#x27;reviewer_id&#x27;</span>, <span class="hljs-string">&#x27;stars&#x27;</span>, <span class="hljs-string">&#x27;review_body&#x27;</span>, <span class="hljs-string">&#x27;review_title&#x27;</span>, <span class="hljs-string">&#x27;language&#x27;</span>, <span class="hljs-string">&#x27;product_category&#x27;</span>],
        num_rows: <span class="hljs-number">200000</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;review_id&#x27;</span>, <span class="hljs-string">&#x27;product_id&#x27;</span>, <span class="hljs-string">&#x27;reviewer_id&#x27;</span>, <span class="hljs-string">&#x27;stars&#x27;</span>, <span class="hljs-string">&#x27;review_body&#x27;</span>, <span class="hljs-string">&#x27;review_title&#x27;</span>, <span class="hljs-string">&#x27;language&#x27;</span>, <span class="hljs-string">&#x27;product_category&#x27;</span>],
        num_rows: <span class="hljs-number">5000</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;review_id&#x27;</span>, <span class="hljs-string">&#x27;product_id&#x27;</span>, <span class="hljs-string">&#x27;reviewer_id&#x27;</span>, <span class="hljs-string">&#x27;stars&#x27;</span>, <span class="hljs-string">&#x27;review_body&#x27;</span>, <span class="hljs-string">&#x27;review_title&#x27;</span>, <span class="hljs-string">&#x27;language&#x27;</span>, <span class="hljs-string">&#x27;product_category&#x27;</span>],
        num_rows: <span class="hljs-number">5000</span>
    })
})`,wrap:!1}}),ee=new U({props:{code:"ZGVmJTIwc2hvd19zYW1wbGVzKGRhdGFzZXQlMkMlMjBudW1fc2FtcGxlcyUzRDMlMkMlMjBzZWVkJTNENDIpJTNBJTBBJTIwJTIwJTIwJTIwc2FtcGxlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRyYWluJTIyJTVELnNodWZmbGUoc2VlZCUzRHNlZWQpLnNlbGVjdChyYW5nZShudW1fc2FtcGxlcykpJTBBJTIwJTIwJTIwJTIwZm9yJTIwZXhhbXBsZSUyMGluJTIwc2FtcGxlJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwcHJpbnQoZiUyMiU1Q24nJTNFJTNFJTIwVGl0bGUlM0ElMjAlN0JleGFtcGxlJTVCJ3Jldmlld190aXRsZSclNUQlN0QnJTIyKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHByaW50KGYlMjInJTNFJTNFJTIwUmV2aWV3JTNBJTIwJTdCZXhhbXBsZSU1QidyZXZpZXdfYm9keSclNUQlN0QnJTIyKSUwQSUwQSUwQXNob3dfc2FtcGxlcyhlbmdsaXNoX2RhdGFzZXQp",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">show_samples</span>(<span class="hljs-params">dataset, num_samples=<span class="hljs-number">3</span>, seed=<span class="hljs-number">42</span></span>):
    sample = dataset[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=seed).select(<span class="hljs-built_in">range</span>(num_samples))
    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> sample:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt; Title: <span class="hljs-subst">{example[<span class="hljs-string">&#x27;review_title&#x27;</span>]}</span>&#x27;&quot;</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt; Review: <span class="hljs-subst">{example[<span class="hljs-string">&#x27;review_body&#x27;</span>]}</span>&#x27;&quot;</span>)


show_samples(english_dataset)`,wrap:!1}}),ce=new U({props:{code:"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",highlighted:`<span class="hljs-string">&#x27;&gt;&gt; Title: Worked in front position, not rear&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: meh&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Does it‚Äôs job and it‚Äôs gorgeous but mine is falling apart, I had to basically put it together again with hot glue&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: Can\\&#x27;t beat these for the money&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Bought this for handling miscellaneous aircraft parts and hanger &quot;stuff&quot; that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\\&#x27;t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\\&#x27;s heavy duty enough to hold metal parts, but being made of plastic it\\&#x27;s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\\&#x27;t beat it. Best one of these I\\&#x27;ve bought to date-- and I\\&#x27;ve been using some version of these for over forty years.&#x27;</span>`,wrap:!1}}),Ie=new as({props:{$$slots:{default:[Ji]},$$scope:{ctx:Z}}}),We=new U({props:{code:"ZW5nbGlzaF9kYXRhc2V0LnNldF9mb3JtYXQoJTIycGFuZGFzJTIyKSUwQWVuZ2xpc2hfZGYlMjAlM0QlMjBlbmdsaXNoX2RhdGFzZXQlNUIlMjJ0cmFpbiUyMiU1RCU1QiUzQSU1RCUwQSUyMyUyMFNob3clMjBjb3VudHMlMjBmb3IlMjB0b3AlMjAyMCUyMHByb2R1Y3RzJTBBZW5nbGlzaF9kZiU1QiUyMnByb2R1Y3RfY2F0ZWdvcnklMjIlNUQudmFsdWVfY291bnRzKCklNUIlM0EyMCU1RA==",highlighted:`english_dataset.set_format(<span class="hljs-string">&quot;pandas&quot;</span>)
english_df = english_dataset[<span class="hljs-string">&quot;train&quot;</span>][:]
<span class="hljs-comment"># Show counts for top 20 products</span>
english_df[<span class="hljs-string">&quot;product_category&quot;</span>].value_counts()[:<span class="hljs-number">20</span>]`,wrap:!1}}),le=new U({props:{code:"aG9tZSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMDE3Njc5JTBBYXBwYXJlbCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMDE1OTUxJTBBd2lyZWxlc3MlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAxNTcxNyUwQW90aGVyJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwMTM0MTglMEFiZWF1dHklMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAxMjA5MSUwQWRydWdzdG9yZSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMDExNzMwJTBBa2l0Y2hlbiUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMDEwMzgyJTBBdG95JTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwODc0NSUwQXNwb3J0cyUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMDgyNzclMEFhdXRvbW90aXZlJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwNzUwNiUwQWxhd25fYW5kX2dhcmRlbiUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMDczMjclMEFob21lX2ltcHJvdmVtZW50JTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwNzEzNiUwQXBldF9wcm9kdWN0cyUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMDcwODIlMEFkaWdpdGFsX2Vib29rX3B1cmNoYXNlJTIwJTIwJTIwJTIwJTIwNjc0OSUwQXBjJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwNjQwMSUwQWVsZWN0cm9uaWNzJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwNjE4NiUwQW9mZmljZV9wcm9kdWN0JTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwNTUyMSUwQXNob2VzJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwNTE5NyUwQWdyb2NlcnklMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjA0NzMwJTBBYm9vayUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMDM3NTYlMEFOYW1lJTNBJTIwcHJvZHVjdF9jYXRlZ29yeSUyQyUyMGR0eXBlJTNBJTIwaW50NjQ=",highlighted:`home                      <span class="hljs-number">17679</span>
apparel                   <span class="hljs-number">15951</span>
wireless                  <span class="hljs-number">15717</span>
other                     <span class="hljs-number">13418</span>
beauty                    <span class="hljs-number">12091</span>
drugstore                 <span class="hljs-number">11730</span>
kitchen                   <span class="hljs-number">10382</span>
toy                        <span class="hljs-number">8745</span>
sports                     <span class="hljs-number">8277</span>
automotive                 <span class="hljs-number">7506</span>
lawn_and_garden            <span class="hljs-number">7327</span>
home_improvement           <span class="hljs-number">7136</span>
pet_products               <span class="hljs-number">7082</span>
digital_ebook_purchase     <span class="hljs-number">6749</span>
pc                         <span class="hljs-number">6401</span>
electronics                <span class="hljs-number">6186</span>
office_product             <span class="hljs-number">5521</span>
shoes                      <span class="hljs-number">5197</span>
grocery                    <span class="hljs-number">4730</span>
book                       <span class="hljs-number">3756</span>
Name: product_category, dtype: int64`,wrap:!1}}),te=new U({props:{code:"ZGVmJTIwZmlsdGVyX2Jvb2tzKGV4YW1wbGUpJTNBJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwKCUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGV4YW1wbGUlNUIlMjJwcm9kdWN0X2NhdGVnb3J5JTIyJTVEJTIwJTNEJTNEJTIwJTIyYm9vayUyMiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMG9yJTIwZXhhbXBsZSU1QiUyMnByb2R1Y3RfY2F0ZWdvcnklMjIlNUQlMjAlM0QlM0QlMjAlMjJkaWdpdGFsX2Vib29rX3B1cmNoYXNlJTIyJTBBJTIwJTIwJTIwJTIwKQ==",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">filter_books</span>(<span class="hljs-params">example</span>):
    <span class="hljs-keyword">return</span> (
        example[<span class="hljs-string">&quot;product_category&quot;</span>] == <span class="hljs-string">&quot;book&quot;</span>
        <span class="hljs-keyword">or</span> example[<span class="hljs-string">&quot;product_category&quot;</span>] == <span class="hljs-string">&quot;digital_ebook_purchase&quot;</span>
    )`,wrap:!1}}),ne=new U({props:{code:"ZW5nbGlzaF9kYXRhc2V0LnJlc2V0X2Zvcm1hdCgp",highlighted:"english_dataset.reset_format()",wrap:!1}}),re=new U({props:{code:"c3BhbmlzaF9ib29rcyUyMCUzRCUyMHNwYW5pc2hfZGF0YXNldC5maWx0ZXIoZmlsdGVyX2Jvb2tzKSUwQWVuZ2xpc2hfYm9va3MlMjAlM0QlMjBlbmdsaXNoX2RhdGFzZXQuZmlsdGVyKGZpbHRlcl9ib29rcyklMEFzaG93X3NhbXBsZXMoZW5nbGlzaF9ib29rcyk=",highlighted:`spanish_books = spanish_dataset.<span class="hljs-built_in">filter</span>(filter_books)
english_books = english_dataset.<span class="hljs-built_in">filter</span>(filter_books)
show_samples(english_books)`,wrap:!1}}),oe=new U({props:{code:"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",highlighted:`<span class="hljs-string">&#x27;&gt;&gt; Title: I\\&#x27;m dissapointed.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: I guess I had higher expectations for this book from the reviews. I really thought I\\&#x27;d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\\&#x27;m dissapointed.&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: Good art, good price, poor design&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\\&#x27;s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: Helpful&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.&#x27;</span>`,wrap:!1}}),pe=new U({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwY29uY2F0ZW5hdGVfZGF0YXNldHMlMkMlMjBEYXRhc2V0RGljdCUwQSUwQWJvb2tzX2RhdGFzZXQlMjAlM0QlMjBEYXRhc2V0RGljdCgpJTBBJTBBZm9yJTIwc3BsaXQlMjBpbiUyMGVuZ2xpc2hfYm9va3Mua2V5cygpJTNBJTBBJTIwJTIwJTIwJTIwYm9va3NfZGF0YXNldCU1QnNwbGl0JTVEJTIwJTNEJTIwY29uY2F0ZW5hdGVfZGF0YXNldHMoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTVCZW5nbGlzaF9ib29rcyU1QnNwbGl0JTVEJTJDJTIwc3BhbmlzaF9ib29rcyU1QnNwbGl0JTVEJTVEJTBBJTIwJTIwJTIwJTIwKSUwQSUyMCUyMCUyMCUyMGJvb2tzX2RhdGFzZXQlNUJzcGxpdCU1RCUyMCUzRCUyMGJvb2tzX2RhdGFzZXQlNUJzcGxpdCU1RC5zaHVmZmxlKHNlZWQlM0Q0MiklMEElMEElMjMlMjBQZWVrJTIwYXQlMjBhJTIwZmV3JTIwZXhhbXBsZXMlMEFzaG93X3NhbXBsZXMoYm9va3NfZGF0YXNldCk=",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

<span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=<span class="hljs-number">42</span>)

<span class="hljs-comment"># Peek at a few examples</span>
show_samples(books_dataset)`,wrap:!1}}),ue=new U({props:{code:"JyUzRSUzRSUyMFRpdGxlJTNBJTIwRWFzeSUyMHRvJTIwZm9sbG93ISEhISclMEEnJTNFJTNFJTIwUmV2aWV3JTNBJTIwSSUyMGxvdmVkJTIwVGhlJTIwZGFzaCUyMGRpZXQlMjB3ZWlnaHQlMjBsb3NzJTIwU29sdXRpb24uJTIwTmV2ZXIlMjBodW5ncnkuJTIwSSUyMHdvdWxkJTIwcmVjb21tZW5kJTIwdGhpcyUyMGRpZXQuJTIwQWxzbyUyMHRoZSUyMG1lbnVzJTIwYXJlJTIwd2VsbCUyMHJvdW5kZWQuJTIwVHJ5JTIwaXQuJTIwSGFzJTIwbG90cyUyMG9mJTIwdGhlJTIwaW5mb3JtYXRpb24lMjBuZWVkJTIwdGhhbmtzLiclMEElMEEnJTNFJTNFJTIwVGl0bGUlM0ElMjBQQVJDSUFMTUVOVEUlMjBEQSVDMyU5MUFETyclMEEnJTNFJTNFJTIwUmV2aWV3JTNBJTIwTWUlMjBsbGVnJUMzJUIzJTIwZWwlMjBkJUMzJUFEYSUyMHF1ZSUyMHRvY2FiYSUyQyUyMGp1bnRvJTIwYSUyMG90cm9zJTIwbGlicm9zJTIwcXVlJTIwcGVkJUMzJUFEJTJDJTIwcGVybyUyMGxhJTIwY2FqYSUyMGxsZWclQzMlQjMlMjBlbiUyMG1hbCUyMGVzdGFkbyUyMGxvJTIwY3VhbCUyMGRhJUMzJUIxJUMzJUIzJTIwbGFzJTIwZXNxdWluYXMlMjBkZSUyMGxvcyUyMGxpYnJvcyUyMHBvcnF1ZSUyMHZlbiVDMyVBRGFuJTIwc2luJTIwcHJvdGVjY2klQzMlQjNuJTIwKGZvcnJvKS4nJTBBJTBBJyUzRSUzRSUyMFRpdGxlJTNBJTIwbm8lMjBsbyUyMGhlJTIwcG9kaWRvJTIwZGVzY2FyZ2FyJyUwQSclM0UlM0UlMjBSZXZpZXclM0ElMjBpZ3VhbCUyMHF1ZSUyMGVsJTIwYW50ZXJpb3In",highlighted:`<span class="hljs-string">&#x27;&gt;&gt; Title: Easy to follow!!!!&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: PARCIALMENTE DA√ëADO&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Me lleg√≥ el d√≠a que tocaba, junto a otros libros que ped√≠, pero la caja lleg√≥ en mal estado lo cual da√±√≥ las esquinas de los libros porque ven√≠an sin protecci√≥n (forro).&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: no lo he podido descargar&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: igual que el anterior&#x27;</span>`,wrap:!1}}),me=new U({props:{code:"Ym9va3NfZGF0YXNldCUyMCUzRCUyMGJvb2tzX2RhdGFzZXQuZmlsdGVyKGxhbWJkYSUyMHglM0ElMjBsZW4oeCU1QiUyMnJldmlld190aXRsZSUyMiU1RC5zcGxpdCgpKSUyMCUzRSUyMDIp",highlighted:'books_dataset = books_dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x[<span class="hljs-string">&quot;review_title&quot;</span>].split()) &gt; <span class="hljs-number">2</span>)',wrap:!1}}),hs=new fe({props:{title:"Models for text summarization",local:"models-for-text-summarization",headingTag:"h2"}}),Ke=new as({props:{$$slots:{default:[Ti]},$$scope:{ctx:Z}}}),Ts=new fe({props:{title:"Preprocessing the data",local:"preprocessing-the-data",headingTag:"h2"}}),Us=new Lt({props:{id:"1m7BerpSq8A"}}),bs=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEFtb2RlbF9jaGVja3BvaW50JTIwJTNEJTIwJTIyZ29vZ2xlJTJGbXQ1LXNtYWxsJTIyJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfY2hlY2twb2ludCk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

model_checkpoint = <span class="hljs-string">&quot;google/mt5-small&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)`,wrap:!1}}),Pe=new as({props:{$$slots:{default:[Ui]},$$scope:{ctx:Z}}}),Is=new U({props:{code:"aW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMkklMjBsb3ZlZCUyMHJlYWRpbmclMjB0aGUlMjBIdW5nZXIlMjBHYW1lcyElMjIpJTBBaW5wdXRz",highlighted:`inputs = tokenizer(<span class="hljs-string">&quot;I loved reading the Hunger Games!&quot;</span>)
inputs`,wrap:!1}}),gs=new U({props:{code:"JTdCJ2lucHV0X2lkcyclM0ElMjAlNUIzMzYlMkMlMjAyNTklMkMlMjAyODM4NyUyQyUyMDExODA3JTJDJTIwMjg3JTJDJTIwNjI4OTMlMkMlMjAyOTUlMkMlMjAxMjUwNyUyQyUyMDElNUQlMkMlMjAnYXR0ZW50aW9uX21hc2snJTNBJTIwJTVCMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTVEJTdE",highlighted:'{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">336</span>, <span class="hljs-number">259</span>, <span class="hljs-number">28387</span>, <span class="hljs-number">11807</span>, <span class="hljs-number">287</span>, <span class="hljs-number">62893</span>, <span class="hljs-number">295</span>, <span class="hljs-number">12507</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}',wrap:!1}}),ks=new U({props:{code:"dG9rZW5pemVyLmNvbnZlcnRfaWRzX3RvX3Rva2VucyhpbnB1dHMuaW5wdXRfaWRzKQ==",highlighted:"tokenizer.convert_ids_to_tokens(inputs.input_ids)",wrap:!1}}),Zs=new U({props:{code:"JTVCJyVFMiU5NiU4MUknJTJDJTIwJyVFMiU5NiU4MSclMkMlMjAnbG92ZWQnJTJDJTIwJyVFMiU5NiU4MXJlYWRpbmcnJTJDJTIwJyVFMiU5NiU4MXRoZSclMkMlMjAnJUUyJTk2JTgxSHVuZyclMkMlMjAnZXInJTJDJTIwJyVFMiU5NiU4MUdhbWVzJyUyQyUyMCclM0MlMkZzJTNFJyU1RA==",highlighted:'[<span class="hljs-string">&#x27;‚ñÅI&#x27;</span>, <span class="hljs-string">&#x27;‚ñÅ&#x27;</span>, <span class="hljs-string">&#x27;loved&#x27;</span>, <span class="hljs-string">&#x27;‚ñÅreading&#x27;</span>, <span class="hljs-string">&#x27;‚ñÅthe&#x27;</span>, <span class="hljs-string">&#x27;‚ñÅHung&#x27;</span>, <span class="hljs-string">&#x27;er&#x27;</span>, <span class="hljs-string">&#x27;‚ñÅGames&#x27;</span>, <span class="hljs-string">&#x27;&lt;/s&gt;&#x27;</span>]',wrap:!1}}),xs=new U({props:{code:"bWF4X2lucHV0X2xlbmd0aCUyMCUzRCUyMDUxMiUwQW1heF90YXJnZXRfbGVuZ3RoJTIwJTNEJTIwMzAlMEElMEElMEFkZWYlMjBwcmVwcm9jZXNzX2Z1bmN0aW9uKGV4YW1wbGVzKSUzQSUwQSUyMCUyMCUyMCUyMG1vZGVsX2lucHV0cyUyMCUzRCUyMHRva2VuaXplciglMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBleGFtcGxlcyU1QiUyMnJldmlld19ib2R5JTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbWF4X2xlbmd0aCUzRG1heF9pbnB1dF9sZW5ndGglMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjB0cnVuY2F0aW9uJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMCklMEElMjAlMjAlMjAlMjBsYWJlbHMlMjAlM0QlMjB0b2tlbml6ZXIoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwZXhhbXBsZXMlNUIlMjJyZXZpZXdfdGl0bGUlMjIlNUQlMkMlMjBtYXhfbGVuZ3RoJTNEbWF4X3RhcmdldF9sZW5ndGglMkMlMjB0cnVuY2F0aW9uJTNEVHJ1ZSUwQSUyMCUyMCUyMCUyMCklMEElMjAlMjAlMjAlMjBtb2RlbF9pbnB1dHMlNUIlMjJsYWJlbHMlMjIlNUQlMjAlM0QlMjBsYWJlbHMlNUIlMjJpbnB1dF9pZHMlMjIlNUQlMEElMjAlMjAlMjAlMjByZXR1cm4lMjBtb2RlbF9pbnB1dHM=",highlighted:`max_input_length = <span class="hljs-number">512</span>
max_target_length = <span class="hljs-number">30</span>


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    model_inputs = tokenizer(
        examples[<span class="hljs-string">&quot;review_body&quot;</span>],
        max_length=max_input_length,
        truncation=<span class="hljs-literal">True</span>,
    )
    labels = tokenizer(
        examples[<span class="hljs-string">&quot;review_title&quot;</span>], max_length=max_target_length, truncation=<span class="hljs-literal">True</span>
    )
    model_inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels[<span class="hljs-string">&quot;input_ids&quot;</span>]
    <span class="hljs-keyword">return</span> model_inputs`,wrap:!1}}),$s=new U({props:{code:"dG9rZW5pemVkX2RhdGFzZXRzJTIwJTNEJTIwYm9va3NfZGF0YXNldC5tYXAocHJlcHJvY2Vzc19mdW5jdGlvbiUyQyUyMGJhdGNoZWQlM0RUcnVlKQ==",highlighted:'tokenized_datasets = books_dataset.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)',wrap:!1}}),Oe=new as({props:{$$slots:{default:[ji]},$$scope:{ctx:Z}}}),Vs=new fe({props:{title:"Metrics for text summarization",local:"metrics-for-text-summarization",headingTag:"h2"}}),Rs=new Lt({props:{id:"TMshhnrEXlg"}}),Xs=new U({props:{code:"Z2VuZXJhdGVkX3N1bW1hcnklMjAlM0QlMjAlMjJJJTIwYWJzb2x1dGVseSUyMGxvdmVkJTIwcmVhZGluZyUyMHRoZSUyMEh1bmdlciUyMEdhbWVzJTIyJTBBcmVmZXJlbmNlX3N1bW1hcnklMjAlM0QlMjAlMjJJJTIwbG92ZWQlMjByZWFkaW5nJTIwdGhlJTIwSHVuZ2VyJTIwR2FtZXMlMjI=",highlighted:`generated_summary = <span class="hljs-string">&quot;I absolutely loved reading the Hunger Games&quot;</span>
reference_summary = <span class="hljs-string">&quot;I loved reading the Hunger Games&quot;</span>`,wrap:!1}}),es=new as({props:{$$slots:{default:[bi]},$$scope:{ctx:Z}}}),Es=new U({props:{code:"IXBpcCUyMGluc3RhbGwlMjByb3VnZV9zY29yZQ==",highlighted:"!pip install rouge_score",wrap:!1}}),Hs=new U({props:{code:"aW1wb3J0JTIwZXZhbHVhdGUlMEElMEFyb3VnZV9zY29yZSUyMCUzRCUyMGV2YWx1YXRlLmxvYWQoJTIycm91Z2UlMjIp",highlighted:`<span class="hljs-keyword">import</span> evaluate

rouge_score = evaluate.load(<span class="hljs-string">&quot;rouge&quot;</span>)`,wrap:!1}}),Ls=new U({props:{code:"c2NvcmVzJTIwJTNEJTIwcm91Z2Vfc2NvcmUuY29tcHV0ZSglMEElMjAlMjAlMjAlMjBwcmVkaWN0aW9ucyUzRCU1QmdlbmVyYXRlZF9zdW1tYXJ5JTVEJTJDJTIwcmVmZXJlbmNlcyUzRCU1QnJlZmVyZW5jZV9zdW1tYXJ5JTVEJTBBKSUwQXNjb3Jlcw==",highlighted:`scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores`,wrap:!1}}),qs=new U({props:{code:"JTdCJ3JvdWdlMSclM0ElMjBBZ2dyZWdhdGVTY29yZShsb3clM0RTY29yZShwcmVjaXNpb24lM0QwLjg2JTJDJTIwcmVjYWxsJTNEMS4wJTJDJTIwZm1lYXN1cmUlM0QwLjkyKSUyQyUyMG1pZCUzRFNjb3JlKHByZWNpc2lvbiUzRDAuODYlMkMlMjByZWNhbGwlM0QxLjAlMkMlMjBmbWVhc3VyZSUzRDAuOTIpJTJDJTIwaGlnaCUzRFNjb3JlKHByZWNpc2lvbiUzRDAuODYlMkMlMjByZWNhbGwlM0QxLjAlMkMlMjBmbWVhc3VyZSUzRDAuOTIpKSUyQyUwQSUyMCdyb3VnZTInJTNBJTIwQWdncmVnYXRlU2NvcmUobG93JTNEU2NvcmUocHJlY2lzaW9uJTNEMC42NyUyQyUyMHJlY2FsbCUzRDAuOCUyQyUyMGZtZWFzdXJlJTNEMC43MyklMkMlMjBtaWQlM0RTY29yZShwcmVjaXNpb24lM0QwLjY3JTJDJTIwcmVjYWxsJTNEMC44JTJDJTIwZm1lYXN1cmUlM0QwLjczKSUyQyUyMGhpZ2glM0RTY29yZShwcmVjaXNpb24lM0QwLjY3JTJDJTIwcmVjYWxsJTNEMC44JTJDJTIwZm1lYXN1cmUlM0QwLjczKSklMkMlMEElMjAncm91Z2VMJyUzQSUyMEFnZ3JlZ2F0ZVNjb3JlKGxvdyUzRFNjb3JlKHByZWNpc2lvbiUzRDAuODYlMkMlMjByZWNhbGwlM0QxLjAlMkMlMjBmbWVhc3VyZSUzRDAuOTIpJTJDJTIwbWlkJTNEU2NvcmUocHJlY2lzaW9uJTNEMC44NiUyQyUyMHJlY2FsbCUzRDEuMCUyQyUyMGZtZWFzdXJlJTNEMC45MiklMkMlMjBoaWdoJTNEU2NvcmUocHJlY2lzaW9uJTNEMC44NiUyQyUyMHJlY2FsbCUzRDEuMCUyQyUyMGZtZWFzdXJlJTNEMC45MikpJTJDJTBBJTIwJ3JvdWdlTHN1bSclM0ElMjBBZ2dyZWdhdGVTY29yZShsb3clM0RTY29yZShwcmVjaXNpb24lM0QwLjg2JTJDJTIwcmVjYWxsJTNEMS4wJTJDJTIwZm1lYXN1cmUlM0QwLjkyKSUyQyUyMG1pZCUzRFNjb3JlKHByZWNpc2lvbiUzRDAuODYlMkMlMjByZWNhbGwlM0QxLjAlMkMlMjBmbWVhc3VyZSUzRDAuOTIpJTJDJTIwaGlnaCUzRFNjb3JlKHByZWNpc2lvbiUzRDAuODYlMkMlMjByZWNhbGwlM0QxLjAlMkMlMjBmbWVhc3VyZSUzRDAuOTIpKSU3RA==",highlighted:`{<span class="hljs-string">&#x27;rouge1&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), mid=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), high=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>)),
 <span class="hljs-string">&#x27;rouge2&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.67</span>, recall=<span class="hljs-number">0.8</span>, fmeasure=<span class="hljs-number">0.73</span>), mid=Score(precision=<span class="hljs-number">0.67</span>, recall=<span class="hljs-number">0.8</span>, fmeasure=<span class="hljs-number">0.73</span>), high=Score(precision=<span class="hljs-number">0.67</span>, recall=<span class="hljs-number">0.8</span>, fmeasure=<span class="hljs-number">0.73</span>)),
 <span class="hljs-string">&#x27;rougeL&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), mid=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), high=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>)),
 <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), mid=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), high=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>))}`,wrap:!1}}),Ps=new U({props:{code:"c2NvcmVzJTVCJTIycm91Z2UxJTIyJTVELm1pZA==",highlighted:'scores[<span class="hljs-string">&quot;rouge1&quot;</span>].mid',wrap:!1}}),Os=new U({props:{code:"U2NvcmUocHJlY2lzaW9uJTNEMC44NiUyQyUyMHJlY2FsbCUzRDEuMCUyQyUyMGZtZWFzdXJlJTNEMC45Mik=",highlighted:'Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>)',wrap:!1}}),ss=new as({props:{$$slots:{default:[fi]},$$scope:{ctx:Z}}}),ll=new fe({props:{title:"Creating a strong baseline",local:"creating-a-strong-baseline",headingTag:"h3"}}),tl=new U({props:{code:"IXBpcCUyMGluc3RhbGwlMjBubHRr",highlighted:"!pip install nltk",wrap:!1}}),il=new U({props:{code:"aW1wb3J0JTIwbmx0ayUwQSUwQW5sdGsuZG93bmxvYWQoJTIycHVua3QlMjIp",highlighted:`<span class="hljs-keyword">import</span> nltk

nltk.download(<span class="hljs-string">&quot;punkt&quot;</span>)`,wrap:!1}}),ol=new U({props:{code:"ZnJvbSUyMG5sdGsudG9rZW5pemUlMjBpbXBvcnQlMjBzZW50X3Rva2VuaXplJTBBJTBBJTBBZGVmJTIwdGhyZWVfc2VudGVuY2Vfc3VtbWFyeSh0ZXh0KSUzQSUwQSUyMCUyMCUyMCUyMHJldHVybiUyMCUyMiU1Q24lMjIuam9pbihzZW50X3Rva2VuaXplKHRleHQpJTVCJTNBMyU1RCklMEElMEElMEFwcmludCh0aHJlZV9zZW50ZW5jZV9zdW1tYXJ5KGJvb2tzX2RhdGFzZXQlNUIlMjJ0cmFpbiUyMiU1RCU1QjElNUQlNUIlMjJyZXZpZXdfYm9keSUyMiU1RCkp",highlighted:`<span class="hljs-keyword">from</span> nltk.tokenize <span class="hljs-keyword">import</span> sent_tokenize


<span class="hljs-keyword">def</span> <span class="hljs-title function_">three_sentence_summary</span>(<span class="hljs-params">text</span>):
    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(text)[:<span class="hljs-number">3</span>])


<span class="hljs-built_in">print</span>(three_sentence_summary(books_dataset[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">1</span>][<span class="hljs-string">&quot;review_body&quot;</span>]))`,wrap:!1}}),pl=new U({props:{code:"J0klMjBncmV3JTIwdXAlMjByZWFkaW5nJTIwS29vbnR6JTJDJTIwYW5kJTIweWVhcnMlMjBhZ28lMkMlMjBJJTIwc3RvcHBlZCUyQ2NvbnZpbmNlZCUyMGklMjBoYWQlMjAlMjJvdXRncm93biUyMiUyMGhpbS4nJTBBJ1N0aWxsJTJDd2hlbiUyMGElMjBmcmllbmQlMjB3YXMlMjBsb29raW5nJTIwZm9yJTIwc29tZXRoaW5nJTIwc3VzcGVuc2VmdWwlMjB0b28lMjByZWFkJTJDJTIwSSUyMHN1Z2dlc3RlZCUyMEtvb250ei4nJTBBJ1NoZSUyMGZvdW5kJTIwU3RyYW5nZXJzLic=",highlighted:`<span class="hljs-string">&#x27;I grew up reading Koontz, and years ago, I stopped,convinced i had &quot;outgrown&quot; him.&#x27;</span>
<span class="hljs-string">&#x27;Still,when a friend was looking for something suspenseful too read, I suggested Koontz.&#x27;</span>
<span class="hljs-string">&#x27;She found Strangers.&#x27;</span>`,wrap:!1}}),Ml=new U({props:{code:"ZGVmJTIwZXZhbHVhdGVfYmFzZWxpbmUoZGF0YXNldCUyQyUyMG1ldHJpYyklM0ElMEElMjAlMjAlMjAlMjBzdW1tYXJpZXMlMjAlM0QlMjAlNUJ0aHJlZV9zZW50ZW5jZV9zdW1tYXJ5KHRleHQpJTIwZm9yJTIwdGV4dCUyMGluJTIwZGF0YXNldCU1QiUyMnJldmlld19ib2R5JTIyJTVEJTVEJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwbWV0cmljLmNvbXB1dGUocHJlZGljdGlvbnMlM0RzdW1tYXJpZXMlMkMlMjByZWZlcmVuY2VzJTNEZGF0YXNldCU1QiUyMnJldmlld190aXRsZSUyMiU1RCk=",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_baseline</span>(<span class="hljs-params">dataset, metric</span>):
    summaries = [three_sentence_summary(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> dataset[<span class="hljs-string">&quot;review_body&quot;</span>]]
    <span class="hljs-keyword">return</span> metric.compute(predictions=summaries, references=dataset[<span class="hljs-string">&quot;review_title&quot;</span>])`,wrap:!1}}),hl=new U({props:{code:"aW1wb3J0JTIwcGFuZGFzJTIwYXMlMjBwZCUwQSUwQXNjb3JlJTIwJTNEJTIwZXZhbHVhdGVfYmFzZWxpbmUoYm9va3NfZGF0YXNldCU1QiUyMnZhbGlkYXRpb24lMjIlNUQlMkMlMjByb3VnZV9zY29yZSklMEFyb3VnZV9uYW1lcyUyMCUzRCUyMCU1QiUyMnJvdWdlMSUyMiUyQyUyMCUyMnJvdWdlMiUyMiUyQyUyMCUyMnJvdWdlTCUyMiUyQyUyMCUyMnJvdWdlTHN1bSUyMiU1RCUwQXJvdWdlX2RpY3QlMjAlM0QlMjBkaWN0KChybiUyQyUyMHJvdW5kKHNjb3JlJTVCcm4lNUQubWlkLmZtZWFzdXJlJTIwKiUyMDEwMCUyQyUyMDIpKSUyMGZvciUyMHJuJTIwaW4lMjByb3VnZV9uYW1lcyklMEFyb3VnZV9kaWN0",highlighted:`<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

score = evaluate_baseline(books_dataset[<span class="hljs-string">&quot;validation&quot;</span>], rouge_score)
rouge_names = [<span class="hljs-string">&quot;rouge1&quot;</span>, <span class="hljs-string">&quot;rouge2&quot;</span>, <span class="hljs-string">&quot;rougeL&quot;</span>, <span class="hljs-string">&quot;rougeLsum&quot;</span>]
rouge_dict = <span class="hljs-built_in">dict</span>((rn, <span class="hljs-built_in">round</span>(score[rn].mid.fmeasure * <span class="hljs-number">100</span>, <span class="hljs-number">2</span>)) <span class="hljs-keyword">for</span> rn <span class="hljs-keyword">in</span> rouge_names)
rouge_dict`,wrap:!1}}),yl=new U({props:{code:"JTdCJ3JvdWdlMSclM0ElMjAxNi43NCUyQyUyMCdyb3VnZTInJTNBJTIwOC44MyUyQyUyMCdyb3VnZUwnJTNBJTIwMTUuNiUyQyUyMCdyb3VnZUxzdW0nJTNBJTIwMTUuOTYlN0Q=",highlighted:'{<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">16.74</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">8.83</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">15.6</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">15.96</span>}',wrap:!1}});const zn=[gi,Ii],Ql=[];function Qn(e,a){return e[0]==="pt"?0:1}Ce=Qn(Z),ke=Ql[Ce]=zn[Ce](Z),ls=new as({props:{$$slots:{default:[Ci]},$$scope:{ctx:Z}}}),ul=new U({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`,wrap:!1}}),Tl=new U({props:{code:"aHVnZ2luZ2ZhY2UtY2xpJTIwbG9naW4=",highlighted:'huggingface-<span class="hljs-keyword">cli</span> login',wrap:!1}});let H=Z[0]==="pt"&&ti();const Yn=[Zi,ki],Yl=[];function Sn(e,a){return e[0]==="pt"?0:1}Ze=Sn(Z),ve=Yl[Ze]=Yn[Ze](Z),fl=new U({props:{code:"dG9rZW5pemVkX2RhdGFzZXRzJTIwJTNEJTIwdG9rZW5pemVkX2RhdGFzZXRzLnJlbW92ZV9jb2x1bW5zKCUwQSUyMCUyMCUyMCUyMGJvb2tzX2RhdGFzZXQlNUIlMjJ0cmFpbiUyMiU1RC5jb2x1bW5fbmFtZXMlMEEp",highlighted:`tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset[<span class="hljs-string">&quot;train&quot;</span>].column_names
)`,wrap:!1}}),gl=new U({props:{code:"ZmVhdHVyZXMlMjAlM0QlMjAlNUJ0b2tlbml6ZWRfZGF0YXNldHMlNUIlMjJ0cmFpbiUyMiU1RCU1QmklNUQlMjBmb3IlMjBpJTIwaW4lMjByYW5nZSgyKSU1RCUwQWRhdGFfY29sbGF0b3IoZmVhdHVyZXMp",highlighted:`features = [tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)]
data_collator(features)`,wrap:!1}}),Cl=new U({props:{code:"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",highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,
         <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,
         <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]), <span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[  <span class="hljs-number">1494</span>,    <span class="hljs-number">259</span>,   <span class="hljs-number">8622</span>,    <span class="hljs-number">390</span>,    <span class="hljs-number">259</span>,    <span class="hljs-number">262</span>,   <span class="hljs-number">2316</span>,   <span class="hljs-number">3435</span>,    <span class="hljs-number">955</span>,
            <span class="hljs-number">772</span>,    <span class="hljs-number">281</span>,    <span class="hljs-number">772</span>,   <span class="hljs-number">1617</span>,    <span class="hljs-number">263</span>,    <span class="hljs-number">305</span>,  <span class="hljs-number">14701</span>,    <span class="hljs-number">260</span>,   <span class="hljs-number">1385</span>,
           <span class="hljs-number">3031</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">24146</span>,    <span class="hljs-number">332</span>,   <span class="hljs-number">1037</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">43906</span>,    <span class="hljs-number">305</span>,    <span class="hljs-number">336</span>,
            <span class="hljs-number">260</span>,      <span class="hljs-number">1</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>],
        [   <span class="hljs-number">259</span>,  <span class="hljs-number">27531</span>,  <span class="hljs-number">13483</span>,    <span class="hljs-number">259</span>,   <span class="hljs-number">7505</span>,    <span class="hljs-number">260</span>, <span class="hljs-number">112240</span>,  <span class="hljs-number">15192</span>,    <span class="hljs-number">305</span>,
          <span class="hljs-number">53198</span>,    <span class="hljs-number">276</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">74060</span>,    <span class="hljs-number">263</span>,    <span class="hljs-number">260</span>,    <span class="hljs-number">459</span>,  <span class="hljs-number">25640</span>,    <span class="hljs-number">776</span>,
           <span class="hljs-number">2119</span>,    <span class="hljs-number">336</span>,    <span class="hljs-number">259</span>,   <span class="hljs-number">2220</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">18896</span>,    <span class="hljs-number">288</span>,   <span class="hljs-number">4906</span>,    <span class="hljs-number">288</span>,
           <span class="hljs-number">1037</span>,   <span class="hljs-number">3931</span>,    <span class="hljs-number">260</span>,   <span class="hljs-number">7083</span>, <span class="hljs-number">101476</span>,   <span class="hljs-number">1143</span>,    <span class="hljs-number">260</span>,      <span class="hljs-number">1</span>]]), <span class="hljs-string">&#x27;labels&#x27;</span>: tensor([[ <span class="hljs-number">7483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">2364</span>, <span class="hljs-number">15695</span>,     <span class="hljs-number">1</span>,  -<span class="hljs-number">100</span>],
        [  <span class="hljs-number">259</span>, <span class="hljs-number">27531</span>, <span class="hljs-number">13483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">7505</span>,     <span class="hljs-number">1</span>]]), <span class="hljs-string">&#x27;decoder_input_ids&#x27;</span>: tensor([[    <span class="hljs-number">0</span>,  <span class="hljs-number">7483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">2364</span>, <span class="hljs-number">15695</span>,     <span class="hljs-number">1</span>],
        [    <span class="hljs-number">0</span>,   <span class="hljs-number">259</span>, <span class="hljs-number">27531</span>, <span class="hljs-number">13483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">7505</span>]])}`,wrap:!1}});const En=[Gi,vi],Sl=[];function Fn(e,a){return e[0]==="pt"?0:1}Ge=Fn(Z),xe=Sl[Ge]=En[Ge](Z);let D=Z[0]==="pt"&&ni(Z);return Zl=new fe({props:{title:"Using your fine-tuned model",local:"using-your-fine-tuned-model",headingTag:"h2"}}),Gl=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBaHViX21vZGVsX2lkJTIwJTNEJTIwJTIyaHVnZ2luZ2ZhY2UtY291cnNlJTJGbXQ1LXNtYWxsLWZpbmV0dW5lZC1hbWF6b24tZW4tZXMlMjIlMEFzdW1tYXJpemVyJTIwJTNEJTIwcGlwZWxpbmUoJTIyc3VtbWFyaXphdGlvbiUyMiUyQyUyMG1vZGVsJTNEaHViX21vZGVsX2lkKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

hub_model_id = <span class="hljs-string">&quot;huggingface-course/mt5-small-finetuned-amazon-en-es&quot;</span>
summarizer = pipeline(<span class="hljs-string">&quot;summarization&quot;</span>, model=hub_model_id)`,wrap:!1}}),Bl=new U({props:{code:"ZGVmJTIwcHJpbnRfc3VtbWFyeShpZHgpJTNBJTBBJTIwJTIwJTIwJTIwcmV2aWV3JTIwJTNEJTIwYm9va3NfZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUJpZHglNUQlNUIlMjJyZXZpZXdfYm9keSUyMiU1RCUwQSUyMCUyMCUyMCUyMHRpdGxlJTIwJTNEJTIwYm9va3NfZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUJpZHglNUQlNUIlMjJyZXZpZXdfdGl0bGUlMjIlNUQlMEElMjAlMjAlMjAlMjBzdW1tYXJ5JTIwJTNEJTIwc3VtbWFyaXplcihib29rc19kYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QmlkeCU1RCU1QiUyMnJldmlld19ib2R5JTIyJTVEKSU1QjAlNUQlNUIlMjJzdW1tYXJ5X3RleHQlMjIlNUQlMEElMjAlMjAlMjAlMjBwcmludChmJTIyJyUzRSUzRSUzRSUyMFJldmlldyUzQSUyMCU3QnJldmlldyU3RCclMjIpJTBBJTIwJTIwJTIwJTIwcHJpbnQoZiUyMiU1Q24nJTNFJTNFJTNFJTIwVGl0bGUlM0ElMjAlN0J0aXRsZSU3RCclMjIpJTBBJTIwJTIwJTIwJTIwcHJpbnQoZiUyMiU1Q24nJTNFJTNFJTNFJTIwU3VtbWFyeSUzQSUyMCU3QnN1bW1hcnklN0QnJTIyKQ==",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">print_summary</span>(<span class="hljs-params">idx</span>):
    review = books_dataset[<span class="hljs-string">&quot;test&quot;</span>][idx][<span class="hljs-string">&quot;review_body&quot;</span>]
    title = books_dataset[<span class="hljs-string">&quot;test&quot;</span>][idx][<span class="hljs-string">&quot;review_title&quot;</span>]
    summary = summarizer(books_dataset[<span class="hljs-string">&quot;test&quot;</span>][idx][<span class="hljs-string">&quot;review_body&quot;</span>])[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;summary_text&quot;</span>]
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Review: <span class="hljs-subst">{review}</span>&#x27;&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; Title: <span class="hljs-subst">{title}</span>&#x27;&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; Summary: <span class="hljs-subst">{summary}</span>&#x27;&quot;</span>)`,wrap:!1}}),$l=new U({props:{code:"cHJpbnRfc3VtbWFyeSgxMDAp",highlighted:'print_summary(<span class="hljs-number">100</span>)',wrap:!1}}),_l=new U({props:{code:"JyUzRSUzRSUzRSUyMFJldmlldyUzQSUyME5vdGhpbmclMjBzcGVjaWFsJTIwYXQlMjBhbGwlMjBhYm91dCUyMHRoaXMlMjBwcm9kdWN0Li4uJTIwdGhlJTIwYm9vayUyMGlzJTIwdG9vJTIwc21hbGwlMjBhbmQlMjBzdGlmZiUyMGFuZCUyMGhhcmQlMjB0byUyMHdyaXRlJTIwaW4uJTIwVGhlJTIwaHVnZSUyMHN0aWNrZXIlMjBvbiUyMHRoZSUyMGJhY2slMjBkb2VzbiVFMiU4MCU5OXQlMjBjb21lJTIwb2ZmJTIwYW5kJTIwbG9va3MlMjBzdXBlciUyMHRhY2t5LiUyMEklMjB3b3VsZCUyMG5vdCUyMHB1cmNoYXNlJTIwdGhpcyUyMGFnYWluLiUyMEklMjBjb3VsZCUyMGhhdmUlMjBqdXN0JTIwYm91Z2h0JTIwYSUyMGpvdXJuYWwlMjBmcm9tJTIwdGhlJTIwZG9sbGFyJTIwc3RvcmUlMjBhbmQlMjBpdCUyMHdvdWxkJTIwYmUlMjBiYXNpY2FsbHklMjB0aGUlMjBzYW1lJTIwdGhpbmcuJTIwSXQlRTIlODAlOTlzJTIwYWxzbyUyMHJlYWxseSUyMGV4cGVuc2l2ZSUyMGZvciUyMHdoYXQlMjBpdCUyMGlzLiclMEElMEEnJTNFJTNFJTNFJTIwVGl0bGUlM0ElMjBOb3QlMjBpbXByZXNzZWQlMjBhdCUyMGFsbC4uLiUyMGJ1eSUyMHNvbWV0aGluZyUyMGVsc2UnJTBBJTBBJyUzRSUzRSUzRSUyMFN1bW1hcnklM0ElMjBOb3RoaW5nJTIwc3BlY2lhbCUyMGF0JTIwYWxsJTIwYWJvdXQlMjB0aGlzJTIwcHJvZHVjdCc=",highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn‚Äôt come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It‚Äôs also really expensive for what it is.&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Title: Not impressed at all... buy something else&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Summary: Nothing special at all about this product&#x27;</span>`,wrap:!1}}),Rl=new U({props:{code:"cHJpbnRfc3VtbWFyeSgwKQ==",highlighted:'print_summary(<span class="hljs-number">0</span>)',wrap:!1}}),Nl=new U({props:{code:"JyUzRSUzRSUzRSUyMFJldmlldyUzQSUyMEVzJTIwdW5hJTIwdHJpbG9naWElMjBxdWUlMjBzZSUyMGhhY2UlMjBtdXklMjBmYWNpbCUyMGRlJTIwbGVlci4lMjBNZSUyMGhhJTIwZ3VzdGFkbyUyQyUyMG5vJTIwbWUlMjBlc3BlcmFiYSUyMGVsJTIwZmluYWwlMjBwYXJhJTIwbmFkYSclMEElMEEnJTNFJTNFJTNFJTIwVGl0bGUlM0ElMjBCdWVuYSUyMGxpdGVyYXR1cmElMjBwYXJhJTIwYWRvbGVzY2VudGVzJyUwQSUwQSclM0UlM0UlM0UlMjBTdW1tYXJ5JTNBJTIwTXV5JTIwZmFjaWwlMjBkZSUyMGxlZXIn",highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Title: Buena literatura para adolescentes&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Summary: Muy facil de leer&#x27;</span>`,wrap:!1}}),{c(){r=M("meta"),j=t(),o=M("p"),I=t(),h(v.$$.fragment),C=t(),h($.$$.fragment),f=t(),R.c(),G=t(),z=M("p"),z.innerHTML=S,N=t(),h(_.$$.fragment),B=t(),g=M("p"),g.innerHTML=x,L=t(),A=M("iframe"),K=t(),X=M("p"),X.textContent=q,E=t(),h(Y.$$.fragment),F=t(),J=M("p"),J.innerHTML=V,Me=t(),h(T.$$.fragment),W=t(),h(P.$$.fragment),Ne=t(),O=M("p"),O.innerHTML=ea,Ae=t(),h(ee.$$.fragment),ns=t(),h(ce.$$.fragment),he=t(),h(Ie.$$.fragment),Xe=t(),se=M("p"),se.innerHTML=is,ye=t(),h(We.$$.fragment),ze=t(),h(le.$$.fragment),Qe=t(),ae=M("p"),ae.innerHTML=sa,Ye=t(),h(te.$$.fragment),rs=t(),de=M("p"),de.innerHTML=$e,Se=t(),h(ne.$$.fragment),Ee=t(),ie=M("p"),ie.textContent=la,Fe=t(),h(re.$$.fragment),He=t(),h(oe.$$.fragment),os=t(),we=M("p"),we.innerHTML=_e,De=t(),h(pe.$$.fragment),ps=t(),h(ue.$$.fragment),Je=t(),Ve=M("p"),Ve.textContent=ms,Te=t(),ge=M("div"),ge.innerHTML=Ms,Ue=t(),je=M("p"),je.innerHTML=Re,Le=t(),h(me.$$.fragment),cs=t(),be=M("p"),be.textContent=i,b=t(),h(hs.$$.fragment),na=t(),ys=M("p"),ys.innerHTML=qt,ia=t(),ds=M("table"),ds.innerHTML=Kt,ra=t(),ws=M("p"),ws.textContent=Pt,oa=t(),us=M("p"),us.innerHTML=Ot,pa=t(),qe=M("div"),qe.innerHTML=en,ma=t(),Js=M("p"),Js.textContent=sn,Ma=t(),h(Ke.$$.fragment),ca=t(),h(Ts.$$.fragment),ha=t(),h(Us.$$.fragment),ya=t(),js=M("p"),js.innerHTML=ln,da=t(),h(bs.$$.fragment),wa=t(),h(Pe.$$.fragment),ua=t(),fs=M("p"),fs.textContent=an,Ja=t(),h(Is.$$.fragment),Ta=t(),h(gs.$$.fragment),Ua=t(),Cs=M("p"),Cs.innerHTML=tn,ja=t(),h(ks.$$.fragment),ba=t(),h(Zs.$$.fragment),fa=t(),vs=M("p"),vs.innerHTML=nn,Ia=t(),Gs=M("p"),Gs.innerHTML=rn,ga=t(),h(xs.$$.fragment),Ca=t(),Bs=M("p"),Bs.innerHTML=on,ka=t(),Ws=M("p"),Ws.innerHTML=pn,Za=t(),h($s.$$.fragment),va=t(),_s=M("p"),_s.textContent=mn,Ga=t(),h(Oe.$$.fragment),xa=t(),h(Vs.$$.fragment),Ba=t(),h(Rs.$$.fragment),Wa=t(),Ns=M("p"),Ns.textContent=Mn,$a=t(),As=M("p"),As.innerHTML=cn,_a=t(),h(Xs.$$.fragment),Va=t(),zs=M("p"),zs.innerHTML=hn,Ra=t(),h(es.$$.fragment),Na=t(),Qs=M("p"),Yt=ei(`For ROUGE, recall measures how much of the reference summary is captured by the generated one. If we are just comparing words, recall can be calculated according to the following formula:
`),Aa=new si(!1),Xa=t(),Ys=M("p"),St=ei(`For our simple example above, this formula gives a perfect recall of 6/6 = 1; i.e., all the words in the reference summary have been produced by the model. This may sound great, but imagine if our generated summary had been ‚ÄúI really really loved reading the Hunger Games all night‚Äù. This would also have perfect recall, but is arguably a worse summary since it is verbose. To deal with these scenarios we also compute the precision, which in the ROUGE context measures how much of the generated summary was relevant:
`),za=new si(!1),Qa=t(),Ss=M("p"),Ss.innerHTML=yn,Ya=t(),h(Es.$$.fragment),Sa=t(),Fs=M("p"),Fs.textContent=dn,Ea=t(),h(Hs.$$.fragment),Fa=t(),Ds=M("p"),Ds.innerHTML=wn,Ha=t(),h(Ls.$$.fragment),Da=t(),h(qs.$$.fragment),La=t(),Ks=M("p"),Ks.innerHTML=un,qa=t(),h(Ps.$$.fragment),Ka=t(),h(Os.$$.fragment),Pa=t(),el=M("p"),el.innerHTML=Jn,Oa=t(),h(ss.$$.fragment),et=t(),sl=M("p"),sl.textContent=Tn,st=t(),h(ll.$$.fragment),lt=t(),al=M("p"),al.innerHTML=Un,at=t(),h(tl.$$.fragment),tt=t(),nl=M("p"),nl.textContent=jn,nt=t(),h(il.$$.fragment),it=t(),rl=M("p"),rl.innerHTML=bn,rt=t(),h(ol.$$.fragment),ot=t(),h(pl.$$.fragment),pt=t(),ml=M("p"),ml.textContent=fn,mt=t(),h(Ml.$$.fragment),Mt=t(),cl=M("p"),cl.textContent=In,ct=t(),h(hl.$$.fragment),ht=t(),h(yl.$$.fragment),yt=t(),dl=M("p"),dl.innerHTML=gn,dt=t(),ke.c(),Fl=t(),h(ls.$$.fragment),wt=t(),wl=M("p"),wl.textContent=Cn,ut=t(),h(ul.$$.fragment),Jt=t(),Jl=M("p"),Jl.textContent=kn,Tt=t(),h(Tl.$$.fragment),Ut=t(),H&&H.c(),Hl=t(),Ul=M("p"),Ul.innerHTML=Zn,jt=t(),jl=M("p"),jl.innerHTML=vn,bt=t(),ve.c(),Dl=t(),bl=M("p"),bl.textContent=Gn,ft=t(),h(fl.$$.fragment),It=t(),Il=M("p"),Il.innerHTML=xn,gt=t(),h(gl.$$.fragment),Ct=t(),h(Cl.$$.fragment),kt=t(),kl=M("p"),kl.innerHTML=Bn,Zt=t(),xe.c(),Ll=t(),D&&D.c(),ql=t(),h(Zl.$$.fragment),vt=t(),vl=M("p"),vl.innerHTML=Wn,Gt=t(),h(Gl.$$.fragment),xt=t(),xl=M("p"),xl.textContent=$n,Bt=t(),h(Bl.$$.fragment),Wt=t(),Wl=M("p"),Wl.textContent=_n,$t=t(),h($l.$$.fragment),_t=t(),h(_l.$$.fragment),Vt=t(),Vl=M("p"),Vl.innerHTML=Vn,Rt=t(),h(Rl.$$.fragment),Nt=t(),h(Nl.$$.fragment),At=t(),Al=M("p"),Al.textContent=Rn,Xt=t(),Xl=M("p"),Xl.textContent=Nn,zt=t(),aa=M("p"),this.h()},l(e){const a=yi("svelte-u9bgzb",document.head);r=c(a,"META",{name:!0,content:!0}),a.forEach(s),j=n(e),o=c(e,"P",{}),ta(o).forEach(s),I=n(e),y(v.$$.fragment,e),C=n(e),y($.$$.fragment,e),f=n(e),R.l(e),G=n(e),z=c(e,"P",{"data-svelte-h":!0}),u(z)!=="svelte-vih9wa"&&(z.innerHTML=S),N=n(e),y(_.$$.fragment,e),B=n(e),g=c(e,"P",{"data-svelte-h":!0}),u(g)!=="svelte-1gh0rb6"&&(g.innerHTML=x),L=n(e),A=c(e,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),ta(A).forEach(s),K=n(e),X=c(e,"P",{"data-svelte-h":!0}),u(X)!=="svelte-15nukw4"&&(X.textContent=q),E=n(e),y(Y.$$.fragment,e),F=n(e),J=c(e,"P",{"data-svelte-h":!0}),u(J)!=="svelte-182urio"&&(J.innerHTML=V),Me=n(e),y(T.$$.fragment,e),W=n(e),y(P.$$.fragment,e),Ne=n(e),O=c(e,"P",{"data-svelte-h":!0}),u(O)!=="svelte-1fcazu2"&&(O.innerHTML=ea),Ae=n(e),y(ee.$$.fragment,e),ns=n(e),y(ce.$$.fragment,e),he=n(e),y(Ie.$$.fragment,e),Xe=n(e),se=c(e,"P",{"data-svelte-h":!0}),u(se)!=="svelte-hpyi6w"&&(se.innerHTML=is),ye=n(e),y(We.$$.fragment,e),ze=n(e),y(le.$$.fragment,e),Qe=n(e),ae=c(e,"P",{"data-svelte-h":!0}),u(ae)!=="svelte-bxczij"&&(ae.innerHTML=sa),Ye=n(e),y(te.$$.fragment,e),rs=n(e),de=c(e,"P",{"data-svelte-h":!0}),u(de)!=="svelte-724upw"&&(de.innerHTML=$e),Se=n(e),y(ne.$$.fragment,e),Ee=n(e),ie=c(e,"P",{"data-svelte-h":!0}),u(ie)!=="svelte-bvpqh"&&(ie.textContent=la),Fe=n(e),y(re.$$.fragment,e),He=n(e),y(oe.$$.fragment,e),os=n(e),we=c(e,"P",{"data-svelte-h":!0}),u(we)!=="svelte-19bhr1q"&&(we.innerHTML=_e),De=n(e),y(pe.$$.fragment,e),ps=n(e),y(ue.$$.fragment,e),Je=n(e),Ve=c(e,"P",{"data-svelte-h":!0}),u(Ve)!=="svelte-1rxhw56"&&(Ve.textContent=ms),Te=n(e),ge=c(e,"DIV",{class:!0,"data-svelte-h":!0}),u(ge)!=="svelte-1pwnbfd"&&(ge.innerHTML=Ms),Ue=n(e),je=c(e,"P",{"data-svelte-h":!0}),u(je)!=="svelte-y3cfja"&&(je.innerHTML=Re),Le=n(e),y(me.$$.fragment,e),cs=n(e),be=c(e,"P",{"data-svelte-h":!0}),u(be)!=="svelte-802ffu"&&(be.textContent=i),b=n(e),y(hs.$$.fragment,e),na=n(e),ys=c(e,"P",{"data-svelte-h":!0}),u(ys)!=="svelte-1unhc81"&&(ys.innerHTML=qt),ia=n(e),ds=c(e,"TABLE",{"data-svelte-h":!0}),u(ds)!=="svelte-1oone28"&&(ds.innerHTML=Kt),ra=n(e),ws=c(e,"P",{"data-svelte-h":!0}),u(ws)!=="svelte-qa3zel"&&(ws.textContent=Pt),oa=n(e),us=c(e,"P",{"data-svelte-h":!0}),u(us)!=="svelte-4rtqmb"&&(us.innerHTML=Ot),pa=n(e),qe=c(e,"DIV",{class:!0,"data-svelte-h":!0}),u(qe)!=="svelte-k7lnur"&&(qe.innerHTML=en),ma=n(e),Js=c(e,"P",{"data-svelte-h":!0}),u(Js)!=="svelte-1iimti4"&&(Js.textContent=sn),Ma=n(e),y(Ke.$$.fragment,e),ca=n(e),y(Ts.$$.fragment,e),ha=n(e),y(Us.$$.fragment,e),ya=n(e),js=c(e,"P",{"data-svelte-h":!0}),u(js)!=="svelte-165o15w"&&(js.innerHTML=ln),da=n(e),y(bs.$$.fragment,e),wa=n(e),y(Pe.$$.fragment,e),ua=n(e),fs=c(e,"P",{"data-svelte-h":!0}),u(fs)!=="svelte-1olylm2"&&(fs.textContent=an),Ja=n(e),y(Is.$$.fragment,e),Ta=n(e),y(gs.$$.fragment,e),Ua=n(e),Cs=c(e,"P",{"data-svelte-h":!0}),u(Cs)!=="svelte-10aknob"&&(Cs.innerHTML=tn),ja=n(e),y(ks.$$.fragment,e),ba=n(e),y(Zs.$$.fragment,e),fa=n(e),vs=c(e,"P",{"data-svelte-h":!0}),u(vs)!=="svelte-1k3p6bc"&&(vs.innerHTML=nn),Ia=n(e),Gs=c(e,"P",{"data-svelte-h":!0}),u(Gs)!=="svelte-1xp22s9"&&(Gs.innerHTML=rn),ga=n(e),y(xs.$$.fragment,e),Ca=n(e),Bs=c(e,"P",{"data-svelte-h":!0}),u(Bs)!=="svelte-cdpb6n"&&(Bs.innerHTML=on),ka=n(e),Ws=c(e,"P",{"data-svelte-h":!0}),u(Ws)!=="svelte-rm19fl"&&(Ws.innerHTML=pn),Za=n(e),y($s.$$.fragment,e),va=n(e),_s=c(e,"P",{"data-svelte-h":!0}),u(_s)!=="svelte-1bchku6"&&(_s.textContent=mn),Ga=n(e),y(Oe.$$.fragment,e),xa=n(e),y(Vs.$$.fragment,e),Ba=n(e),y(Rs.$$.fragment,e),Wa=n(e),Ns=c(e,"P",{"data-svelte-h":!0}),u(Ns)!=="svelte-ft2w74"&&(Ns.textContent=Mn),$a=n(e),As=c(e,"P",{"data-svelte-h":!0}),u(As)!=="svelte-ffia08"&&(As.innerHTML=cn),_a=n(e),y(Xs.$$.fragment,e),Va=n(e),zs=c(e,"P",{"data-svelte-h":!0}),u(zs)!=="svelte-1pifmxc"&&(zs.innerHTML=hn),Ra=n(e),y(es.$$.fragment,e),Na=n(e),Qs=c(e,"P",{});var Kl=ta(Qs);Yt=li(Kl,`For ROUGE, recall measures how much of the reference summary is captured by the generated one. If we are just comparing words, recall can be calculated according to the following formula:
`),Aa=ai(Kl,!1),Kl.forEach(s),Xa=n(e),Ys=c(e,"P",{});var El=ta(Ys);St=li(El,`For our simple example above, this formula gives a perfect recall of 6/6 = 1; i.e., all the words in the reference summary have been produced by the model. This may sound great, but imagine if our generated summary had been ‚ÄúI really really loved reading the Hunger Games all night‚Äù. This would also have perfect recall, but is arguably a worse summary since it is verbose. To deal with these scenarios we also compute the precision, which in the ROUGE context measures how much of the generated summary was relevant:
`),za=ai(El,!1),El.forEach(s),Qa=n(e),Ss=c(e,"P",{"data-svelte-h":!0}),u(Ss)!=="svelte-128ef20"&&(Ss.innerHTML=yn),Ya=n(e),y(Es.$$.fragment,e),Sa=n(e),Fs=c(e,"P",{"data-svelte-h":!0}),u(Fs)!=="svelte-1moj4p7"&&(Fs.textContent=dn),Ea=n(e),y(Hs.$$.fragment,e),Fa=n(e),Ds=c(e,"P",{"data-svelte-h":!0}),u(Ds)!=="svelte-1pm5m1"&&(Ds.innerHTML=wn),Ha=n(e),y(Ls.$$.fragment,e),Da=n(e),y(qs.$$.fragment,e),La=n(e),Ks=c(e,"P",{"data-svelte-h":!0}),u(Ks)!=="svelte-vxjqyc"&&(Ks.innerHTML=un),qa=n(e),y(Ps.$$.fragment,e),Ka=n(e),y(Os.$$.fragment,e),Pa=n(e),el=c(e,"P",{"data-svelte-h":!0}),u(el)!=="svelte-tfvibj"&&(el.innerHTML=Jn),Oa=n(e),y(ss.$$.fragment,e),et=n(e),sl=c(e,"P",{"data-svelte-h":!0}),u(sl)!=="svelte-i8acz1"&&(sl.textContent=Tn),st=n(e),y(ll.$$.fragment,e),lt=n(e),al=c(e,"P",{"data-svelte-h":!0}),u(al)!=="svelte-1cuk3qa"&&(al.innerHTML=Un),at=n(e),y(tl.$$.fragment,e),tt=n(e),nl=c(e,"P",{"data-svelte-h":!0}),u(nl)!=="svelte-1phjm6q"&&(nl.textContent=jn),nt=n(e),y(il.$$.fragment,e),it=n(e),rl=c(e,"P",{"data-svelte-h":!0}),u(rl)!=="svelte-18wyy6x"&&(rl.innerHTML=bn),rt=n(e),y(ol.$$.fragment,e),ot=n(e),y(pl.$$.fragment,e),pt=n(e),ml=c(e,"P",{"data-svelte-h":!0}),u(ml)!=="svelte-1isqbeo"&&(ml.textContent=fn),mt=n(e),y(Ml.$$.fragment,e),Mt=n(e),cl=c(e,"P",{"data-svelte-h":!0}),u(cl)!=="svelte-65wxwj"&&(cl.textContent=In),ct=n(e),y(hl.$$.fragment,e),ht=n(e),y(yl.$$.fragment,e),yt=n(e),dl=c(e,"P",{"data-svelte-h":!0}),u(dl)!=="svelte-154pk5t"&&(dl.innerHTML=gn),dt=n(e),ke.l(e),Fl=n(e),y(ls.$$.fragment,e),wt=n(e),wl=c(e,"P",{"data-svelte-h":!0}),u(wl)!=="svelte-11h147n"&&(wl.textContent=Cn),ut=n(e),y(ul.$$.fragment,e),Jt=n(e),Jl=c(e,"P",{"data-svelte-h":!0}),u(Jl)!=="svelte-8p0cke"&&(Jl.textContent=kn),Tt=n(e),y(Tl.$$.fragment,e),Ut=n(e),H&&H.l(e),Hl=n(e),Ul=c(e,"P",{"data-svelte-h":!0}),u(Ul)!=="svelte-1nhn5pq"&&(Ul.innerHTML=Zn),jt=n(e),jl=c(e,"P",{"data-svelte-h":!0}),u(jl)!=="svelte-1yx0z91"&&(jl.innerHTML=vn),bt=n(e),ve.l(e),Dl=n(e),bl=c(e,"P",{"data-svelte-h":!0}),u(bl)!=="svelte-1dta7o8"&&(bl.textContent=Gn),ft=n(e),y(fl.$$.fragment,e),It=n(e),Il=c(e,"P",{"data-svelte-h":!0}),u(Il)!=="svelte-xaeqdc"&&(Il.innerHTML=xn),gt=n(e),y(gl.$$.fragment,e),Ct=n(e),y(Cl.$$.fragment,e),kt=n(e),kl=c(e,"P",{"data-svelte-h":!0}),u(kl)!=="svelte-lmn2nq"&&(kl.innerHTML=Bn),Zt=n(e),xe.l(e),Ll=n(e),D&&D.l(e),ql=n(e),y(Zl.$$.fragment,e),vt=n(e),vl=c(e,"P",{"data-svelte-h":!0}),u(vl)!=="svelte-kygi7d"&&(vl.innerHTML=Wn),Gt=n(e),y(Gl.$$.fragment,e),xt=n(e),xl=c(e,"P",{"data-svelte-h":!0}),u(xl)!=="svelte-nht6a4"&&(xl.textContent=$n),Bt=n(e),y(Bl.$$.fragment,e),Wt=n(e),Wl=c(e,"P",{"data-svelte-h":!0}),u(Wl)!=="svelte-1wlcxao"&&(Wl.textContent=_n),$t=n(e),y($l.$$.fragment,e),_t=n(e),y(_l.$$.fragment,e),Vt=n(e),Vl=c(e,"P",{"data-svelte-h":!0}),u(Vl)!=="svelte-4jjf7d"&&(Vl.innerHTML=Vn),Rt=n(e),y(Rl.$$.fragment,e),Nt=n(e),y(Nl.$$.fragment,e),At=n(e),Al=c(e,"P",{"data-svelte-h":!0}),u(Al)!=="svelte-1p3b7rw"&&(Al.textContent=Rn),Xt=n(e),Xl=c(e,"P",{"data-svelte-h":!0}),u(Xl)!=="svelte-1gun2uf"&&(Xl.textContent=Nn),zt=n(e),aa=c(e,"P",{}),ta(aa).forEach(s),this.h()},h(){Be(r,"name","hf:doc:metadata"),Be(r,"content",Wi),mi(A.src,Q="https://course-demos-mt5-small-finetuned-amazon-en-es.hf.space")||Be(A,"src",Q),Be(A,"frameborder","0"),Be(A,"height","400"),Be(A,"title","Gradio app"),Be(A,"class","block dark:hidden container p-0 flex-grow space-iframe"),Be(A,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),Be(A,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),Be(ge,"class","flex justify-center"),Be(qe,"class","flex justify-center"),Aa.a=null,za.a=null},m(e,a){Dt(document.head,r),l(e,j,a),l(e,o,a),l(e,I,a),d(v,e,a),l(e,C,a),d($,e,a),l(e,f,a),zl[k].m(e,a),l(e,G,a),l(e,z,a),l(e,N,a),d(_,e,a),l(e,B,a),l(e,g,a),l(e,L,a),l(e,A,a),l(e,K,a),l(e,X,a),l(e,E,a),d(Y,e,a),l(e,F,a),l(e,J,a),l(e,Me,a),d(T,e,a),l(e,W,a),d(P,e,a),l(e,Ne,a),l(e,O,a),l(e,Ae,a),d(ee,e,a),l(e,ns,a),d(ce,e,a),l(e,he,a),d(Ie,e,a),l(e,Xe,a),l(e,se,a),l(e,ye,a),d(We,e,a),l(e,ze,a),d(le,e,a),l(e,Qe,a),l(e,ae,a),l(e,Ye,a),d(te,e,a),l(e,rs,a),l(e,de,a),l(e,Se,a),d(ne,e,a),l(e,Ee,a),l(e,ie,a),l(e,Fe,a),d(re,e,a),l(e,He,a),d(oe,e,a),l(e,os,a),l(e,we,a),l(e,De,a),d(pe,e,a),l(e,ps,a),d(ue,e,a),l(e,Je,a),l(e,Ve,a),l(e,Te,a),l(e,ge,a),l(e,Ue,a),l(e,je,a),l(e,Le,a),d(me,e,a),l(e,cs,a),l(e,be,a),l(e,b,a),d(hs,e,a),l(e,na,a),l(e,ys,a),l(e,ia,a),l(e,ds,a),l(e,ra,a),l(e,ws,a),l(e,oa,a),l(e,us,a),l(e,pa,a),l(e,qe,a),l(e,ma,a),l(e,Js,a),l(e,Ma,a),d(Ke,e,a),l(e,ca,a),d(Ts,e,a),l(e,ha,a),d(Us,e,a),l(e,ya,a),l(e,js,a),l(e,da,a),d(bs,e,a),l(e,wa,a),d(Pe,e,a),l(e,ua,a),l(e,fs,a),l(e,Ja,a),d(Is,e,a),l(e,Ta,a),d(gs,e,a),l(e,Ua,a),l(e,Cs,a),l(e,ja,a),d(ks,e,a),l(e,ba,a),d(Zs,e,a),l(e,fa,a),l(e,vs,a),l(e,Ia,a),l(e,Gs,a),l(e,ga,a),d(xs,e,a),l(e,Ca,a),l(e,Bs,a),l(e,ka,a),l(e,Ws,a),l(e,Za,a),d($s,e,a),l(e,va,a),l(e,_s,a),l(e,Ga,a),d(Oe,e,a),l(e,xa,a),d(Vs,e,a),l(e,Ba,a),d(Rs,e,a),l(e,Wa,a),l(e,Ns,a),l(e,$a,a),l(e,As,a),l(e,_a,a),d(Xs,e,a),l(e,Va,a),l(e,zs,a),l(e,Ra,a),d(es,e,a),l(e,Na,a),l(e,Qs,a),Dt(Qs,Yt),Aa.m(ri,Qs),l(e,Xa,a),l(e,Ys,a),Dt(Ys,St),za.m(oi,Ys),l(e,Qa,a),l(e,Ss,a),l(e,Ya,a),d(Es,e,a),l(e,Sa,a),l(e,Fs,a),l(e,Ea,a),d(Hs,e,a),l(e,Fa,a),l(e,Ds,a),l(e,Ha,a),d(Ls,e,a),l(e,Da,a),d(qs,e,a),l(e,La,a),l(e,Ks,a),l(e,qa,a),d(Ps,e,a),l(e,Ka,a),d(Os,e,a),l(e,Pa,a),l(e,el,a),l(e,Oa,a),d(ss,e,a),l(e,et,a),l(e,sl,a),l(e,st,a),d(ll,e,a),l(e,lt,a),l(e,al,a),l(e,at,a),d(tl,e,a),l(e,tt,a),l(e,nl,a),l(e,nt,a),d(il,e,a),l(e,it,a),l(e,rl,a),l(e,rt,a),d(ol,e,a),l(e,ot,a),d(pl,e,a),l(e,pt,a),l(e,ml,a),l(e,mt,a),d(Ml,e,a),l(e,Mt,a),l(e,cl,a),l(e,ct,a),d(hl,e,a),l(e,ht,a),d(yl,e,a),l(e,yt,a),l(e,dl,a),l(e,dt,a),Ql[Ce].m(e,a),l(e,Fl,a),d(ls,e,a),l(e,wt,a),l(e,wl,a),l(e,ut,a),d(ul,e,a),l(e,Jt,a),l(e,Jl,a),l(e,Tt,a),d(Tl,e,a),l(e,Ut,a),H&&H.m(e,a),l(e,Hl,a),l(e,Ul,a),l(e,jt,a),l(e,jl,a),l(e,bt,a),Yl[Ze].m(e,a),l(e,Dl,a),l(e,bl,a),l(e,ft,a),d(fl,e,a),l(e,It,a),l(e,Il,a),l(e,gt,a),d(gl,e,a),l(e,Ct,a),d(Cl,e,a),l(e,kt,a),l(e,kl,a),l(e,Zt,a),Sl[Ge].m(e,a),l(e,Ll,a),D&&D.m(e,a),l(e,ql,a),d(Zl,e,a),l(e,vt,a),l(e,vl,a),l(e,Gt,a),d(Gl,e,a),l(e,xt,a),l(e,xl,a),l(e,Bt,a),d(Bl,e,a),l(e,Wt,a),l(e,Wl,a),l(e,$t,a),d($l,e,a),l(e,_t,a),d(_l,e,a),l(e,Vt,a),l(e,Vl,a),l(e,Rt,a),d(Rl,e,a),l(e,Nt,a),d(Nl,e,a),l(e,At,a),l(e,Al,a),l(e,Xt,a),l(e,Xl,a),l(e,zt,a),l(e,aa,a),Qt=!0},p(e,[a]){const Kl={};a&1&&(Kl.fw=e[0]),v.$set(Kl);let El=k;k=Xn(e),k!==El&&(Ol(),m(zl[El],1,1,()=>{zl[El]=null}),Pl(),R=zl[k],R||(R=zl[k]=An[k](e),R.c()),p(R,1),R.m(G.parentNode,G));const Hn={};a&2&&(Hn.$$scope={dirty:a,ctx:e}),Ie.$set(Hn);const Dn={};a&2&&(Dn.$$scope={dirty:a,ctx:e}),Ke.$set(Dn);const Ln={};a&2&&(Ln.$$scope={dirty:a,ctx:e}),Pe.$set(Ln);const qn={};a&2&&(qn.$$scope={dirty:a,ctx:e}),Oe.$set(qn);const Kn={};a&2&&(Kn.$$scope={dirty:a,ctx:e}),es.$set(Kn);const Pn={};a&2&&(Pn.$$scope={dirty:a,ctx:e}),ss.$set(Pn);let Et=Ce;Ce=Qn(e),Ce!==Et&&(Ol(),m(Ql[Et],1,1,()=>{Ql[Et]=null}),Pl(),ke=Ql[Ce],ke||(ke=Ql[Ce]=zn[Ce](e),ke.c()),p(ke,1),ke.m(Fl.parentNode,Fl));const On={};a&2&&(On.$$scope={dirty:a,ctx:e}),ls.$set(On),e[0]==="pt"?H?a&1&&p(H,1):(H=ti(),H.c(),p(H,1),H.m(Hl.parentNode,Hl)):H&&(Ol(),m(H,1,1,()=>{H=null}),Pl());let Ft=Ze;Ze=Sn(e),Ze!==Ft&&(Ol(),m(Yl[Ft],1,1,()=>{Yl[Ft]=null}),Pl(),ve=Yl[Ze],ve||(ve=Yl[Ze]=Yn[Ze](e),ve.c()),p(ve,1),ve.m(Dl.parentNode,Dl));let Ht=Ge;Ge=Fn(e),Ge!==Ht&&(Ol(),m(Sl[Ht],1,1,()=>{Sl[Ht]=null}),Pl(),xe=Sl[Ge],xe||(xe=Sl[Ge]=En[Ge](e),xe.c()),p(xe,1),xe.m(Ll.parentNode,Ll)),e[0]==="pt"?D?a&1&&p(D,1):(D=ni(e),D.c(),p(D,1),D.m(ql.parentNode,ql)):D&&(Ol(),m(D,1,1,()=>{D=null}),Pl())},i(e){Qt||(p(v.$$.fragment,e),p($.$$.fragment,e),p(R),p(_.$$.fragment,e),p(Y.$$.fragment,e),p(T.$$.fragment,e),p(P.$$.fragment,e),p(ee.$$.fragment,e),p(ce.$$.fragment,e),p(Ie.$$.fragment,e),p(We.$$.fragment,e),p(le.$$.fragment,e),p(te.$$.fragment,e),p(ne.$$.fragment,e),p(re.$$.fragment,e),p(oe.$$.fragment,e),p(pe.$$.fragment,e),p(ue.$$.fragment,e),p(me.$$.fragment,e),p(hs.$$.fragment,e),p(Ke.$$.fragment,e),p(Ts.$$.fragment,e),p(Us.$$.fragment,e),p(bs.$$.fragment,e),p(Pe.$$.fragment,e),p(Is.$$.fragment,e),p(gs.$$.fragment,e),p(ks.$$.fragment,e),p(Zs.$$.fragment,e),p(xs.$$.fragment,e),p($s.$$.fragment,e),p(Oe.$$.fragment,e),p(Vs.$$.fragment,e),p(Rs.$$.fragment,e),p(Xs.$$.fragment,e),p(es.$$.fragment,e),p(Es.$$.fragment,e),p(Hs.$$.fragment,e),p(Ls.$$.fragment,e),p(qs.$$.fragment,e),p(Ps.$$.fragment,e),p(Os.$$.fragment,e),p(ss.$$.fragment,e),p(ll.$$.fragment,e),p(tl.$$.fragment,e),p(il.$$.fragment,e),p(ol.$$.fragment,e),p(pl.$$.fragment,e),p(Ml.$$.fragment,e),p(hl.$$.fragment,e),p(yl.$$.fragment,e),p(ke),p(ls.$$.fragment,e),p(ul.$$.fragment,e),p(Tl.$$.fragment,e),p(H),p(ve),p(fl.$$.fragment,e),p(gl.$$.fragment,e),p(Cl.$$.fragment,e),p(xe),p(D),p(Zl.$$.fragment,e),p(Gl.$$.fragment,e),p(Bl.$$.fragment,e),p($l.$$.fragment,e),p(_l.$$.fragment,e),p(Rl.$$.fragment,e),p(Nl.$$.fragment,e),Qt=!0)},o(e){m(v.$$.fragment,e),m($.$$.fragment,e),m(R),m(_.$$.fragment,e),m(Y.$$.fragment,e),m(T.$$.fragment,e),m(P.$$.fragment,e),m(ee.$$.fragment,e),m(ce.$$.fragment,e),m(Ie.$$.fragment,e),m(We.$$.fragment,e),m(le.$$.fragment,e),m(te.$$.fragment,e),m(ne.$$.fragment,e),m(re.$$.fragment,e),m(oe.$$.fragment,e),m(pe.$$.fragment,e),m(ue.$$.fragment,e),m(me.$$.fragment,e),m(hs.$$.fragment,e),m(Ke.$$.fragment,e),m(Ts.$$.fragment,e),m(Us.$$.fragment,e),m(bs.$$.fragment,e),m(Pe.$$.fragment,e),m(Is.$$.fragment,e),m(gs.$$.fragment,e),m(ks.$$.fragment,e),m(Zs.$$.fragment,e),m(xs.$$.fragment,e),m($s.$$.fragment,e),m(Oe.$$.fragment,e),m(Vs.$$.fragment,e),m(Rs.$$.fragment,e),m(Xs.$$.fragment,e),m(es.$$.fragment,e),m(Es.$$.fragment,e),m(Hs.$$.fragment,e),m(Ls.$$.fragment,e),m(qs.$$.fragment,e),m(Ps.$$.fragment,e),m(Os.$$.fragment,e),m(ss.$$.fragment,e),m(ll.$$.fragment,e),m(tl.$$.fragment,e),m(il.$$.fragment,e),m(ol.$$.fragment,e),m(pl.$$.fragment,e),m(Ml.$$.fragment,e),m(hl.$$.fragment,e),m(yl.$$.fragment,e),m(ke),m(ls.$$.fragment,e),m(ul.$$.fragment,e),m(Tl.$$.fragment,e),m(H),m(ve),m(fl.$$.fragment,e),m(gl.$$.fragment,e),m(Cl.$$.fragment,e),m(xe),m(D),m(Zl.$$.fragment,e),m(Gl.$$.fragment,e),m(Bl.$$.fragment,e),m($l.$$.fragment,e),m(_l.$$.fragment,e),m(Rl.$$.fragment,e),m(Nl.$$.fragment,e),Qt=!1},d(e){e&&(s(j),s(o),s(I),s(C),s(f),s(G),s(z),s(N),s(B),s(g),s(L),s(A),s(K),s(X),s(E),s(F),s(J),s(Me),s(W),s(Ne),s(O),s(Ae),s(ns),s(he),s(Xe),s(se),s(ye),s(ze),s(Qe),s(ae),s(Ye),s(rs),s(de),s(Se),s(Ee),s(ie),s(Fe),s(He),s(os),s(we),s(De),s(ps),s(Je),s(Ve),s(Te),s(ge),s(Ue),s(je),s(Le),s(cs),s(be),s(b),s(na),s(ys),s(ia),s(ds),s(ra),s(ws),s(oa),s(us),s(pa),s(qe),s(ma),s(Js),s(Ma),s(ca),s(ha),s(ya),s(js),s(da),s(wa),s(ua),s(fs),s(Ja),s(Ta),s(Ua),s(Cs),s(ja),s(ba),s(fa),s(vs),s(Ia),s(Gs),s(ga),s(Ca),s(Bs),s(ka),s(Ws),s(Za),s(va),s(_s),s(Ga),s(xa),s(Ba),s(Wa),s(Ns),s($a),s(As),s(_a),s(Va),s(zs),s(Ra),s(Na),s(Qs),s(Xa),s(Ys),s(Qa),s(Ss),s(Ya),s(Sa),s(Fs),s(Ea),s(Fa),s(Ds),s(Ha),s(Da),s(La),s(Ks),s(qa),s(Ka),s(Pa),s(el),s(Oa),s(et),s(sl),s(st),s(lt),s(al),s(at),s(tt),s(nl),s(nt),s(it),s(rl),s(rt),s(ot),s(pt),s(ml),s(mt),s(Mt),s(cl),s(ct),s(ht),s(yt),s(dl),s(dt),s(Fl),s(wt),s(wl),s(ut),s(Jt),s(Jl),s(Tt),s(Ut),s(Hl),s(Ul),s(jt),s(jl),s(bt),s(Dl),s(bl),s(ft),s(It),s(Il),s(gt),s(Ct),s(kt),s(kl),s(Zt),s(Ll),s(ql),s(vt),s(vl),s(Gt),s(xt),s(xl),s(Bt),s(Wt),s(Wl),s($t),s(_t),s(Vt),s(Vl),s(Rt),s(Nt),s(At),s(Al),s(Xt),s(Xl),s(zt),s(aa)),s(r),w(v,e),w($,e),zl[k].d(e),w(_,e),w(Y,e),w(T,e),w(P,e),w(ee,e),w(ce,e),w(Ie,e),w(We,e),w(le,e),w(te,e),w(ne,e),w(re,e),w(oe,e),w(pe,e),w(ue,e),w(me,e),w(hs,e),w(Ke,e),w(Ts,e),w(Us,e),w(bs,e),w(Pe,e),w(Is,e),w(gs,e),w(ks,e),w(Zs,e),w(xs,e),w($s,e),w(Oe,e),w(Vs,e),w(Rs,e),w(Xs,e),w(es,e),w(Es,e),w(Hs,e),w(Ls,e),w(qs,e),w(Ps,e),w(Os,e),w(ss,e),w(ll,e),w(tl,e),w(il,e),w(ol,e),w(pl,e),w(Ml,e),w(hl,e),w(yl,e),Ql[Ce].d(e),w(ls,e),w(ul,e),w(Tl,e),H&&H.d(e),Yl[Ze].d(e),w(fl,e),w(gl,e),w(Cl,e),Sl[Ge].d(e),D&&D.d(e),w(Zl,e),w(Gl,e),w(Bl,e),w($l,e),w(_l,e),w(Rl,e),w(Nl,e)}}}const Wi='{"title":"Summarization","local":"summarization","sections":[{"title":"Preparing a multilingual corpus","local":"preparing-a-multilingual-corpus","sections":[],"depth":2},{"title":"Models for text summarization","local":"models-for-text-summarization","sections":[],"depth":2},{"title":"Preprocessing the data","local":"preprocessing-the-data","sections":[],"depth":2},{"title":"Metrics for text summarization","local":"metrics-for-text-summarization","sections":[{"title":"Creating a strong baseline","local":"creating-a-strong-baseline","sections":[],"depth":3}],"depth":2},{"title":"Fine-tuning mT5 with the Trainer API","local":"fine-tuning-mt5-with-the-trainer-api","sections":[],"depth":2},{"title":"Fine-tuning mT5 with Keras","local":"fine-tuning-mt5-with-keras","sections":[],"depth":2},{"title":"Fine-tuning mT5 with ü§ó Accelerate","local":"fine-tuning-mt5-with-accelerate","sections":[{"title":"Preparing everything for training","local":"preparing-everything-for-training","sections":[],"depth":3},{"title":"Training loop","local":"training-loop","sections":[],"depth":3}],"depth":2},{"title":"Using your fine-tuned model","local":"using-your-fine-tuned-model","sections":[],"depth":2}],"depth":1}';function $i(Z,r,j){let o="pt";return Mi(()=>{const I=new URLSearchParams(window.location.search);j(0,o=I.get("fw")||"pt")}),[o]}class Yi extends ci{constructor(r){super(),hi(this,r,$i,Bi,pi,{})}}export{Yi as component};
