import{s as ee,o as te}from"../chunks/scheduler.37c15a92.js";import{S as ae,i as ne,g as j,s as r,r as c,A as se,h as G,f as n,c as h,j as V,u as f,x as oe,k as Z,y as ie,a as i,v as m,t as l,b as re,d,w as p,p as he}from"../chunks/index.2bf4358c.js";import{C as le}from"../chunks/CourseFloatingBanner.15ba07e6.js";import{Q as L}from"../chunks/Question.668688bc.js";import{F as de}from"../chunks/FrameworkSwitchCourse.8d4d4ab6.js";import{H as A}from"../chunks/Heading.8ada512a.js";function ce(B){let u,T,$,k,g,_,x,v,b,q,y,C;return u=new A({props:{title:"4. What happens when you instantiate one of the TFAutoModelForXxx classes with a pretrained language model (such as bert-base-uncased ) that corresponds to a different task than the one for which it was trained?",local:"4-what-happens-when-you-instantiate-one-of-the-tfautomodelforxxx-classes-with-a-pretrained-language-model-such-as-bert-base-uncased--that-corresponds-to-a-different-task-than-the-one-for-which-it-was-trained",headingTag:"h3"}}),$=new L({props:{choices:[{text:"Nothing, but you get a warning.",explain:"You do get a warning, but that's not all!"},{text:"The head of the pretrained model is discarded and a new head suitable for the task is inserted instead.",explain:"Correct. For example, when we used <code>TFAutoModelForSequenceClassification</code> with <code>bert-base-uncased</code>, we got warnings when instantiating the model. The pretrained head is not used for the sequence classification task, so it's discarded and a new head is instantiated with random weights.",correct:!0},{text:"The head of the pretrained model is discarded.",explain:"Something else needs to happen. Try again!"},{text:"Nothing, since the model can still be fine-tuned for the different task.",explain:"The head of the pretrained model was not trained to solve this task, so we should discard the head!"}]}}),g=new A({props:{title:"5. The TensorFlow models from transformers are already Keras models. What benefit does this offer?",local:"5-the-tensorflow-models-from-transformers-are-already-keras-models-what-benefit-does-this-offer",headingTag:"h3"}}),x=new L({props:{choices:[{text:"The models work on a TPU out of the box.",explain:"Almost! There are some small additional changes required. For example, you need to run everything in a <code>TPUStrategy</code> scope, including the initialization of the model."},{text:"You can leverage existing methods such as <code>compile()</code>, <code>fit()</code>, and <code>predict()</code>.",explain:"Correct! Once you have the data, training on it requires very little work.",correct:!0},{text:"You get to learn Keras as well as transformers.",explain:"Correct, but we're looking for something else :)",correct:!0},{text:"You can easily compute metrics related to the dataset.",explain:"Keras helps us with training and evaluating the model, not computing dataset-related metrics."}]}}),b=new A({props:{title:"6. How can you define your own custom metric?",local:"6-how-can-you-define-your-own-custom-metric",headingTag:"h3"}}),y=new L({props:{choices:[{text:"By subclassing <code>tf.keras.metrics.Metric</code>.",explain:"Great!",correct:!0},{text:"Using the Keras functional API.",explain:"Try again!"},{text:"By using a callable with signature <code>metric_fn(y_true, y_pred)</code>.",explain:"Correct!",correct:!0},{text:"By Googling it.",explain:"That's not the answer we're looking for, but it should help you find it.",correct:!0}]}}),{c(){c(u.$$.fragment),T=r(),c($.$$.fragment),k=r(),c(g.$$.fragment),_=r(),c(x.$$.fragment),v=r(),c(b.$$.fragment),q=r(),c(y.$$.fragment)},l(a){f(u.$$.fragment,a),T=h(a),f($.$$.fragment,a),k=h(a),f(g.$$.fragment,a),_=h(a),f(x.$$.fragment,a),v=h(a),f(b.$$.fragment,a),q=h(a),f(y.$$.fragment,a)},m(a,w){m(u,a,w),i(a,T,w),m($,a,w),i(a,k,w),m(g,a,w),i(a,_,w),m(x,a,w),i(a,v,w),m(b,a,w),i(a,q,w),m(y,a,w),C=!0},i(a){C||(d(u.$$.fragment,a),d($.$$.fragment,a),d(g.$$.fragment,a),d(x.$$.fragment,a),d(b.$$.fragment,a),d(y.$$.fragment,a),C=!0)},o(a){l(u.$$.fragment,a),l($.$$.fragment,a),l(g.$$.fragment,a),l(x.$$.fragment,a),l(b.$$.fragment,a),l(y.$$.fragment,a),C=!1},d(a){a&&(n(T),n(k),n(_),n(v),n(q)),p(u,a),p($,a),p(g,a),p(x,a),p(b,a),p(y,a)}}}function fe(B){let u,T,$,k,g,_,x,v,b,q,y,C,a,w,S,I,W,P,F,E,z,H,M,N;return u=new A({props:{title:"4. What are the benefits of the Dataset.map() method?",local:"4-what-are-the-benefits-of-the-datasetmap-method",headingTag:"h3"}}),$=new L({props:{choices:[{text:"The results of the function are cached, so it won't take any time if we re-execute the code.",explain:"That is indeed one of the neat benefits of this method! It's not the only one, though...",correct:!0},{text:"It can apply multiprocessing to go faster than applying the function on each element of the dataset.",explain:"This is a neat feature of this method, but it's not the only one!",correct:!0},{text:"It does not load the whole dataset into memory, saving the results as soon as one element is processed.",explain:"That's one advantage of this method. There are others, though!",correct:!0}]}}),g=new A({props:{title:"5. What does dynamic padding mean?",local:"5-what-does-dynamic-padding-mean",headingTag:"h3"}}),x=new L({props:{choices:[{text:"It's when you pad the inputs for each batch to the maximum length in the whole dataset.",explain:"It does imply padding when creating the batch, but not to the maximum length in the whole dataset."},{text:"It's when you pad your inputs when the batch is created, to the maximum length of the sentences inside that batch.",explain:`That's correct! The "dynamic" part comes from the fact that the size of each batch is determined at the time of creation, and all your batches might have different shapes as a result.`,correct:!0},{text:"It's when you pad your inputs so that each sentence has the same number of tokens as the previous one in the dataset.",explain:"That's incorrect, plus it doesn't make sense to look at the order in the dataset since we shuffle it during training."}]}}),b=new A({props:{title:"6. What is the purpose of a collate function?",local:"6-what-is-the-purpose-of-a-collate-function",headingTag:"h3"}}),y=new L({props:{choices:[{text:"It ensures all the sequences in the dataset have the same length.",explain:"A collate function is involved in handling individual batches, not the whole dataset. Additionally, we're talking about generic collate functions, not <code>DataCollatorWithPadding</code> specifically."},{text:"It puts together all the samples in a batch.",explain:"Correct! You can pass the collate function as an argument of a <code>DataLoader</code>. We used the <code>DataCollatorWithPadding</code> function, which pads all items in a batch so they have the same length.",correct:!0},{text:"It preprocesses the whole dataset.",explain:"That would be a preprocessing function, not a collate function."},{text:"It truncates the sequences in the dataset.",explain:"A collate function is involved in handling individual batches, not the whole dataset. If you're interested in truncating, you can use the <code>truncate</code> argument of <code>tokenizer</code>."}]}}),a=new A({props:{title:"7. What happens when you instantiate one of the AutoModelForXxx classes with a pretrained language model (such as bert-base-uncased ) that corresponds to a different task than the one for which it was trained?",local:"7-what-happens-when-you-instantiate-one-of-the-automodelforxxx-classes-with-a-pretrained-language-model-such-as-bert-base-uncased--that-corresponds-to-a-different-task-than-the-one-for-which-it-was-trained",headingTag:"h3"}}),S=new L({props:{choices:[{text:"Nothing, but you get a warning.",explain:"You do get a warning, but that's not all!"},{text:"The head of the pretrained model is discarded and a new head suitable for the task is inserted instead.",explain:"Correct. For example, when we used <code>AutoModelForSequenceClassification</code> with <code>bert-base-uncased</code>, we got warnings when instantiating the model. The pretrained head is not used for the sequence classification task, so it's discarded and a new head is instantiated with random weights.",correct:!0},{text:"The head of the pretrained model is discarded.",explain:"Something else needs to happen. Try again!"},{text:"Nothing, since the model can still be fine-tuned for the different task.",explain:"The head of the pretrained model was not trained to solve this task, so we should discard the head!"}]}}),W=new A({props:{title:"8. Whatâ€™s the purpose of TrainingArguments ?",local:"8-whats-the-purpose-of-trainingarguments-",headingTag:"h3"}}),F=new L({props:{choices:[{text:"It contains all the hyperparameters used for training and evaluation with the <code>Trainer</code>.",explain:"Correct!",correct:!0},{text:"It specifies the size of the model.",explain:"The model size is defined by the model configuration, not the class <code>TrainingArguments</code>."},{text:"It just contains the hyperparameters used for evaluation.",explain:"In the example, we specified where the model and its checkpoints will be saved. Try again!"},{text:"It just contains the hyperparameters used for training.",explain:"In the example, we used an <code>evaluation_strategy</code> as well, so this impacts evaluation. Try again!"}]}}),z=new A({props:{title:"9. Why should you use the ðŸ¤— Accelerate library?",local:"9-why-should-you-use-the--accelerate-library",headingTag:"h3"}}),M=new L({props:{choices:[{text:"It provides access to faster models.",explain:"No, the ðŸ¤— Accelerate library does not provide any models."},{text:"It provides a high-level API so I don't have to implement my own training loop.",explain:"This is what we did with <code>Trainer</code>, not the ðŸ¤— Accelerate library. Try again!"},{text:"It makes our training loops work on distributed strategies.",explain:"Correct! With ðŸ¤— Accelerate, your training loops will work for multiple GPUs and TPUs.",correct:!0},{text:"It provides more optimization functions.",explain:"No, the ðŸ¤— Accelerate library does not provide any optimization functions."}]}}),{c(){c(u.$$.fragment),T=r(),c($.$$.fragment),k=r(),c(g.$$.fragment),_=r(),c(x.$$.fragment),v=r(),c(b.$$.fragment),q=r(),c(y.$$.fragment),C=r(),c(a.$$.fragment),w=r(),c(S.$$.fragment),I=r(),c(W.$$.fragment),P=r(),c(F.$$.fragment),E=r(),c(z.$$.fragment),H=r(),c(M.$$.fragment)},l(e){f(u.$$.fragment,e),T=h(e),f($.$$.fragment,e),k=h(e),f(g.$$.fragment,e),_=h(e),f(x.$$.fragment,e),v=h(e),f(b.$$.fragment,e),q=h(e),f(y.$$.fragment,e),C=h(e),f(a.$$.fragment,e),w=h(e),f(S.$$.fragment,e),I=h(e),f(W.$$.fragment,e),P=h(e),f(F.$$.fragment,e),E=h(e),f(z.$$.fragment,e),H=h(e),f(M.$$.fragment,e)},m(e,s){m(u,e,s),i(e,T,s),m($,e,s),i(e,k,s),m(g,e,s),i(e,_,s),m(x,e,s),i(e,v,s),m(b,e,s),i(e,q,s),m(y,e,s),i(e,C,s),m(a,e,s),i(e,w,s),m(S,e,s),i(e,I,s),m(W,e,s),i(e,P,s),m(F,e,s),i(e,E,s),m(z,e,s),i(e,H,s),m(M,e,s),N=!0},i(e){N||(d(u.$$.fragment,e),d($.$$.fragment,e),d(g.$$.fragment,e),d(x.$$.fragment,e),d(b.$$.fragment,e),d(y.$$.fragment,e),d(a.$$.fragment,e),d(S.$$.fragment,e),d(W.$$.fragment,e),d(F.$$.fragment,e),d(z.$$.fragment,e),d(M.$$.fragment,e),N=!0)},o(e){l(u.$$.fragment,e),l($.$$.fragment,e),l(g.$$.fragment,e),l(x.$$.fragment,e),l(b.$$.fragment,e),l(y.$$.fragment,e),l(a.$$.fragment,e),l(S.$$.fragment,e),l(W.$$.fragment,e),l(F.$$.fragment,e),l(z.$$.fragment,e),l(M.$$.fragment,e),N=!1},d(e){e&&(n(T),n(k),n(_),n(v),n(q),n(C),n(w),n(I),n(P),n(E),n(H)),p(u,e),p($,e),p(g,e),p(x,e),p(b,e),p(y,e),p(a,e),p(S,e),p(W,e),p(F,e),p(z,e),p(M,e)}}}function me(B){let u,T,$,k,g,_,x,v,b,q,y,C="Test what you learned in this chapter!",a,w,S,I,W,P,F,E,z,H,M,N,e,s,Y,D,K,X;g=new de({props:{fw:B[0]}}),x=new A({props:{title:"End-of-chapter quiz",local:"end-of-chapter-quiz",headingTag:"h1"}}),b=new le({props:{chapter:3,classNames:"absolute z-10 right-0 top-0"}}),w=new A({props:{title:"1. The emotion dataset contains Twitter messages labeled with emotions. Search for it in the Hub , and read the dataset card. Which of these is not one of its basic emotions?",local:"1-the-emotion-dataset-contains-twitter-messages-labeled-with-emotions-search-for-it-in-the-hub--and-read-the-dataset-card-which-of-these-is-not-one-of-its-basic-emotions",headingTag:"h3"}}),I=new L({props:{choices:[{text:"Joy",explain:"Try again â€” this emotion is present in that dataset!"},{text:"Love",explain:"Try again â€” this emotion is present in that dataset!"},{text:"Confusion",explain:"Correct! Confusion is not one of the six basic emotions.",correct:!0},{text:"Surprise",explain:"Surprise! Try another one!"}]}}),P=new A({props:{title:"2. Search for the ar_sarcasm dataset in the Hub . Which task does it support?",local:"2-search-for-the-arsarcasm-dataset-in-the-hub--which-task-does-it-support",headingTag:"h3"}}),E=new L({props:{choices:[{text:"Sentiment classification",explain:"That's right! You can tell thanks to the tags.",correct:!0},{text:"Machine translation",explain:"That's not it â€” take another look at the <a href='https://huggingface.co/datasets/ar_sarcasm'>dataset card</a>!"},{text:"Named entity recognition",explain:"That's not it â€” take another look at the <a href='https://huggingface.co/datasets/ar_sarcasm'>dataset card</a>!"},{text:"Question answering",explain:"Alas, this question was not answered correctly. Try again!"}]}}),H=new A({props:{title:"3. How does the BERT model expect a pair of sentences to be processed?",local:"3-how-does-the-bert-model-expect-a-pair-of-sentences-to-be-processed",headingTag:"h3"}}),N=new L({props:{choices:[{text:"Tokens_of_sentence_1 [SEP] Tokens_of_sentence_2",explain:"A <code>[SEP]</code> special token is needed to separate the two sentences, but that's not the only thing!"},{text:"[CLS] Tokens_of_sentence_1 Tokens_of_sentence_2",explain:"A <code>[CLS]</code> special token is required at the beginning, but that's not the only thing!"},{text:"[CLS] Tokens_of_sentence_1 [SEP] Tokens_of_sentence_2 [SEP]",explain:"That's correct!",correct:!0},{text:"[CLS] Tokens_of_sentence_1 [SEP] Tokens_of_sentence_2",explain:"A <code>[CLS]</code> special token is needed at the beginning as well as a <code>[SEP]</code> special token to separate the two sentences, but that's not all!"}]}});const R=[fe,ce],U=[];function J(t,o){return t[0]==="pt"?0:1}return s=J(B),Y=U[s]=R[s](B),{c(){u=j("meta"),T=r(),$=j("p"),k=r(),c(g.$$.fragment),_=r(),c(x.$$.fragment),v=r(),c(b.$$.fragment),q=r(),y=j("p"),y.textContent=C,a=r(),c(w.$$.fragment),S=r(),c(I.$$.fragment),W=r(),c(P.$$.fragment),F=r(),c(E.$$.fragment),z=r(),c(H.$$.fragment),M=r(),c(N.$$.fragment),e=r(),Y.c(),D=r(),K=j("p"),this.h()},l(t){const o=se("svelte-u9bgzb",document.head);u=G(o,"META",{name:!0,content:!0}),o.forEach(n),T=h(t),$=G(t,"P",{}),V($).forEach(n),k=h(t),f(g.$$.fragment,t),_=h(t),f(x.$$.fragment,t),v=h(t),f(b.$$.fragment,t),q=h(t),y=G(t,"P",{"data-svelte-h":!0}),oe(y)!=="svelte-1nxov2f"&&(y.textContent=C),a=h(t),f(w.$$.fragment,t),S=h(t),f(I.$$.fragment,t),W=h(t),f(P.$$.fragment,t),F=h(t),f(E.$$.fragment,t),z=h(t),f(H.$$.fragment,t),M=h(t),f(N.$$.fragment,t),e=h(t),Y.l(t),D=h(t),K=G(t,"P",{}),V(K).forEach(n),this.h()},h(){Z(u,"name","hf:doc:metadata"),Z(u,"content",pe)},m(t,o){ie(document.head,u),i(t,T,o),i(t,$,o),i(t,k,o),m(g,t,o),i(t,_,o),m(x,t,o),i(t,v,o),m(b,t,o),i(t,q,o),i(t,y,o),i(t,a,o),m(w,t,o),i(t,S,o),m(I,t,o),i(t,W,o),m(P,t,o),i(t,F,o),m(E,t,o),i(t,z,o),m(H,t,o),i(t,M,o),m(N,t,o),i(t,e,o),U[s].m(t,o),i(t,D,o),i(t,K,o),X=!0},p(t,[o]){const O={};o&1&&(O.fw=t[0]),g.$set(O);let Q=s;s=J(t),s!==Q&&(he(),l(U[Q],1,1,()=>{U[Q]=null}),re(),Y=U[s],Y||(Y=U[s]=R[s](t),Y.c()),d(Y,1),Y.m(D.parentNode,D))},i(t){X||(d(g.$$.fragment,t),d(x.$$.fragment,t),d(b.$$.fragment,t),d(w.$$.fragment,t),d(I.$$.fragment,t),d(P.$$.fragment,t),d(E.$$.fragment,t),d(H.$$.fragment,t),d(N.$$.fragment,t),d(Y),X=!0)},o(t){l(g.$$.fragment,t),l(x.$$.fragment,t),l(b.$$.fragment,t),l(w.$$.fragment,t),l(I.$$.fragment,t),l(P.$$.fragment,t),l(E.$$.fragment,t),l(H.$$.fragment,t),l(N.$$.fragment,t),l(Y),X=!1},d(t){t&&(n(T),n($),n(k),n(_),n(v),n(q),n(y),n(a),n(S),n(W),n(F),n(z),n(M),n(e),n(D),n(K)),n(u),p(g,t),p(x,t),p(b,t),p(w,t),p(I,t),p(P,t),p(E,t),p(H,t),p(N,t),U[s].d(t)}}}const pe='{"title":"End-of-chapter quiz","local":"end-of-chapter-quiz","sections":[{"title":"1. The emotion dataset contains Twitter messages labeled with emotions. Search for it in the Hub , and read the dataset card. Which of these is not one of its basic emotions?","local":"1-the-emotion-dataset-contains-twitter-messages-labeled-with-emotions-search-for-it-in-the-hub--and-read-the-dataset-card-which-of-these-is-not-one-of-its-basic-emotions","sections":[],"depth":3},{"title":"2. Search for the ar_sarcasm dataset in the Hub . Which task does it support?","local":"2-search-for-the-arsarcasm-dataset-in-the-hub--which-task-does-it-support","sections":[],"depth":3},{"title":"3. How does the BERT model expect a pair of sentences to be processed?","local":"3-how-does-the-bert-model-expect-a-pair-of-sentences-to-be-processed","sections":[],"depth":3},{"title":"4. What are the benefits of the Dataset.map() method?","local":"4-what-are-the-benefits-of-the-datasetmap-method","sections":[],"depth":3},{"title":"5. What does dynamic padding mean?","local":"5-what-does-dynamic-padding-mean","sections":[],"depth":3},{"title":"6. What is the purpose of a collate function?","local":"6-what-is-the-purpose-of-a-collate-function","sections":[],"depth":3},{"title":"7. What happens when you instantiate one of the AutoModelForXxx classes with a pretrained language model (such as bert-base-uncased ) that corresponds to a different task than the one for which it was trained?","local":"7-what-happens-when-you-instantiate-one-of-the-automodelforxxx-classes-with-a-pretrained-language-model-such-as-bert-base-uncased--that-corresponds-to-a-different-task-than-the-one-for-which-it-was-trained","sections":[],"depth":3},{"title":"8. Whatâ€™s the purpose of TrainingArguments ?","local":"8-whats-the-purpose-of-trainingarguments-","sections":[],"depth":3},{"title":"9. Why should you use the ðŸ¤— Accelerate library?","local":"9-why-should-you-use-the--accelerate-library","sections":[],"depth":3},{"title":"4. What happens when you instantiate one of the TFAutoModelForXxx classes with a pretrained language model (such as bert-base-uncased ) that corresponds to a different task than the one for which it was trained?","local":"4-what-happens-when-you-instantiate-one-of-the-tfautomodelforxxx-classes-with-a-pretrained-language-model-such-as-bert-base-uncased--that-corresponds-to-a-different-task-than-the-one-for-which-it-was-trained","sections":[],"depth":3},{"title":"5. The TensorFlow models from transformers are already Keras models. What benefit does this offer?","local":"5-the-tensorflow-models-from-transformers-are-already-keras-models-what-benefit-does-this-offer","sections":[],"depth":3},{"title":"6. How can you define your own custom metric?","local":"6-how-can-you-define-your-own-custom-metric","sections":[],"depth":3}],"depth":1}';function ue(B,u,T){let $="pt";return te(()=>{const k=new URLSearchParams(window.location.search);T(0,$=k.get("fw")||"pt")}),[$]}class Te extends ae{constructor(u){super(),ne(this,u,ue,me,ee,{})}}export{Te as component};
