import{s as ve,o as We}from"../chunks/scheduler.37c15a92.js";import{S as Ae,i as je,g as $e,s as c,r as m,A as Je,h as we,f as s,c as u,j as Me,u as f,k as _e,y as Ze,a as h,v as p,t as r,b as qe,d as l,w as d,p as Ie}from"../chunks/index.2bf4358c.js";import{C as be}from"../chunks/CodeBlock.4f5fc1ad.js";import{C as Ue}from"../chunks/CourseFloatingBanner.15ba07e6.js";import{Q as k}from"../chunks/Question.668688bc.js";import{F as Ce}from"../chunks/FrameworkSwitchCourse.8d4d4ab6.js";import{H as y}from"../chunks/Heading.8ada512a.js";function Fe(x){let a,g,i,$;return a=new y({props:{title:"5. What is an TFAutoModel?",local:"5-what-is-an-tfautomodel",headingTag:"h3"}}),i=new k({props:{choices:[{text:"A model that automatically trains on your data",explain:"Incorrect. Are you mistaking this with our <a href='https://huggingface.co/autotrain'>AutoTrain</a> product?"},{text:"An object that returns the correct architecture based on the checkpoint",explain:"Exactly: the <code>TFAutoModel</code> only needs to know the checkpoint from which to initialize to return the correct architecture.",correct:!0},{text:"A model that automatically detects the language used for its inputs to load the correct weights",explain:"Incorrect; while some checkpoints and models are capable of handling multiple languages, there are no built-in tools for automatic checkpoint selection according to language. You should head over to the <a href='https://huggingface.co/models'>Model Hub</a> to find the best checkpoint for your task!"}]}}),{c(){m(a.$$.fragment),g=c(),m(i.$$.fragment)},l(t){f(a.$$.fragment,t),g=u(t),f(i.$$.fragment,t)},m(t,w){p(a,t,w),h(t,g,w),p(i,t,w),$=!0},i(t){$||(l(a.$$.fragment,t),l(i.$$.fragment,t),$=!0)},o(t){r(a.$$.fragment,t),r(i.$$.fragment,t),$=!1},d(t){t&&s(g),d(a,t),d(i,t)}}}function Ve(x){let a,g,i,$;return a=new y({props:{title:"5. What is an AutoModel?",local:"5-what-is-an-automodel",headingTag:"h3"}}),i=new k({props:{choices:[{text:"A model that automatically trains on your data",explain:"Incorrect. Are you mistaking this with our <a href='https://huggingface.co/autotrain'>AutoTrain</a> product?"},{text:"An object that returns the correct architecture based on the checkpoint",explain:"Exactly: the <code>AutoModel</code> only needs to know the checkpoint from which to initialize to return the correct architecture.",correct:!0},{text:"A model that automatically detects the language used for its inputs to load the correct weights",explain:"Incorrect; while some checkpoints and models are capable of handling multiple languages, there are no built-in tools for automatic checkpoint selection according to language. You should head over to the <a href='https://huggingface.co/models'>Model Hub</a> to find the best checkpoint for your task!"}]}}),{c(){m(a.$$.fragment),g=c(),m(i.$$.fragment)},l(t){f(a.$$.fragment,t),g=u(t),f(i.$$.fragment,t)},m(t,w){p(a,t,w),h(t,g,w),p(i,t,w),$=!0},i(t){$||(l(a.$$.fragment,t),l(i.$$.fragment,t),$=!0)},o(t){r(a.$$.fragment,t),r(i.$$.fragment,t),$=!1},d(t){t&&s(g),d(a,t),d(i,t)}}}function He(x){let a,g,i,$,t,w;return a=new y({props:{title:"10. Is there something wrong with the following code?",local:"10-is-there-something-wrong-with-the-following-code",headingTag:"h3"}}),i=new be({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBURkF1dG9Nb2RlbCUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJncHQyJTIyKSUwQSUwQWVuY29kZWQlMjAlM0QlMjB0b2tlbml6ZXIoJTIySGV5ISUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBcmVzdWx0JTIwJTNEJTIwbW9kZWwoKiplbmNvZGVkKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

encoded = tokenizer(<span class="hljs-string">&quot;Hey!&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
result = model(**encoded)`,wrap:!1}}),t=new k({props:{choices:[{text:"No, it seems correct.",explain:"Unfortunately, coupling a model with a tokenizer that was trained with a different checkpoint is rarely a good idea. The model was not trained to make sense out of this tokenizer's output, so the model output (if it can even run!) will not make any sense."},{text:"The tokenizer and model should always be from the same checkpoint.",explain:"Right!",correct:!0},{text:"It's good practice to pad and truncate with the tokenizer as every input is a batch.",explain:"It's true that every model input needs to be a batch. However, truncating or padding this sequence wouldn't necessarily make sense as there is only one of it, and those are techniques to batch together a list of sentences."}]}}),{c(){m(a.$$.fragment),g=c(),m(i.$$.fragment),$=c(),m(t.$$.fragment)},l(n){f(a.$$.fragment,n),g=u(n),f(i.$$.fragment,n),$=u(n),f(t.$$.fragment,n)},m(n,b){p(a,n,b),h(n,g,b),p(i,n,b),h(n,$,b),p(t,n,b),w=!0},i(n){w||(l(a.$$.fragment,n),l(i.$$.fragment,n),l(t.$$.fragment,n),w=!0)},o(n){r(a.$$.fragment,n),r(i.$$.fragment,n),r(t.$$.fragment,n),w=!1},d(n){n&&(s(g),s($)),d(a,n),d(i,n),d(t,n)}}}function Ee(x){let a,g,i,$,t,w;return a=new y({props:{title:"10. Is there something wrong with the following code?",local:"10-is-there-something-wrong-with-the-following-code",headingTag:"h3"}}),i=new be({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBBdXRvTW9kZWwlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmdwdDIlMjIpJTBBJTBBZW5jb2RlZCUyMCUzRCUyMHRva2VuaXplciglMjJIZXkhJTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEFyZXN1bHQlMjAlM0QlMjBtb2RlbCgqKmVuY29kZWQp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
model = AutoModel.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

encoded = tokenizer(<span class="hljs-string">&quot;Hey!&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
result = model(**encoded)`,wrap:!1}}),t=new k({props:{choices:[{text:"No, it seems correct.",explain:"Unfortunately, coupling a model with a tokenizer that was trained with a different checkpoint is rarely a good idea. The model was not trained to make sense out of this tokenizer's output, so the model output (if it can even run!) will not make any sense."},{text:"The tokenizer and model should always be from the same checkpoint.",explain:"Right!",correct:!0},{text:"It's good practice to pad and truncate with the tokenizer as every input is a batch.",explain:"It's true that every model input needs to be a batch. However, truncating or padding this sequence wouldn't necessarily make sense as there is only one of it, and those are techniques to batch together a list of sentences."}]}}),{c(){m(a.$$.fragment),g=c(),m(i.$$.fragment),$=c(),m(t.$$.fragment)},l(n){f(a.$$.fragment,n),g=u(n),f(i.$$.fragment,n),$=u(n),f(t.$$.fragment,n)},m(n,b){p(a,n,b),h(n,g,b),p(i,n,b),h(n,$,b),p(t,n,b),w=!0},i(n){w||(l(a.$$.fragment,n),l(i.$$.fragment,n),l(t.$$.fragment,n),w=!0)},o(n){r(a.$$.fragment,n),r(i.$$.fragment,n),r(t.$$.fragment,n),w=!1},d(n){n&&(s(g),s($)),d(a,n),d(i,n),d(t,n)}}}function Be(x){let a,g,i,$,t,w,n,b,q,D,I,K,v,L,W,O,A,ee,j,te,J,ne,Z,oe,U,ae,T,z,Y,C,ie,F,se,V,re,H,le,E,he,B,ce,R,ue,Q,me,X,fe,M,_,N,P,pe;t=new Ce({props:{fw:x[0]}}),n=new y({props:{title:"End-of-chapter quiz",local:"end-of-chapter-quiz",headingTag:"h1"}}),q=new Ue({props:{chapter:2,classNames:"absolute z-10 right-0 top-0"}}),I=new y({props:{title:"1. What is the order of the language modeling pipeline?",local:"1-what-is-the-order-of-the-language-modeling-pipeline",headingTag:"h3"}}),v=new k({props:{choices:[{text:"First, the model, which handles text and returns raw predictions. The tokenizer then makes sense of these predictions and converts them back to text when needed.",explain:"The model cannot understand text! The tokenizer must first tokenize the text and convert it to IDs so that it is understandable by the model."},{text:"First, the tokenizer, which handles text and returns IDs. The model handles these IDs and outputs a prediction, which can be some text.",explain:"The model's prediction cannot be text straight away. The tokenizer has to be used in order to convert the prediction back to text!"},{text:"The tokenizer handles text and returns IDs. The model handles these IDs and outputs a prediction. The tokenizer can then be used once again to convert these predictions back to some text.",explain:"Correct! The tokenizer can be used for both tokenizing and de-tokenizing.",correct:!0}]}}),W=new y({props:{title:"2. How many dimensions does the tensor output by the base Transformer model have, and what are they?",local:"2-how-many-dimensions-does-the-tensor-output-by-the-base-transformer-model-have-and-what-are-they",headingTag:"h3"}}),A=new k({props:{choices:[{text:"2: The sequence length and the batch size",explain:"False! The tensor output by the model has a third dimension: hidden size."},{text:"2: The sequence length and the hidden size",explain:"False! All Transformer models handle batches, even with a single sequence; that would be a batch size of 1!"},{text:"3: The sequence length, the batch size, and the hidden size",explain:"Correct!",correct:!0}]}}),j=new y({props:{title:"3. Which of the following is an example of subword tokenization?",local:"3-which-of-the-following-is-an-example-of-subword-tokenization",headingTag:"h3"}}),J=new k({props:{choices:[{text:"WordPiece",explain:"Yes, that's one example of subword tokenization!",correct:!0},{text:"Character-based tokenization",explain:"Character-based tokenization is not a type of subword tokenization."},{text:"Splitting on whitespace and punctuation",explain:"That's a word-based tokenization scheme!"},{text:"BPE",explain:"Yes, that's one example of subword tokenization!",correct:!0},{text:"Unigram",explain:"Yes, that's one example of subword tokenization!",correct:!0},{text:"None of the above",explain:"Incorrect!"}]}}),Z=new y({props:{title:"4. What is a model head?",local:"4-what-is-a-model-head",headingTag:"h3"}}),U=new k({props:{choices:[{text:"A component of the base Transformer network that redirects tensors to their correct layers",explain:"Incorrect! There's no such component."},{text:"Also known as the self-attention mechanism, it adapts the representation of a token according to the other tokens of the sequence",explain:'Incorrect! The self-attention layer does contain attention "heads," but these are not adaptation heads.'},{text:"An additional component, usually made up of one or a few layers, to convert the transformer predictions to a task-specific output",explain:"That's right. Adaptation heads, also known simply as heads, come up in different forms: language modeling heads, question answering heads, sequence classification heads... ",correct:!0}]}});const ye=[Ve,Fe],S=[];function ke(e,o){return e[0]==="pt"?0:1}T=ke(x),z=S[T]=ye[T](x),C=new y({props:{title:"6. What are the techniques to be aware of when batching sequences of different lengths together?",local:"6-what-are-the-techniques-to-be-aware-of-when-batching-sequences-of-different-lengths-together",headingTag:"h3"}}),F=new k({props:{choices:[{text:"Truncating",explain:"Yes, truncation is a correct way of evening out sequences so that they fit in a rectangular shape. Is it the only one, though?",correct:!0},{text:"Returning tensors",explain:"While the other techniques allow you to return rectangular tensors, returning tensors isn't helpful when batching sequences together."},{text:"Padding",explain:"Yes, padding is a correct way of evening out sequences so that they fit in a rectangular shape. Is it the only one, though?",correct:!0},{text:"Attention masking",explain:"Absolutely! Attention masks are of prime importance when handling sequences of different lengths. That's not the only technique to be aware of, however.",correct:!0}]}}),V=new y({props:{title:"7. What is the point of applying a SoftMax function to the logits output by a sequence classification model?",local:"7-what-is-the-point-of-applying-a-softmax-function-to-the-logits-output-by-a-sequence-classification-model",headingTag:"h3"}}),H=new k({props:{choices:[{text:"It softens the logits so that they're more reliable.",explain:"No, the SoftMax function does not affect the reliability of results."},{text:"It applies a lower and upper bound so that they're understandable.",explain:"Correct! The resulting values are bound between 0 and 1. That's not the only reason we use a SoftMax function, though.",correct:!0},{text:"The total sum of the output is then 1, resulting in a possible probabilistic interpretation.",explain:"Correct! That's not the only reason we use a SoftMax function, though.",correct:!0}]}}),E=new y({props:{title:"8. What method is most of the tokenizer API centered around?",local:"8-what-method-is-most-of-the-tokenizer-api-centered-around",headingTag:"h3"}}),B=new k({props:{choices:[{text:"<code>encode</code>, as it can encode text into IDs and IDs into predictions",explain:"Wrong! While the <code>encode</code> method does exist on tokenizers, it does not exist on models."},{text:"Calling the tokenizer object directly.",explain:"Exactly! The <code>__call__</code> method of the tokenizer is a very powerful method which can handle pretty much anything. It is also the method used to retrieve predictions from a model.",correct:!0},{text:"<code>pad</code>",explain:"Wrong! Padding is very useful, but it's just one part of the tokenizer API."},{text:"<code>tokenize</code>",explain:"The <code>tokenize</code> method is arguably one of the most useful methods, but it isn't the core of the tokenizer API."}]}}),R=new y({props:{title:"9. What does the result variable contain in this code sample?",local:"9-what-does-the-result-variable-contain-in-this-code-sample",headingTag:"h3"}}),Q=new be({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBcmVzdWx0JTIwJTNEJTIwdG9rZW5pemVyLnRva2VuaXplKCUyMkhlbGxvISUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
result = tokenizer.tokenize(<span class="hljs-string">&quot;Hello!&quot;</span>)`,wrap:!1}}),X=new k({props:{choices:[{text:"A list of strings, each string being a token",explain:"Absolutely! Convert this to IDs, and send them to a model!",correct:!0},{text:"A list of IDs",explain:"Incorrect; that's what the <code>__call__</code> or <code>convert_tokens_to_ids</code> method is for!"},{text:"A string containing all of the tokens",explain:"This would be suboptimal, as the goal is to split the string into multiple tokens."}]}});const xe=[Ee,He],G=[];function Te(e,o){return e[0]==="pt"?0:1}return M=Te(x),_=G[M]=xe[M](x),{c(){a=$e("meta"),g=c(),i=$e("p"),$=c(),m(t.$$.fragment),w=c(),m(n.$$.fragment),b=c(),m(q.$$.fragment),D=c(),m(I.$$.fragment),K=c(),m(v.$$.fragment),L=c(),m(W.$$.fragment),O=c(),m(A.$$.fragment),ee=c(),m(j.$$.fragment),te=c(),m(J.$$.fragment),ne=c(),m(Z.$$.fragment),oe=c(),m(U.$$.fragment),ae=c(),z.c(),Y=c(),m(C.$$.fragment),ie=c(),m(F.$$.fragment),se=c(),m(V.$$.fragment),re=c(),m(H.$$.fragment),le=c(),m(E.$$.fragment),he=c(),m(B.$$.fragment),ce=c(),m(R.$$.fragment),ue=c(),m(Q.$$.fragment),me=c(),m(X.$$.fragment),fe=c(),_.c(),N=c(),P=$e("p"),this.h()},l(e){const o=Je("svelte-u9bgzb",document.head);a=we(o,"META",{name:!0,content:!0}),o.forEach(s),g=u(e),i=we(e,"P",{}),Me(i).forEach(s),$=u(e),f(t.$$.fragment,e),w=u(e),f(n.$$.fragment,e),b=u(e),f(q.$$.fragment,e),D=u(e),f(I.$$.fragment,e),K=u(e),f(v.$$.fragment,e),L=u(e),f(W.$$.fragment,e),O=u(e),f(A.$$.fragment,e),ee=u(e),f(j.$$.fragment,e),te=u(e),f(J.$$.fragment,e),ne=u(e),f(Z.$$.fragment,e),oe=u(e),f(U.$$.fragment,e),ae=u(e),z.l(e),Y=u(e),f(C.$$.fragment,e),ie=u(e),f(F.$$.fragment,e),se=u(e),f(V.$$.fragment,e),re=u(e),f(H.$$.fragment,e),le=u(e),f(E.$$.fragment,e),he=u(e),f(B.$$.fragment,e),ce=u(e),f(R.$$.fragment,e),ue=u(e),f(Q.$$.fragment,e),me=u(e),f(X.$$.fragment,e),fe=u(e),_.l(e),N=u(e),P=we(e,"P",{}),Me(P).forEach(s),this.h()},h(){_e(a,"name","hf:doc:metadata"),_e(a,"content",Re)},m(e,o){Ze(document.head,a),h(e,g,o),h(e,i,o),h(e,$,o),p(t,e,o),h(e,w,o),p(n,e,o),h(e,b,o),p(q,e,o),h(e,D,o),p(I,e,o),h(e,K,o),p(v,e,o),h(e,L,o),p(W,e,o),h(e,O,o),p(A,e,o),h(e,ee,o),p(j,e,o),h(e,te,o),p(J,e,o),h(e,ne,o),p(Z,e,o),h(e,oe,o),p(U,e,o),h(e,ae,o),S[T].m(e,o),h(e,Y,o),p(C,e,o),h(e,ie,o),p(F,e,o),h(e,se,o),p(V,e,o),h(e,re,o),p(H,e,o),h(e,le,o),p(E,e,o),h(e,he,o),p(B,e,o),h(e,ce,o),p(R,e,o),h(e,ue,o),p(Q,e,o),h(e,me,o),p(X,e,o),h(e,fe,o),G[M].m(e,o),h(e,N,o),h(e,P,o),pe=!0},p(e,[o]){const ze={};o&1&&(ze.fw=e[0]),t.$set(ze);let de=T;T=ke(e),T!==de&&(Ie(),r(S[de],1,1,()=>{S[de]=null}),qe(),z=S[T],z||(z=S[T]=ye[T](e),z.c()),l(z,1),z.m(Y.parentNode,Y));let ge=M;M=Te(e),M!==ge&&(Ie(),r(G[ge],1,1,()=>{G[ge]=null}),qe(),_=G[M],_||(_=G[M]=xe[M](e),_.c()),l(_,1),_.m(N.parentNode,N))},i(e){pe||(l(t.$$.fragment,e),l(n.$$.fragment,e),l(q.$$.fragment,e),l(I.$$.fragment,e),l(v.$$.fragment,e),l(W.$$.fragment,e),l(A.$$.fragment,e),l(j.$$.fragment,e),l(J.$$.fragment,e),l(Z.$$.fragment,e),l(U.$$.fragment,e),l(z),l(C.$$.fragment,e),l(F.$$.fragment,e),l(V.$$.fragment,e),l(H.$$.fragment,e),l(E.$$.fragment,e),l(B.$$.fragment,e),l(R.$$.fragment,e),l(Q.$$.fragment,e),l(X.$$.fragment,e),l(_),pe=!0)},o(e){r(t.$$.fragment,e),r(n.$$.fragment,e),r(q.$$.fragment,e),r(I.$$.fragment,e),r(v.$$.fragment,e),r(W.$$.fragment,e),r(A.$$.fragment,e),r(j.$$.fragment,e),r(J.$$.fragment,e),r(Z.$$.fragment,e),r(U.$$.fragment,e),r(z),r(C.$$.fragment,e),r(F.$$.fragment,e),r(V.$$.fragment,e),r(H.$$.fragment,e),r(E.$$.fragment,e),r(B.$$.fragment,e),r(R.$$.fragment,e),r(Q.$$.fragment,e),r(X.$$.fragment,e),r(_),pe=!1},d(e){e&&(s(g),s(i),s($),s(w),s(b),s(D),s(K),s(L),s(O),s(ee),s(te),s(ne),s(oe),s(ae),s(Y),s(ie),s(se),s(re),s(le),s(he),s(ce),s(ue),s(me),s(fe),s(N),s(P)),s(a),d(t,e),d(n,e),d(q,e),d(I,e),d(v,e),d(W,e),d(A,e),d(j,e),d(J,e),d(Z,e),d(U,e),S[T].d(e),d(C,e),d(F,e),d(V,e),d(H,e),d(E,e),d(B,e),d(R,e),d(Q,e),d(X,e),G[M].d(e)}}}const Re='{"title":"End-of-chapter quiz","local":"end-of-chapter-quiz","sections":[{"title":"1. What is the order of the language modeling pipeline?","local":"1-what-is-the-order-of-the-language-modeling-pipeline","sections":[],"depth":3},{"title":"2. How many dimensions does the tensor output by the base Transformer model have, and what are they?","local":"2-how-many-dimensions-does-the-tensor-output-by-the-base-transformer-model-have-and-what-are-they","sections":[],"depth":3},{"title":"3. Which of the following is an example of subword tokenization?","local":"3-which-of-the-following-is-an-example-of-subword-tokenization","sections":[],"depth":3},{"title":"4. What is a model head?","local":"4-what-is-a-model-head","sections":[],"depth":3},{"title":"5. What is an AutoModel?","local":"5-what-is-an-automodel","sections":[],"depth":3},{"title":"5. What is an TFAutoModel?","local":"5-what-is-an-tfautomodel","sections":[],"depth":3},{"title":"6. What are the techniques to be aware of when batching sequences of different lengths together?","local":"6-what-are-the-techniques-to-be-aware-of-when-batching-sequences-of-different-lengths-together","sections":[],"depth":3},{"title":"7. What is the point of applying a SoftMax function to the logits output by a sequence classification model?","local":"7-what-is-the-point-of-applying-a-softmax-function-to-the-logits-output-by-a-sequence-classification-model","sections":[],"depth":3},{"title":"8. What method is most of the tokenizer API centered around?","local":"8-what-method-is-most-of-the-tokenizer-api-centered-around","sections":[],"depth":3},{"title":"9. What does the result variable contain in this code sample?","local":"9-what-does-the-result-variable-contain-in-this-code-sample","sections":[],"depth":3},{"title":"10. Is there something wrong with the following code?","local":"10-is-there-something-wrong-with-the-following-code","sections":[],"depth":3},{"title":"10. Is there something wrong with the following code?","local":"10-is-there-something-wrong-with-the-following-code","sections":[],"depth":3}],"depth":1}';function Qe(x,a,g){let i="pt";return We(()=>{const $=new URLSearchParams(window.location.search);g(0,i=$.get("fw")||"pt")}),[i]}class Ke extends Ae{constructor(a){super(),je(this,a,Qe,Be,ve,{})}}export{Ke as component};
